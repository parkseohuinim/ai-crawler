#!/usr/bin/env python3
"""
MCP ì„œë²„ ë©”ì¸ ëª¨ë“ˆ (OpenAI MCP í‘œì¤€)
PROJECT_SPECIFICATION.md ìš”êµ¬ì‚¬í•­ì— ë”°ë¥¸ ì˜¬ë°”ë¥¸ MCP êµ¬í˜„
"""

import os
import sys
import json
import asyncio
import logging
from typing import Any, Dict, List, Optional
from dotenv import load_dotenv

# MCP ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
from mcp.server.fastmcp import FastMCP

# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(current_dir)

from tools.site_analyzer import SiteAnalyzer
from tools.crawler_selector import CrawlerSelector  
from tools.structure_detector import StructureDetector
from tools.quality_validator import QualityValidator
from tools.content_extractor import ContentExtractor

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (ë£¨íŠ¸ ë””ë ‰í† ë¦¬ì—ì„œ)
load_dotenv(os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), '.env'))

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# MCP ì„œë²„ ì´ˆê¸°í™” (OpenAI MCP í‘œì¤€)
mcp = FastMCP("AI Crawler MCP Server")

# MCP ë„êµ¬ ì¸ìŠ¤í„´ìŠ¤ë“¤
site_analyzer = SiteAnalyzer()
crawler_selector = CrawlerSelector()
structure_detector = StructureDetector()
quality_validator = QualityValidator()
content_extractor = ContentExtractor()

print(f"ğŸ§  MCP ì„œë²„ ì‹œì‘: {mcp.name}")

@mcp.tool()
async def analyze_site_and_select_crawler(url: str, sample_html: str = "", headers: dict = {}) -> dict:
    """
    ì‚¬ì´íŠ¸ ì¢…í•© ë¶„ì„ ë° ìµœì  í¬ë¡¤ëŸ¬ ì„ íƒ:
    - SPA/SSR/Static íŒë³„
    - JavaScript ë³µì¡ë„ ë¶„ì„
    - ì•ˆí‹°ë´‡ ì‹œìŠ¤í…œ ê°ì§€
    - ì½˜í…ì¸  ë¡œë”© ë°©ì‹ íŒŒì•…
    - ì¶”ì²œ í¬ë¡¤ë§ ì—”ì§„ ê²°ì •
    
    Args:
        url: ë¶„ì„í•  ì›¹ì‚¬ì´íŠ¸ URL
        sample_html: ì‚¬ì´íŠ¸ ìƒ˜í”Œ HTML (ì„ íƒì‚¬í•­)
        headers: HTTP í—¤ë” ì •ë³´ (ì„ íƒì‚¬í•­)
    
    Returns:
        ì‚¬ì´íŠ¸ ë¶„ì„ ê²°ê³¼ ë° ì¶”ì²œ í¬ë¡¤ëŸ¬ ì •ë³´
    """
    try:
        logger.info(f"ì‚¬ì´íŠ¸ ë¶„ì„ ì‹œì‘: {url}")
        result = await site_analyzer.analyze_and_select(
            url=url,
            sample_html=sample_html,
            headers=headers
        )
        return result
    except Exception as e:
        logger.error(f"ì‚¬ì´íŠ¸ ë¶„ì„ ì˜¤ë¥˜: {e}")
        return {
            "error": str(e),
            "url": url,
            "status": "failed"
        }

@mcp.tool()
async def detect_content_structure(html_sample: str, url: str = "") -> dict:
    """
    ì½˜í…ì¸  êµ¬ì¡° íŒ¨í„´ ë¶„ì„:
    - ê³„ì¸µêµ¬ì¡° ì‹ë³„
    - ì£¼ìš” ì½˜í…ì¸  ì˜ì—­ ê°ì§€
    - ë„¤ë¹„ê²Œì´ì…˜/ì‚¬ì´ë“œë°” êµ¬ë¶„
    - ì œëª©/ë³¸ë¬¸ íŒ¨í„´ ì¸ì‹
    
    Args:
        html_sample: ë¶„ì„í•  HTML ìƒ˜í”Œ
        url: ì›ë³¸ URL (ì„ íƒì‚¬í•­)
    
    Returns:
        ì½˜í…ì¸  êµ¬ì¡° ë¶„ì„ ê²°ê³¼
    """
    try:
        logger.info(f"ì½˜í…ì¸  êµ¬ì¡° ë¶„ì„ ì‹œì‘ (URL: {url})")
        result = await structure_detector.detect_structure(
            html_sample=html_sample,
            url=url
        )
        return result
    except Exception as e:
        logger.error(f"êµ¬ì¡° ë¶„ì„ ì˜¤ë¥˜: {e}")
        return {
            "error": str(e),
            "url": url,
            "status": "failed"
        }

@mcp.tool()
async def generate_extraction_strategy(site_analysis: dict, content_structure: dict) -> dict:
    """
    ì¶”ì¶œ ì „ëµ ìˆ˜ë¦½:
    - ì—”ì§„ë³„ ìµœì  ì„¤ì •
    - CSS ì…€ë ‰í„° ê·œì¹™
    - ì œì™¸ ì˜ì—­ ì •ì˜
    - í›„ì²˜ë¦¬ ë°©ë²•
    
    Args:
        site_analysis: ì‚¬ì´íŠ¸ ë¶„ì„ ê²°ê³¼
        content_structure: ì½˜í…ì¸  êµ¬ì¡° ë¶„ì„ ê²°ê³¼
    
    Returns:
        í¬ë¡¤ë§ ì „ëµ ë° ì„¤ì •
    """
    try:
        logger.info("ì¶”ì¶œ ì „ëµ ìƒì„± ì‹œì‘")
        result = await crawler_selector.generate_strategy(
            site_analysis=site_analysis,
            content_structure=content_structure
        )
        return result
    except Exception as e:
        logger.error(f"ì „ëµ ìƒì„± ì˜¤ë¥˜: {e}")
        return {
            "error": str(e),
            "status": "failed"
        }

@mcp.tool()
async def validate_crawling_result(extracted_data: dict, url: str, expected_quality: float = 70.0) -> dict:
    """
    í¬ë¡¤ë§ ê²°ê³¼ í’ˆì§ˆ ê²€ì¦:
    - í•„ìˆ˜ ì½˜í…ì¸  ì¡´ì¬ í™•ì¸
    - í…ìŠ¤íŠ¸ í’ˆì§ˆ í‰ê°€
    - êµ¬ì¡°ì  ì™„ì„±ë„ ê²€ì‚¬
    - ì¬ì‹œë„ í•„ìš”ì„± íŒë‹¨
    
    Args:
        extracted_data: ì¶”ì¶œëœ ë°ì´í„°
        url: ì›ë³¸ URL
        expected_quality: ê¸°ëŒ€ í’ˆì§ˆ ì ìˆ˜ (ê¸°ë³¸ê°’: 70.0)
    
    Returns:
        í’ˆì§ˆ ê²€ì¦ ê²°ê³¼
    """
    try:
        logger.info(f"í’ˆì§ˆ ê²€ì¦ ì‹œì‘: {url}")
        result = await quality_validator.validate_result(
            extracted_data=extracted_data,
            url=url,
            expected_quality=expected_quality
        )
        return result
    except Exception as e:
        logger.error(f"í’ˆì§ˆ ê²€ì¦ ì˜¤ë¥˜: {e}")
        return {
            "error": str(e),
            "url": url,
            "status": "failed"
        }

@mcp.tool()
async def extract_selective_content(html_content: str, target_content: str, url: str = "") -> dict:
    """
    ì„ íƒì  ì½˜í…ì¸  ì¶”ì¶œ:
    - ì œëª©ë§Œ ì¶”ì¶œ
    - ê°€ê²©ë§Œ ì¶”ì¶œ  
    - ë³¸ë¬¸ë§Œ ì¶”ì¶œ
    - ë¦¬ë·°ë§Œ ì¶”ì¶œ
    - ê¸°íƒ€ íŠ¹ì • ë¶€ë¶„ë§Œ ì¶”ì¶œ
    
    Args:
        html_content: HTML ë‚´ìš© ë˜ëŠ” ë§ˆí¬ë‹¤ìš´ í…ìŠ¤íŠ¸
        target_content: ì¶”ì¶œí•  ì½˜í…ì¸  íƒ€ì… ("ì œëª©", "ê°€ê²©", "ë³¸ë¬¸", "ë¦¬ë·°" ë“±)
        url: ì›ë³¸ URL (ì„ íƒì‚¬í•­)
    
    Returns:
        ì„ íƒì ìœ¼ë¡œ ì¶”ì¶œëœ ì½˜í…ì¸  ë°ì´í„°
    """
    try:
        logger.info(f"ì„ íƒì  ì½˜í…ì¸  ì¶”ì¶œ ì‹œì‘: {target_content} from {url}")
        result = await content_extractor.extract_selective_content(
            html_content=html_content,
            target_content=target_content,
            url=url
        )
        return result
    except Exception as e:
        logger.error(f"ì„ íƒì  ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        return {
            "error": str(e),
            "target_content": target_content,
            "url": url,
            "status": "failed"
        }

if __name__ == "__main__":
    # MCP ì„œë²„ ì‹¤í–‰ (stdio ì „ì†¡ ë°©ì‹)
    mcp.run(transport="stdio") 