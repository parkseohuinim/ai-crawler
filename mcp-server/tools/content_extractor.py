"""
ì„ íƒì  ì½˜í…ì¸  ì¶”ì¶œ ë„êµ¬
íŠ¹ì • ë¶€ë¶„ë§Œ íƒ€ê²ŸíŒ…í•˜ì—¬ ì¶”ì¶œí•˜ëŠ” AI ê¸°ë°˜ ë„êµ¬
"""

import re
import logging
from typing import Dict, Any, Optional, List
from bs4 import BeautifulSoup
import asyncio

logger = logging.getLogger(__name__)

class ContentExtractor:
    """AI ê¸°ë°˜ ì„ íƒì  ì½˜í…ì¸  ì¶”ì¶œê¸°"""
    
    def __init__(self):
        # ì½˜í…ì¸  íƒ€ì…ë³„ ì¶”ì¶œ ì „ëµ
        self.extraction_strategies = {
            "ì œëª©": self._extract_title,
            "ê°€ê²©": self._extract_price,
            "ë³¸ë¬¸": self._extract_main_content,
            "ë¦¬ë·°": self._extract_reviews,
            "ìš”ì•½": self._extract_summary,
            "ì´ë¯¸ì§€": self._extract_images,
            "ë§í¬": self._extract_links,
            "ë‚ ì§œ": self._extract_date
        }
    
    async def extract_selective_content(
        self, 
        html_content: str, 
        target_content: str, 
        url: str = ""
    ) -> Dict[str, Any]:
        """
        ì„ íƒì  ì½˜í…ì¸  ì¶”ì¶œ
        
        Args:
            html_content: HTML ë‚´ìš© ë˜ëŠ” ë§ˆí¬ë‹¤ìš´ í…ìŠ¤íŠ¸
            target_content: ì¶”ì¶œí•  ì½˜í…ì¸  íƒ€ì… ("ì œëª©", "ê°€ê²©" ë“±)
            url: ì›ë³¸ URL (ì»¨í…ìŠ¤íŠ¸ ìš©)
            
        Returns:
            ì¶”ì¶œëœ ë°ì´í„° ë° ë©”íƒ€ì •ë³´
        """
        try:
            logger.info(f"ğŸ¯ ì„ íƒì  ì¶”ì¶œ ì‹œì‘: {target_content} from {url}")
            
            # HTML íŒŒì‹±
            if html_content.startswith('<'):
                soup = BeautifulSoup(html_content, 'html.parser')
                is_html = True
            else:
                # ë§ˆí¬ë‹¤ìš´ì´ë‚˜ í”Œë ˆì¸ í…ìŠ¤íŠ¸
                soup = None
                is_html = False
            
            # ì „ëµì— ë”°ë¥¸ ì¶”ì¶œ ì‹¤í–‰
            extraction_func = self.extraction_strategies.get(
                target_content, 
                self._extract_fallback
            )
            
            if is_html and soup:
                extracted_data = await extraction_func(soup, html_content, url)
            else:
                extracted_data = await self._extract_from_text(
                    html_content, target_content, url
                )
            
            # ê²°ê³¼ ê²€ì¦ ë° í’ˆì§ˆ í‰ê°€
            quality_score = self._calculate_extraction_quality(
                extracted_data, target_content
            )
            
            result = {
                "target_content": target_content,
                "extracted_data": extracted_data,
                "url": url,
                "extraction_method": "html" if is_html else "text",
                "quality_score": quality_score,
                "metadata": {
                    "extraction_time": "fast",
                    "confidence": self._calculate_confidence(extracted_data),
                    "source_type": "html" if is_html else "markdown/text"
                }
            }
            
            logger.info(f"âœ… ì„ íƒì  ì¶”ì¶œ ì™„ë£Œ: {target_content}, í’ˆì§ˆ: {quality_score:.1f}")
            return result
            
        except Exception as e:
            logger.error(f"âŒ ì„ íƒì  ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return {
                "target_content": target_content,
                "extracted_data": {},
                "error": str(e),
                "url": url,
                "quality_score": 0.0
            }
    
    async def _extract_title(self, soup: BeautifulSoup, html: str, url: str) -> Dict:
        """ì œëª© ì¶”ì¶œ"""
        titles = []
        
        # HTML title íƒœê·¸
        title_tag = soup.find('title')
        if title_tag:
            titles.append({
                "type": "page_title",
                "text": title_tag.get_text().strip(),
                "confidence": 0.9
            })
        
        # H1 íƒœê·¸ë“¤
        h1_tags = soup.find_all('h1')
        for h1 in h1_tags[:3]:  # ìƒìœ„ 3ê°œë§Œ
            text = h1.get_text().strip()
            if text and len(text) > 5:
                titles.append({
                    "type": "main_heading",
                    "text": text,
                    "confidence": 0.8
                })
        
        # ë©”íƒ€ og:title
        og_title = soup.find('meta', property='og:title')
        if og_title and og_title.get('content'):
            titles.append({
                "type": "og_title",
                "text": og_title['content'].strip(),
                "confidence": 0.7
            })
        
        # H2 íƒœê·¸ë“¤ (ë³´ì¡° ì œëª©)
        h2_tags = soup.find_all('h2')
        for h2 in h2_tags[:2]:  # ìƒìœ„ 2ê°œë§Œ
            text = h2.get_text().strip()
            if text and len(text) > 3:
                titles.append({
                    "type": "sub_heading",
                    "text": text,
                    "confidence": 0.6
                })
        
        return {
            "titles": titles,
            "primary_title": titles[0]["text"] if titles else "ì œëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
            "total_found": len(titles)
        }
    
    async def _extract_price(self, soup: BeautifulSoup, html: str, url: str) -> Dict:
        """ê°€ê²© ì •ë³´ ì¶”ì¶œ"""
        prices = []
        
        # ê°€ê²© íŒ¨í„´ë“¤
        price_patterns = [
            r'[\â‚©$Â¥â‚¬Â£]\s*[\d,]+(?:\.\d{2})?',  # í†µí™” ê¸°í˜¸ + ìˆ«ì
            r'[\d,]+\s*[ì›ë‹¬ëŸ¬ìœ ë¡œì—”å††]',         # ìˆ«ì + í†µí™” ë‹¨ìœ„
            r'[\d,]+(?:\.\d{2})?\s*[ì›ë‹¬ëŸ¬]',     # ì†Œìˆ˜ì  í¬í•¨
            r'price["\s:]*[\d,]+',              # price: 123456
            r'[\d,]+\s*WON',                    # ìˆ«ì + WON
        ]
        
        # HTML í…ìŠ¤íŠ¸ì—ì„œ ê°€ê²© íŒ¨í„´ ê²€ìƒ‰
        text_content = soup.get_text()
        
        for pattern in price_patterns:
            matches = re.findall(pattern, text_content, re.IGNORECASE)
            for match in matches:
                # ìˆ«ì ì¶”ì¶œ ë° ì •ê·œí™”
                numbers = re.findall(r'[\d,]+', match)
                if numbers:
                    price_value = numbers[0].replace(',', '')
                    if price_value.isdigit() and len(price_value) >= 3:
                        prices.append({
                            "raw_text": match,
                            "value": int(price_value),
                            "formatted": f"{int(price_value):,}ì›",
                            "confidence": 0.7
                        })
        
        # íŠ¹ì • í´ë˜ìŠ¤ë‚˜ IDì—ì„œ ê°€ê²© ê²€ìƒ‰
        price_selectors = [
            '[class*="price"]', '[id*="price"]',
            '[class*="cost"]', '[id*="cost"]',
            '[class*="amount"]', '.money', '.currency'
        ]
        
        for selector in price_selectors:
            elements = soup.select(selector)
            for elem in elements:
                text = elem.get_text().strip()
                numbers = re.findall(r'[\d,]+', text)
                if numbers:
                    price_value = numbers[0].replace(',', '')
                    if price_value.isdigit() and len(price_value) >= 3:
                        prices.append({
                            "raw_text": text,
                            "value": int(price_value),
                            "formatted": f"{int(price_value):,}ì›",
                            "confidence": 0.8,
                            "source": "css_selector"
                        })
        
        # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
        unique_prices = []
        seen_values = set()
        
        for price in sorted(prices, key=lambda x: x["confidence"], reverse=True):
            if price["value"] not in seen_values:
                unique_prices.append(price)
                seen_values.add(price["value"])
                
        return {
            "prices": unique_prices[:5],  # ìƒìœ„ 5ê°œë§Œ
            "primary_price": unique_prices[0] if unique_prices else None,
            "total_found": len(unique_prices),
            "price_range": {
                "min": min(p["value"] for p in unique_prices) if unique_prices else 0,
                "max": max(p["value"] for p in unique_prices) if unique_prices else 0
            }
        }
    
    async def _extract_main_content(self, soup: BeautifulSoup, html: str, url: str) -> Dict:
        """ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ"""
        content_parts = []
        
        # ì£¼ìš” ì½˜í…ì¸  ì˜ì—­ ìš°ì„ ìˆœìœ„
        content_selectors = [
            'main', 'article', '[role="main"]',
            '.content', '.post-content', '.article-content',
            '.entry-content', '.post-body', '.content-body'
        ]
        
        main_content = None
        for selector in content_selectors:
            element = soup.select_one(selector)
            if element:
                main_content = element
                break
        
        if not main_content:
            # bodyì—ì„œ script, style ì œê±° í›„ ì¶”ì¶œ
            main_content = soup.find('body') or soup
        
        # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
        for tag in main_content.find_all(['script', 'style', 'nav', 'header', 'footer', 'aside']):
            tag.decompose()
        
        # ë¬¸ë‹¨ë³„ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        paragraphs = main_content.find_all(['p', 'div'], string=True)
        
        for p in paragraphs:
            text = p.get_text().strip()
            if text and len(text) > 20:  # ì˜ë¯¸ìˆëŠ” ê¸¸ì´ì˜ í…ìŠ¤íŠ¸ë§Œ
                content_parts.append({
                    "text": text,
                    "tag": p.name,
                    "length": len(text)
                })
        
        # ì „ì²´ í…ìŠ¤íŠ¸
        full_text = main_content.get_text(separator='\n', strip=True)
        
        return {
            "paragraphs": content_parts[:10],  # ìƒìœ„ 10ê°œ ë¬¸ë‹¨
            "full_text": full_text[:2000],  # 2000ì ì œí•œ
            "total_paragraphs": len(content_parts),
            "total_length": len(full_text),
            "summary": full_text[:200] + "..." if len(full_text) > 200 else full_text
        }
    
    async def _extract_reviews(self, soup: BeautifulSoup, html: str, url: str) -> Dict:
        """ë¦¬ë·° ì •ë³´ ì¶”ì¶œ"""
        reviews = []
        
        # ë¦¬ë·° ê´€ë ¨ ì„ íƒìë“¤
        review_selectors = [
            '[class*="review"]', '[id*="review"]',
            '[class*="comment"]', '[id*="comment"]',
            '[class*="feedback"]', '.testimonial',
            '[data-testid*="review"]', '[role="review"]'
        ]
        
        for selector in review_selectors:
            elements = soup.select(selector)
            for elem in elements[:5]:  # ìµœëŒ€ 5ê°œ ë¦¬ë·°
                text = elem.get_text().strip()
                if text and len(text) > 10:
                    # í‰ì  ì¶”ì¶œ ì‹œë„
                    rating_match = re.search(r'(\d+(?:\.\d+)?)\s*[/â˜…â­ì stars]', text)
                    rating = float(rating_match.group(1)) if rating_match else None
                    
                    reviews.append({
                        "text": text[:300],  # 300ì ì œí•œ
                        "rating": rating,
                        "length": len(text),
                        "confidence": 0.7
                    })
        
        # í‰ì  íŒ¨í„´ ê²€ìƒ‰
        rating_patterns = [
            r'(\d+(?:\.\d+)?)\s*[/â˜…â­ì ]',
            r'(\d+(?:\.\d+)?)\s*stars?',
            r'í‰ì [:\s]*(\d+(?:\.\d+)?)'
        ]
        
        ratings = []
        for pattern in rating_patterns:
            matches = re.findall(pattern, soup.get_text(), re.IGNORECASE)
            for match in matches:
                try:
                    rating_value = float(match)
                    if 0 <= rating_value <= 5:
                        ratings.append(rating_value)
                except ValueError:
                    continue
        
        return {
            "reviews": reviews,
            "total_reviews": len(reviews),
            "ratings": ratings,
            "average_rating": sum(ratings) / len(ratings) if ratings else None,
            "rating_count": len(ratings)
        }
    
    async def _extract_summary(self, soup: BeautifulSoup, html: str, url: str) -> Dict:
        """ìš”ì•½ ì •ë³´ ì¶”ì¶œ"""
        summary_parts = []
        
        # ë©”íƒ€ ì„¤ëª…
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        if meta_desc and meta_desc.get('content'):
            summary_parts.append({
                "type": "meta_description",
                "text": meta_desc['content'].strip(),
                "confidence": 0.9
            })
        
        # og:description  
        og_desc = soup.find('meta', property='og:description')
        if og_desc and og_desc.get('content'):
            summary_parts.append({
                "type": "og_description", 
                "text": og_desc['content'].strip(),
                "confidence": 0.8
            })
        
        # ì²« ë²ˆì§¸ ë¬¸ë‹¨ (ìš”ì•½ìœ¼ë¡œ ì‚¬ìš©)
        first_p = soup.find('p')
        if first_p:
            text = first_p.get_text().strip()
            if text and len(text) > 50:
                summary_parts.append({
                    "type": "first_paragraph",
                    "text": text,
                    "confidence": 0.6
                })
        
        # abstractë‚˜ summary í´ë˜ìŠ¤
        summary_elements = soup.select('.summary, .abstract, .excerpt, .intro')
        for elem in summary_elements:
            text = elem.get_text().strip()
            if text and len(text) > 20:
                summary_parts.append({
                    "type": "summary_section",
                    "text": text,
                    "confidence": 0.7
                })
        
        return {
            "summaries": summary_parts,
            "primary_summary": summary_parts[0]["text"] if summary_parts else "ìš”ì•½ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
            "total_found": len(summary_parts)
        }
    
    async def _extract_images(self, soup: BeautifulSoup, html: str, url: str) -> Dict:
        """ì´ë¯¸ì§€ ì •ë³´ ì¶”ì¶œ"""
        images = []
        
        # ëª¨ë“  img íƒœê·¸ ìˆ˜ì§‘
        img_tags = soup.find_all('img')
        
        for img in img_tags[:10]:  # ìµœëŒ€ 10ê°œ ì´ë¯¸ì§€
            src = img.get('src', '')
            alt = img.get('alt', '')
            title = img.get('title', '')
            
            if src:
                # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                if src.startswith('/') and url:
                    from urllib.parse import urljoin
                    src = urljoin(url, src)
                
                images.append({
                    "src": src,
                    "alt": alt,
                    "title": title,
                    "width": img.get('width'),
                    "height": img.get('height'),
                    "confidence": 0.8 if alt or title else 0.6
                })
        
        # og:image
        og_image = soup.find('meta', property='og:image')
        if og_image and og_image.get('content'):
            images.insert(0, {
                "src": og_image['content'],
                "alt": "Open Graph Image",
                "title": "",
                "type": "og_image",
                "confidence": 0.9
            })
        
        return {
            "images": images,
            "total_images": len(images),
            "primary_image": images[0] if images else None
        }
    
    async def _extract_links(self, soup: BeautifulSoup, html: str, url: str) -> Dict:
        """ë§í¬ ì •ë³´ ì¶”ì¶œ"""
        links = []
        
        # ëª¨ë“  a íƒœê·¸ ìˆ˜ì§‘
        a_tags = soup.find_all('a', href=True)
        
        for a in a_tags[:20]:  # ìµœëŒ€ 20ê°œ ë§í¬
            href = a.get('href', '')
            text = a.get_text().strip()
            title = a.get('title', '')
            
            if href and text:
                # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                if href.startswith('/') and url:
                    from urllib.parse import urljoin
                    href = urljoin(url, href)
                
                # ì™¸ë¶€ ë§í¬ vs ë‚´ë¶€ ë§í¬ êµ¬ë¶„
                is_external = href.startswith('http') and url and url not in href
                
                links.append({
                    "href": href,
                    "text": text,
                    "title": title,
                    "is_external": is_external,
                    "confidence": 0.8
                })
        
        return {
            "links": links,
            "total_links": len(links),
            "external_links": [l for l in links if l.get("is_external")],
            "internal_links": [l for l in links if not l.get("is_external")]
        }
    
    async def _extract_date(self, soup: BeautifulSoup, html: str, url: str) -> Dict:
        """ë‚ ì§œ ì •ë³´ ì¶”ì¶œ"""
        dates = []
        
        # ë©”íƒ€ íƒœê·¸ì—ì„œ ë‚ ì§œ ì¶”ì¶œ
        date_meta_selectors = [
            'meta[name="date"]',
            'meta[property="article:published_time"]',
            'meta[property="article:modified_time"]', 
            'meta[name="publish_date"]',
            'meta[name="created"]'
        ]
        
        for selector in date_meta_selectors:
            meta = soup.select_one(selector)
            if meta and meta.get('content'):
                dates.append({
                    "type": "meta_date",
                    "raw_date": meta['content'],
                    "source": selector,
                    "confidence": 0.9
                })
        
        # time íƒœê·¸
        time_tags = soup.find_all('time')
        for time_tag in time_tags:
            datetime_attr = time_tag.get('datetime')
            text = time_tag.get_text().strip()
            
            if datetime_attr:
                dates.append({
                    "type": "time_tag",
                    "raw_date": datetime_attr,
                    "display_text": text,
                    "confidence": 0.8
                })
        
        # í…ìŠ¤íŠ¸ì—ì„œ ë‚ ì§œ íŒ¨í„´ ê²€ìƒ‰
        date_patterns = [
            r'\d{4}[-/.]\d{1,2}[-/.]\d{1,2}',  # YYYY-MM-DD
            r'\d{1,2}[-/.]\d{1,2}[-/.]\d{4}',  # MM-DD-YYYY
            r'\d{4}ë…„\s*\d{1,2}ì›”\s*\d{1,2}ì¼',  # í•œêµ­ì–´ ë‚ ì§œ
            r'\d{1,2}ì›”\s*\d{1,2}ì¼',  # ì›”ì¼ë§Œ
        ]
        
        text_content = soup.get_text()
        for pattern in date_patterns:
            matches = re.findall(pattern, text_content)
            for match in matches[:3]:  # ìµœëŒ€ 3ê°œ
                dates.append({
                    "type": "text_pattern",
                    "raw_date": match,
                    "confidence": 0.6
                })
        
        return {
            "dates": dates,
            "primary_date": dates[0] if dates else None,
            "total_found": len(dates)
        }
    
    async def _extract_from_text(self, text_content: str, target_content: str, url: str) -> Dict:
        """í…ìŠ¤íŠ¸(ë§ˆí¬ë‹¤ìš´)ì—ì„œ ì¶”ì¶œ"""
        if target_content == "ì œëª©":
            # ë§ˆí¬ë‹¤ìš´ í—¤ë” ì¶”ì¶œ
            headers = re.findall(r'^(#{1,3})\s+(.+)$', text_content, re.MULTILINE)
            titles = []
            
            for header_level, title_text in headers:
                titles.append({
                    "type": f"h{len(header_level)}",
                    "text": title_text.strip(),
                    "confidence": 0.9 - len(header_level) * 0.1
                })
            
            return {
                "titles": titles,
                "primary_title": titles[0]["text"] if titles else "ì œëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                "total_found": len(titles)
            }
        
        elif target_content == "ë³¸ë¬¸":
            # ì²« ë²ˆì§¸ í—¤ë” ì´í›„ì˜ ë‚´ìš©ì„ ë³¸ë¬¸ìœ¼ë¡œ ê°„ì£¼
            lines = text_content.split('\n')
            content_lines = []
            
            for line in lines:
                line = line.strip()
                if line and not line.startswith('#') and len(line) > 10:
                    content_lines.append(line)
            
            return {
                "paragraphs": [{"text": line, "length": len(line)} for line in content_lines[:10]],
                "full_text": '\n'.join(content_lines)[:2000],
                "total_paragraphs": len(content_lines)
            }
        
        # ê¸°ë³¸ í´ë°±
        return {"text": text_content[:500], "type": "fallback"}
    
    async def _extract_fallback(self, soup: BeautifulSoup, html: str, url: str) -> Dict:
        """í´ë°± ì¶”ì¶œ (ì „ì²´ í…ìŠ¤íŠ¸)"""
        return {
            "text": soup.get_text()[:1000] if soup else html[:1000],
            "type": "fallback_extraction",
            "message": f"'{self}' íƒ€ì…ì€ ì•„ì§ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."
        }
    
    def _calculate_extraction_quality(self, extracted_data: Dict, target_content: str) -> float:
        """ì¶”ì¶œ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        if not extracted_data or "error" in extracted_data:
            return 0.0
        
        base_score = 50.0
        
        if target_content == "ì œëª©":
            titles = extracted_data.get("titles", [])
            if titles:
                base_score += min(len(titles) * 10, 40)  # ì œëª© ê°œìˆ˜ì— ë”°ë¼ ì ìˆ˜
                if titles[0].get("confidence", 0) > 0.8:
                    base_score += 10
        
        elif target_content == "ê°€ê²©":
            prices = extracted_data.get("prices", [])
            if prices:
                base_score += min(len(prices) * 15, 45)  # ê°€ê²© ê°œìˆ˜ì— ë”°ë¼ ì ìˆ˜
                if prices[0] and prices[0].get("confidence", 0) > 0.7:
                    base_score += 5
        
        elif target_content == "ë³¸ë¬¸":
            paragraphs = extracted_data.get("paragraphs", [])
            total_length = extracted_data.get("total_length", 0)
            if paragraphs and total_length > 100:
                base_score += min(len(paragraphs) * 5, 30)
                base_score += min(total_length / 100, 20)
        
        return min(base_score, 100.0)
    
    def _calculate_confidence(self, extracted_data: Dict) -> float:
        """ì‹ ë¢°ë„ ê³„ì‚°"""
        if not extracted_data:
            return 0.0
        
        # ì¶”ì¶œëœ ë°ì´í„°ì˜ ì–‘ê³¼ ì§ˆì— ë”°ë¼ ì‹ ë¢°ë„ ê³„ì‚°
        if "titles" in extracted_data:
            titles = extracted_data["titles"]
            if titles:
                return sum(t.get("confidence", 0.5) for t in titles) / len(titles)
        
        if "prices" in extracted_data:
            prices = extracted_data["prices"]
            if prices:
                return sum(p.get("confidence", 0.5) for p in prices) / len(prices)
        
        if "paragraphs" in extracted_data:
            return 0.8 if len(extracted_data["paragraphs"]) > 3 else 0.6
        
        return 0.5  # ê¸°ë³¸ ì‹ ë¢°ë„ 