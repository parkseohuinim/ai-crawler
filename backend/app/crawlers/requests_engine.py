import logging
import asyncio
import aiohttp
from typing import Dict, Any
from datetime import datetime
from urllib.parse import urljoin, urlparse
import re

from .base import BaseCrawler, CrawlResult, CrawlStrategy, EngineCapabilities

logger = logging.getLogger(__name__)

try:
    from bs4 import BeautifulSoup
    BS4_AVAILABLE = True
except ImportError:
    BS4_AVAILABLE = False
    logger.warning("BeautifulSoup4 ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

class RequestsEngine(BaseCrawler):
    """Requests + BeautifulSoup ê¸°ë°˜ ë¹ ë¥¸ í¬ë¡¤ë§ ì—”ì§„"""
    
    def __init__(self):
        super().__init__("requests")
        self.session = None
    
    async def initialize(self) -> None:
        """HTTP ì„¸ì…˜ ì´ˆê¸°í™”"""
        if not BS4_AVAILABLE:
            raise RuntimeError("BeautifulSoup4 ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        # aiohttp ì„¸ì…˜ ìƒì„±
        timeout = aiohttp.ClientTimeout(total=30)
        self.session = aiohttp.ClientSession(
            timeout=timeout,
            headers={
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
                'Accept-Encoding': 'gzip, deflate, br',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
            }
        )
        
        self.is_initialized = True
        logger.info("ğŸŒ Requests ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def cleanup(self) -> None:
        """HTTP ì„¸ì…˜ ì •ë¦¬"""
        if self.session:
            await self.session.close()
        self.session = None
        self.is_initialized = False
        logger.info("ğŸŒ Requests ì—”ì§„ ì •ë¦¬ ì™„ë£Œ")
    
    def get_capabilities(self) -> Dict[str, Any]:
        """Requests ì—”ì§„ì˜ ëŠ¥ë ¥"""
        return {
            EngineCapabilities.JAVASCRIPT_RENDERING: False,
            EngineCapabilities.ANTI_BOT_BYPASS: False,
            EngineCapabilities.FAST_STATIC: True,
            EngineCapabilities.BULK_PROCESSING: True,
            "supported_formats": ["html", "text"],
            "rate_limits": "ë§¤ìš° ë‚®ìŒ (ì§ì ‘ HTTP ìš”ì²­)",
            "best_for": ["ì •ì  HTML", "ë¹ ë¥¸ ì²˜ë¦¬", "ëŒ€ëŸ‰ í¬ë¡¤ë§", "API ì—”ë“œí¬ì¸íŠ¸"]
        }
    
    def _extract_text_content(self, soup: BeautifulSoup) -> str:
        """HTMLì—ì„œ í…ìŠ¤íŠ¸ ë‚´ìš© ì¶”ì¶œ"""
        # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
        for tag in soup(["script", "style", "nav", "header", "footer", "aside", "advertisement"]):
            tag.decompose()
        
        # ì£¼ìš” ì½˜í…ì¸  ì˜ì—­ ì°¾ê¸°
        main_content = soup.find('main') or soup.find('article') or soup.find('div', class_=re.compile(r'content|main|article|post'))
        
        if main_content:
            text = main_content.get_text(separator='\n', strip=True)
        else:
            text = soup.get_text(separator='\n', strip=True)
        
        # í…ìŠ¤íŠ¸ ì •ë¦¬
        lines = text.split('\n')
        cleaned_lines = []
        
        for line in lines:
            line = line.strip()
            if line and len(line) > 2:  # ë„ˆë¬´ ì§§ì€ ì¤„ ì œì™¸
                cleaned_lines.append(line)
        
        return '\n'.join(cleaned_lines)
    
    def _extract_hierarchy_from_html(self, soup: BeautifulSoup, url: str) -> Dict[str, Any]:
        """HTMLì—ì„œ ê³„ì¸µêµ¬ì¡° ì¶”ì¶œ"""
        hierarchy = {"depth1": "ì›¹í˜ì´ì§€", "depth2": {}, "depth3": {}}
        
        # í˜ì´ì§€ ì œëª©ì„ depth1ìœ¼ë¡œ ì‚¬ìš©
        title_tag = soup.find('title')
        if title_tag:
            hierarchy["depth1"] = title_tag.get_text().strip()
        
        # í—¤ë” íƒœê·¸ë“¤ì—ì„œ ê³„ì¸µêµ¬ì¡° ì¶”ì¶œ
        headers = soup.find_all(['h1', 'h2', 'h3', 'h4'])
        
        current_h1 = None
        current_h2 = None
        
        for header in headers:
            text = header.get_text().strip()
            if not text:
                continue
            
            if header.name == 'h1':
                current_h1 = text
                if current_h1 not in hierarchy["depth2"]:
                    hierarchy["depth2"][current_h1] = []
                    
            elif header.name == 'h2':
                current_h2 = text
                if current_h1:
                    hierarchy["depth2"][current_h1].append(current_h2)
                else:
                    hierarchy["depth2"]["ê¸°íƒ€"] = hierarchy["depth2"].get("ê¸°íƒ€", [])
                    hierarchy["depth2"]["ê¸°íƒ€"].append(current_h2)
                    
            elif header.name in ['h3', 'h4']:
                depth3_key = current_h2 or current_h1 or "ê¸°íƒ€"
                if depth3_key not in hierarchy["depth3"]:
                    hierarchy["depth3"][depth3_key] = []
                hierarchy["depth3"][depth3_key].append(text)
        
        # ë„¤ë¹„ê²Œì´ì…˜ ë©”ë‰´ì—ì„œë„ êµ¬ì¡° ì¶”ì¶œ
        nav_elements = soup.find_all(['nav', 'ul', 'ol'], class_=re.compile(r'menu|nav|navigation'))
        for nav in nav_elements:
            links = nav.find_all('a')
            if len(links) > 2:  # ì˜ë¯¸ìˆëŠ” ë„¤ë¹„ê²Œì´ì…˜ìœ¼ë¡œ íŒë‹¨
                nav_items = [link.get_text().strip() for link in links if link.get_text().strip()]
                if nav_items:
                    hierarchy["depth2"]["ë„¤ë¹„ê²Œì´ì…˜"] = nav_items[:10]  # ìµœëŒ€ 10ê°œ
        
        return hierarchy
    
    def _calculate_quality_score(self, soup: BeautifulSoup, text_content: str, response_size: int) -> float:
        """í¬ë¡¤ë§ ê²°ê³¼ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        score = 40  # ê¸°ë³¸ ì„±ê³µ ì ìˆ˜
        
        # í…ìŠ¤íŠ¸ ê¸¸ì´ ì ìˆ˜ (0-25ì )
        text_length = len(text_content)
        if text_length > 3000:
            score += 25
        elif text_length > 1000:
            score += 15
        elif text_length > 300:
            score += 10
        elif text_length > 50:
            score += 5
        
        # HTML êµ¬ì¡° ì ìˆ˜ (0-20ì )
        structure_score = 0
        if soup.find('title'):
            structure_score += 3
        if soup.find_all(['h1', 'h2', 'h3']):
            structure_score += 5
        if soup.find_all('p'):
            structure_score += 4
        if soup.find_all('a'):
            structure_score += 3
        if soup.find(['main', 'article', 'section']):
            structure_score += 5
        score += structure_score
        
        # ë©”íƒ€ë°ì´í„° ì ìˆ˜ (0-10ì )
        meta_score = 0
        if soup.find('meta', attrs={'name': 'description'}):
            meta_score += 3
        if soup.find('meta', attrs={'name': 'keywords'}):
            meta_score += 2
        if soup.find('meta', attrs={'property': 'og:title'}):
            meta_score += 2
        if soup.find('meta', attrs={'property': 'og:description'}):
            meta_score += 3
        score += meta_score
        
        # ì‘ë‹µ í¬ê¸° ì ìˆ˜ (0-5ì )
        if response_size > 10000:
            score += 5
        elif response_size > 5000:
            score += 3
        elif response_size > 1000:
            score += 1
        
        return min(score, 100.0)
    
    async def _read_response_with_activity_timeout(self, response, activity_timeout: int, 
                                                  max_total_time: int, url: str) -> bytes:
        """
        í™œë™ ê¸°ë°˜ ì‘ë‹µ ì½ê¸° - ë‹¨ìˆœí™”ëœ ë²„ì „
        """
        import time
        
        content_chunks = []
        total_size = 0
        start_time = time.time()
        last_chunk_time = start_time
        
        logger.info(f"ğŸ“¡ í™œë™ ê¸°ë°˜ ì½ê¸° ì‹œì‘: {url} (í™œë™íƒ€ì„ì•„ì›ƒ: {activity_timeout}s, ìµœëŒ€: {max_total_time}s)")
        
        try:
            # ì²­í¬ ë‹¨ìœ„ë¡œ ì½ê¸° (í•œ ë²ˆë§Œ!)
            async for chunk in response.content.iter_chunked(8192):  # 8KB ì²­í¬
                if not chunk:
                    break
                    
                content_chunks.append(chunk)
                total_size += len(chunk)
                last_chunk_time = time.time()
                
                # ì§„í–‰ ìƒí™© ë¡œê¹… (500KBë§ˆë‹¤)
                if total_size % (500 * 1024) < 8192:  # ì²­í¬ í¬ê¸° ê³ ë ¤
                    elapsed = time.time() - start_time
                    logger.info(f"ğŸ“Š ì½ê¸° ì§„í–‰ì¤‘: {total_size/1024:.0f}KB ({elapsed:.1f}s ê²½ê³¼)")
                
                # ìµœëŒ€ ì´ ì‹œê°„ ì´ˆê³¼ ì²´í¬ (ì•ˆì „ì¥ì¹˜)
                if time.time() - start_time > max_total_time:
                    logger.warning(f"âš ï¸ ìµœëŒ€ ì´ ì‹œê°„ ì´ˆê³¼ ({max_total_time}s), í˜„ì¬ê¹Œì§€ ì½ì€ ë°ì´í„° ë°˜í™˜")
                    break
                    
                # ì²­í¬ ê°„ ê°„ê²©ì´ ë„ˆë¬´ ê¸¸ë©´ ì¤‘ë‹¨
                time_since_last_chunk = time.time() - last_chunk_time
                if time_since_last_chunk > activity_timeout:
                    logger.warning(f"âš ï¸ ì²­í¬ ê°„ ê°„ê²© ì´ˆê³¼ ({time_since_last_chunk:.1f}s > {activity_timeout}s)")
                    break
            
            # ì™„ë£Œ
            total_time = time.time() - start_time
            logger.info(f"âœ… í™œë™ ê¸°ë°˜ ì½ê¸° ì™„ë£Œ: {total_size/1024:.1f}KB, {total_time:.1f}s ì†Œìš”")
            
            return b''.join(content_chunks)
            
        except Exception as e:
            total_time = time.time() - start_time  
            logger.error(f"âŒ í™œë™ ê¸°ë°˜ ì½ê¸° ì‹¤íŒ¨: {e} ({total_time:.1f}s ê²½ê³¼, {total_size/1024:.1f}KB ì½ìŒ)")
            
            # ë¶€ë¶„ì ìœ¼ë¡œë¼ë„ ì½ì€ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ë°˜í™˜
            if content_chunks:
                logger.info(f"ğŸ”„ ë¶€ë¶„ ë°ì´í„° ë°˜í™˜: {total_size/1024:.1f}KB")
                return b''.join(content_chunks)
            else:
                raise
    
    async def crawl(self, url: str, strategy: CrawlStrategy) -> CrawlResult:
        """Requestsë¥¼ ì‚¬ìš©í•œ ì›¹í˜ì´ì§€ í¬ë¡¤ë§ (í™œë™ ê¸°ë°˜ íƒ€ì„ì•„ì›ƒ)"""
        if not self.is_initialized or not self.session:
            raise RuntimeError("Requests ì—”ì§„ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        logger.info(f"ğŸŒ Requestsë¡œ í¬ë¡¤ë§ ì‹œì‘: {url}")
        
        try:
            # ì´ˆê¸° ì—°ê²° íƒ€ì„ì•„ì›ƒ (ë¹ ë¥´ê²Œ)
            connector_timeout = aiohttp.ClientTimeout(total=strategy.timeout, connect=10)
            
            async with self.session.get(url, timeout=connector_timeout) as response:
                # ìƒíƒœ ì½”ë“œ í™•ì¸
                if response.status >= 400:
                    raise Exception(f"HTTP {response.status}: {response.reason}")
                
                # Content-Type í™•ì¸
                content_type = response.headers.get('content-type', '').lower()
                if 'text/html' not in content_type and 'application/xml' not in content_type:
                    logger.warning(f"âš ï¸ ì˜ˆìƒì¹˜ ëª»í•œ Content-Type: {content_type}")
                
                # ğŸ”¥ í™œë™ ê¸°ë°˜ ìŠ¤íŠ¸ë¦¬ë° ì½ê¸°
                content = await self._read_response_with_activity_timeout(
                    response, strategy.activity_timeout, strategy.max_total_time, url
                )
                
                # ì¸ì½”ë”© ì²˜ë¦¬
                encoding = response.charset or 'utf-8'
                try:
                    html_content = content.decode(encoding)
                except UnicodeDecodeError:
                    html_content = content.decode('utf-8', errors='ignore')
                
                # BeautifulSoupë¡œ íŒŒì‹±
                soup = BeautifulSoup(html_content, 'html.parser')
                
                # ì œëª© ì¶”ì¶œ
                title_tag = soup.find('title')
                title = title_tag.get_text().strip() if title_tag else "ì œëª© ì—†ìŒ"
                
                # í…ìŠ¤íŠ¸ ë‚´ìš© ì¶”ì¶œ
                text_content = self._extract_text_content(soup)
                
                # ê³„ì¸µêµ¬ì¡° ì¶”ì¶œ
                hierarchy = self._extract_hierarchy_from_html(soup, url)
                
                # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
                quality_score = self._calculate_quality_score(soup, text_content, len(content))
                
                # ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘
                meta_description = soup.find('meta', attrs={'name': 'description'})
                meta_keywords = soup.find('meta', attrs={'name': 'keywords'})
                og_title = soup.find('meta', attrs={'property': 'og:title'})
                og_description = soup.find('meta', attrs={'property': 'og:description'})
                
                # ê²°ê³¼ ê°ì²´ ìƒì„±
                crawl_result = CrawlResult(
                    url=url,
                    title=title,
                    text=text_content,
                    hierarchy=hierarchy,
                    metadata={
                        "crawler_used": "requests",
                        "processing_time": "í™œë™ê¸°ë°˜",
                        "content_quality": "high" if quality_score > 80 else "medium" if quality_score > 50 else "low",
                        "extraction_confidence": quality_score / 100,
                        "http_status": response.status,
                        "content_type": content_type,
                        "content_length": len(content),
                        "text_length": len(text_content),
                        "quality_score": quality_score,
                        "timeout_strategy": "activity_based",
                        "meta_description": meta_description.get('content') if meta_description else None,
                        "meta_keywords": meta_keywords.get('content') if meta_keywords else None,
                        "og_title": og_title.get('content') if og_title else None,
                        "og_description": og_description.get('content') if og_description else None,
                    },
                    status="complete",
                    timestamp=datetime.now()
                )
                
                logger.info(f"âœ… Requests í¬ë¡¤ë§ ì„±ê³µ: {url} (í’ˆì§ˆ: {quality_score:.1f}/100, í¬ê¸°: {len(content)/1024:.1f}KB)")
                return crawl_result
                
        except asyncio.TimeoutError:
            error_msg = f"ì—°ê²° ì‹œê°„ ì´ˆê³¼ ({strategy.timeout}ì´ˆ)"
            logger.error(f"â° {error_msg}: {url}")
        except Exception as e:
            error_msg = str(e)
            logger.error(f"âŒ Requests í¬ë¡¤ë§ ì‹¤íŒ¨: {url} - {error_msg}")
        
        # ì‹¤íŒ¨ ì‹œ ê²°ê³¼
        return CrawlResult(
            url=url,
            title="",
            text="",
            hierarchy={},
            metadata={
                "crawler_used": "requests",
                "error_type": type(e).__name__ if 'e' in locals() else "TimeoutError",
                "processing_time": "0s"
            },
            status="failed",
            timestamp=datetime.now(),
            error=error_msg if 'error_msg' in locals() else "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜"
        ) 