from abc import ABC, abstractmethod
from typing import Dict, Any, Optional, List
from dataclasses import dataclass
from datetime import datetime
import asyncio
import logging

logger = logging.getLogger(__name__)

@dataclass
class CrawlResult:
    """í¬ë¡¤ë§ ê²°ê³¼ í‘œì¤€ ë°ì´í„° í´ë˜ìŠ¤"""
    url: str
    title: str
    text: str
    hierarchy: Dict[str, Any]
    metadata: Dict[str, Any]
    status: str
    timestamp: datetime
    error: Optional[str] = None

@dataclass
class CrawlStrategy:
    """í¬ë¡¤ë§ ì „ëµ ì„¤ì •"""
    engine_priority: List[str]
    timeout: int = 30  # ì´ˆê¸° ì—°ê²° íƒ€ì„ì•„ì›ƒ
    max_retries: int = 3
    wait_time: float = 1.0
    extract_images: bool = False
    extract_links: bool = True
    custom_selectors: Dict[str, str] = None
    anti_bot_mode: bool = False
    # í™œë™ ê¸°ë°˜ íƒ€ì„ì•„ì›ƒ ì„¤ì •
    activity_timeout: int = 15  # ë§ˆì§€ë§‰ í™œë™ìœ¼ë¡œë¶€í„° 15ì´ˆ í›„ íƒ€ì„ì•„ì›ƒ
    max_total_time: int = 300   # ìµœëŒ€ ì´ ì‹œê°„ 5ë¶„ (ì•ˆì „ì¥ì¹˜)
    
    def __post_init__(self):
        if self.custom_selectors is None:
            self.custom_selectors = {}

class BaseCrawler(ABC):
    """í¬ë¡¤ë§ ì—”ì§„ ë² ì´ìŠ¤ í´ë˜ìŠ¤"""
    
    def __init__(self, name: str):
        self.name = name
        self.is_initialized = False
        self.stats = {
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "avg_response_time": 0.0
        }
    
    @abstractmethod
    async def initialize(self) -> None:
        """ì—”ì§„ ì´ˆê¸°í™” (API í‚¤ ì„¤ì •, ë¸Œë¼ìš°ì € ì‹œì‘ ë“±)"""
        pass
    
    @abstractmethod
    async def crawl(self, url: str, strategy: CrawlStrategy) -> CrawlResult:
        """ì›¹í˜ì´ì§€ í¬ë¡¤ë§ ì‹¤í–‰"""
        pass
    
    @abstractmethod
    async def cleanup(self) -> None:
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬ (ë¸Œë¼ìš°ì € ì¢…ë£Œ, ì—°ê²° ë‹«ê¸° ë“±)"""
        pass
    
    @abstractmethod
    def get_capabilities(self) -> Dict[str, Any]:
        """ì—”ì§„ì˜ ëŠ¥ë ¥ê³¼ íŠ¹ì„± ë°˜í™˜"""
        pass
    
    async def health_check(self) -> Dict[str, Any]:
        """ì—”ì§„ ìƒíƒœ í™•ì¸"""
        return {
            "name": self.name,
            "initialized": self.is_initialized,
            "stats": self.stats,
            "capabilities": self.get_capabilities()
        }
    
    def _update_stats(self, success: bool, response_time: float):
        """í†µê³„ ì—…ë°ì´íŠ¸"""
        self.stats["total_requests"] += 1
        if success:
            self.stats["successful_requests"] += 1
        else:
            self.stats["failed_requests"] += 1
        
        # í‰ê·  ì‘ë‹µ ì‹œê°„ ê³„ì‚°
        total = self.stats["total_requests"]
        current_avg = self.stats["avg_response_time"]
        self.stats["avg_response_time"] = ((current_avg * (total - 1)) + response_time) / total
    
    async def crawl_with_retry(self, url: str, strategy: CrawlStrategy) -> CrawlResult:
        """ì¬ì‹œë„ ë¡œì§ì´ í¬í•¨ëœ í¬ë¡¤ë§"""
        last_error = None
        
        for attempt in range(strategy.max_retries):
            try:
                start_time = asyncio.get_event_loop().time()
                result = await self.crawl(url, strategy)
                end_time = asyncio.get_event_loop().time()
                
                self._update_stats(True, end_time - start_time)
                return result
                
            except Exception as e:
                last_error = e
                error_msg = str(e).lower()
                
                # ì¬ì‹œë„í•˜ì§€ ì•Šì„ ì—ëŸ¬ ìœ í˜•ë“¤
                permanent_errors = [
                    "404", "not found",  # HTTP 404
                    "403", "forbidden",  # ì ‘ê·¼ ê¸ˆì§€
                    "dns", "name resolution failed",  # DNS ì˜¤ë¥˜
                    "connection refused",  # ì—°ê²° ê±°ë¶€
                    "invalid url", "malformed url",  # ì˜ëª»ëœ URL
                    "ssl certificate", "certificate verify failed"  # SSL ì¸ì¦ì„œ ì˜¤ë¥˜
                ]
                
                # ì˜êµ¬ì  ì—ëŸ¬ì¸ì§€ í™•ì¸
                is_permanent = any(err in error_msg for err in permanent_errors)
                
                if is_permanent:
                    logger.warning(f"ğŸš« ì˜êµ¬ì  ì—ëŸ¬ ê°ì§€, ì¬ì‹œë„ ê±´ë„ˆëœ€: {type(e).__name__}: {e}")
                    break
                
                if attempt < strategy.max_retries - 1:
                    wait_time = strategy.wait_time * (2 ** attempt)  # ì§€ìˆ˜ ë°±ì˜¤í”„
                    logger.info(f"ğŸ”„ ì¬ì‹œë„ {attempt + 2}/{strategy.max_retries} ({wait_time:.1f}ì´ˆ í›„): {type(e).__name__}")
                    await asyncio.sleep(wait_time)
                    continue
        
        # ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨
        self._update_stats(False, 0)
        return CrawlResult(
            url=url,
            title="",
            text="",
            hierarchy={},
            metadata={"crawler_used": self.name, "error": str(last_error)},
            status="failed",
            timestamp=datetime.now(),
            error=str(last_error)
        )

class EngineCapabilities:
    """ì—”ì§„ ëŠ¥ë ¥ ìƒìˆ˜"""
    JAVASCRIPT_RENDERING = "javascript_rendering"
    ANTI_BOT_BYPASS = "anti_bot_bypass"
    BULK_PROCESSING = "bulk_processing"
    AI_EXTRACTION = "ai_extraction"
    FAST_STATIC = "fast_static"
    PREMIUM_SERVICE = "premium_service"
    INFINITE_SCROLL = "infinite_scroll"
    LOGIN_SUPPORT = "login_support" 