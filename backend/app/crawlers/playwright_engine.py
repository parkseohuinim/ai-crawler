import os
import logging
from typing import Dict, Any, List, Optional
from datetime import datetime
import asyncio
import json
from urllib.parse import urljoin, urlparse

from .base import BaseCrawler, CrawlResult, CrawlStrategy, EngineCapabilities

logger = logging.getLogger(__name__)

try:
    from playwright.async_api import async_playwright, Browser, BrowserContext, Page
    from playwright._impl._errors import Error as PlaywrightError, TimeoutError as PlaywrightTimeoutError
    PLAYWRIGHT_AVAILABLE = True
except ImportError:
    PLAYWRIGHT_AVAILABLE = False
    logger.warning("Playwright ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

class PlaywrightEngine(BaseCrawler):
    """Playwright ê¸°ë°˜ í¬ë¡¤ë§ ì—”ì§„ - ë¸Œë¼ìš°ì € ìë™í™” ê¸°ë°˜ ê³ ê¸‰ í¬ë¡¤ë§"""
    
    def __init__(self):
        super().__init__("playwright")
        self.playwright = None
        self.browser = None
        self.context = None
    
    async def initialize(self) -> None:
        """Playwright ë¸Œë¼ìš°ì € ì´ˆê¸°í™”"""
        if not PLAYWRIGHT_AVAILABLE:
            raise RuntimeError("Playwright ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        try:
            # Playwright ì‹œì‘
            self.playwright = await async_playwright().start()
            
            # ë¸Œë¼ìš°ì € ì‹œì‘ (Chromium ì‚¬ìš©)
            self.browser = await self.playwright.chromium.launch(
                headless=True,
                args=[
                    '--no-sandbox',
                    '--disable-setuid-sandbox',
                    '--disable-dev-shm-usage',
                    '--disable-accelerated-2d-canvas',
                    '--disable-gpu',
                    '--window-size=1920,1080'
                ]
            )
            
            # ë¸Œë¼ìš°ì € ì»¨í…ìŠ¤íŠ¸ ìƒì„±
            self.context = await self.browser.new_context(
                viewport={'width': 1920, 'height': 1080},
                user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                # ì•ˆí‹°ë´‡ ì„¤ì •
                java_script_enabled=True,
                bypass_csp=True,
                # ì¶”ê°€ í—¤ë”
                extra_http_headers={
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
                    'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
                    'Accept-Encoding': 'gzip, deflate, br',
                    'DNT': '1',
                    'Connection': 'keep-alive',
                    'Upgrade-Insecure-Requests': '1',
                }
            )
            
            self.is_initialized = True
            logger.info("ğŸ­ Playwright ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"Playwright ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            await self.cleanup()
            raise
    
    async def cleanup(self) -> None:
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            if self.context:
                await self.context.close()
                logger.info("ğŸ­ Playwright ì»¨í…ìŠ¤íŠ¸ ì¢…ë£Œ ì™„ë£Œ")
                
            if self.browser:
                await self.browser.close()
                logger.info("ğŸ­ Playwright ë¸Œë¼ìš°ì € ì¢…ë£Œ ì™„ë£Œ")
                
            if self.playwright:
                await self.playwright.stop()
                logger.info("ğŸ­ Playwright ì¢…ë£Œ ì™„ë£Œ")
                
        except Exception as e:
            logger.error(f"Playwright ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        finally:
            self.context = None
            self.browser = None
            self.playwright = None
            self.is_initialized = False
            
        logger.info("ğŸ­ Playwright ì—”ì§„ ì •ë¦¬ ì™„ë£Œ")
    
    def get_capabilities(self) -> Dict[str, Any]:
        """Playwright ì—”ì§„ì˜ ëŠ¥ë ¥"""
        return {
            EngineCapabilities.JAVASCRIPT_RENDERING: True,
            EngineCapabilities.ANTI_BOT_BYPASS: True,
            EngineCapabilities.PREMIUM_SERVICE: False,  # ì˜¤í”ˆì†ŒìŠ¤
            EngineCapabilities.INFINITE_SCROLL: True,
            EngineCapabilities.BULK_PROCESSING: True,
            "supported_formats": ["markdown", "html", "screenshot", "pdf"],
            "interaction_features": ["click", "scroll", "form_fill", "wait_for_elements"],
            "browser_features": ["full_js_execution", "network_interception", "cookie_management"],
            "rate_limits": "ë¸Œë¼ìš°ì € ê¸°ë°˜ (ë¬´ì œí•œ)",
            "best_for": ["ë³µì¡í•œ SPA", "ì¸í„°ë™ì…˜ í•„ìš” ì‚¬ì´íŠ¸", "ì•ˆí‹°ë´‡ ìš°íšŒ", "ìŠ¤í¬ë¦°ìƒ· í•„ìš”"]
        }
    
    async def _wait_for_content_load(self, page: Page, strategy: CrawlStrategy) -> None:
        """í™œë™ ê¸°ë°˜ ì½˜í…ì¸  ë¡œë”© ëŒ€ê¸°"""
        import time
        
        start_time = time.time()
        logger.info(f"ğŸ­ í™œë™ ê¸°ë°˜ í˜ì´ì§€ ë¡œë”© ì‹œì‘ (ìµœëŒ€: {strategy.max_total_time}s)")
        
        try:
            # 1ë‹¨ê³„: ê¸°ë³¸ DOM ë¡œë”© ëŒ€ê¸° (ë¹ ë¥¸ íƒ€ì„ì•„ì›ƒ)
            await page.wait_for_load_state('domcontentloaded', timeout=strategy.timeout * 1000)
            
            # 2ë‹¨ê³„: í™œë™ ê¸°ë°˜ ì™„ë£Œ ëŒ€ê¸°
            await self._wait_for_loading_activity(page, strategy, start_time)
                
        except PlaywrightTimeoutError:
            elapsed = time.time() - start_time
            logger.warning(f"âš ï¸ ê¸°ë³¸ ë¡œë”© íƒ€ì„ì•„ì›ƒ ({elapsed:.1f}s) - í˜„ì¬ ìƒíƒœë¡œ ì§„í–‰")
    
    async def _wait_for_loading_activity(self, page: Page, strategy: CrawlStrategy, start_time: float) -> None:
        """ë¡œë”© í™œë™ì´ ì™„ë£Œë  ë•Œê¹Œì§€ ëŒ€ê¸°"""
        import time
        
        last_activity_time = time.time()
        content_size_history = []
        network_idle_count = 0
        
        logger.info(f"ğŸ“Š ë¡œë”© í™œë™ ëª¨ë‹ˆí„°ë§ ì‹œì‘...")
        
        while True:
            current_time = time.time()
            
            # ìµœëŒ€ ì‹œê°„ ì´ˆê³¼ ì²´í¬
            if current_time - start_time > strategy.max_total_time:
                logger.warning(f"â° ìµœëŒ€ ì‹œê°„ ì´ˆê³¼ ({strategy.max_total_time}s) - ê°•ì œ ì™„ë£Œ")
                break
            
            try:
                # í˜„ì¬ ì½˜í…ì¸  í¬ê¸° ì¸¡ì •
                content_size = await page.evaluate("""
                    () => {
                        const html = document.documentElement.outerHTML;
                        const scripts = document.querySelectorAll('script').length;
                        const images = document.querySelectorAll('img').length;
                        return {
                            htmlSize: html.length,
                            scriptCount: scripts,
                            imageCount: images,
                            readyState: document.readyState
                        };
                    }
                """)
                
                content_size_history.append(content_size)
                
                # ìµœê·¼ í™œë™ í™•ì¸ (ì§€ë‚œ 3ì´ˆê°„ ë³€í™” í™•ì¸)
                if len(content_size_history) > 3:
                    recent_sizes = content_size_history[-3:]
                    size_changes = [
                        abs(recent_sizes[i]['htmlSize'] - recent_sizes[i-1]['htmlSize']) 
                        for i in range(1, len(recent_sizes))
                    ]
                    
                    # ë³€í™”ê°€ ìˆìœ¼ë©´ í™œë™ìœ¼ë¡œ ê°„ì£¼
                    if any(change > 1000 for change in size_changes):  # 1KB ì´ìƒ ë³€í™”
                        last_activity_time = current_time
                        network_idle_count = 0
                        logger.debug(f"ğŸ“ˆ ì½˜í…ì¸  ë³€í™” ê°ì§€: {max(size_changes)/1024:.1f}KB")
                    else:
                        network_idle_count += 1
                
                # ë„¤íŠ¸ì›Œí¬ ìƒíƒœ í™•ì¸
                try:
                    await page.wait_for_load_state('networkidle', timeout=1000)
                    network_idle_count += 1
                except PlaywrightTimeoutError:
                    # ë„¤íŠ¸ì›Œí¬ í™œë™ì´ ìˆìŒ
                    last_activity_time = current_time
                    network_idle_count = 0
                
                # ì™„ë£Œ ì¡°ê±´ í™•ì¸
                idle_time = current_time - last_activity_time
                if (idle_time > strategy.activity_timeout and 
                    network_idle_count >= 3 and 
                    content_size['readyState'] == 'complete'):
                    
                    elapsed = current_time - start_time
                    logger.info(f"âœ… ë¡œë”© ì™„ë£Œ: {elapsed:.1f}s, í¬ê¸°: {content_size['htmlSize']/1024:.1f}KB")
                    break
                
                # ìƒíƒœ ë¡œê¹…
                if int(current_time) % 5 == 0:  # 5ì´ˆë§ˆë‹¤
                    elapsed = current_time - start_time
                    logger.debug(f"â³ ë¡œë”© ì¤‘: {elapsed:.0f}s, ìœ íœ´: {idle_time:.1f}s, í¬ê¸°: {content_size['htmlSize']/1024:.0f}KB")
                
                await asyncio.sleep(1)  # 1ì´ˆ ê°„ê²©ìœ¼ë¡œ ì²´í¬
                
            except Exception as e:
                logger.debug(f"í™œë™ ëª¨ë‹ˆí„°ë§ ì¤‘ ì˜¤ë¥˜: {e}")
                await asyncio.sleep(1)
                
        total_time = time.time() - start_time
        logger.info(f"ğŸ í™œë™ ê¸°ë°˜ ë¡œë”© ì™„ë£Œ: {total_time:.1f}s")
    
    async def _handle_infinite_scroll(self, page: Page) -> None:
        """ë¬´í•œìŠ¤í¬ë¡¤ ì²˜ë¦¬"""
        try:
            # ìŠ¤í¬ë¡¤ ë‹¤ìš´ì„ ëª‡ ë²ˆ ì‹œë„
            for i in range(3):
                # í˜„ì¬ í˜ì´ì§€ ë†’ì´ ê°€ì ¸ì˜¤ê¸°
                previous_height = await page.evaluate("document.body.scrollHeight")
                
                # í˜ì´ì§€ ëê¹Œì§€ ìŠ¤í¬ë¡¤
                await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                
                # ìƒˆ ì½˜í…ì¸  ë¡œë”© ëŒ€ê¸°
                await page.wait_for_timeout(1500)
                
                # ìƒˆë¡œìš´ ë†’ì´ í™•ì¸
                new_height = await page.evaluate("document.body.scrollHeight")
                
                # ë” ì´ìƒ ë¡œë“œí•  ì½˜í…ì¸ ê°€ ì—†ìœ¼ë©´ ì¤‘ë‹¨
                if new_height == previous_height:
                    break
                    
                logger.info(f"ğŸ­ ìŠ¤í¬ë¡¤ ì§„í–‰ ì¤‘: {i+1}/3 (ë†’ì´: {previous_height} â†’ {new_height})")
            
            # ë§¨ ìœ„ë¡œ ìŠ¤í¬ë¡¤
            await page.evaluate("window.scrollTo(0, 0)")
            await page.wait_for_timeout(500)
            
        except Exception as e:
            logger.warning(f"ë¬´í•œìŠ¤í¬ë¡¤ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    async def _extract_content(self, page: Page, url: str) -> Dict[str, Any]:
        """í˜ì´ì§€ì—ì„œ ì½˜í…ì¸  ì¶”ì¶œ"""
        try:
            # ê¸°ë³¸ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            title = await page.title()
            
            # ë©”íƒ€ íƒœê·¸ ì •ë³´ ì¶”ì¶œ (íƒ€ì„ì•„ì›ƒ ë°©ì§€)
            try:
                meta_description = await page.get_attribute('meta[name="description"]', 'content', timeout=5000) or ""
            except:
                meta_description = ""
                
            try:
                meta_keywords = await page.get_attribute('meta[name="keywords"]', 'content', timeout=5000) or ""
            except:
                meta_keywords = ""
                
            try:
                og_title = await page.get_attribute('meta[property="og:title"]', 'content', timeout=5000) or ""
            except:
                og_title = ""
                
            try:
                og_description = await page.get_attribute('meta[property="og:description"]', 'content', timeout=5000) or ""
            except:
                og_description = ""
            
            # HTML ì½˜í…ì¸  ì¶”ì¶œ
            html_content = await page.content()
            
            # ë©”ì¸ ì½˜í…ì¸  ì¶”ì¶œ (ì—¬ëŸ¬ ì„ íƒì ì‹œë„)
            main_content_selectors = [
                'main',
                'article', 
                '.content',
                '.main-content',
                '.post-content',
                '.entry-content',
                '[role="main"]',
                'body'  # ë§ˆì§€ë§‰ ì˜µì…˜
            ]
            
            main_text = ""
            for selector in main_content_selectors:
                try:
                    element = await page.query_selector(selector)
                    if element:
                        main_text = await element.inner_text()
                        if len(main_text.strip()) > 100:  # ì¶©ë¶„í•œ í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ì‚¬ìš©
                            break
                except Exception:
                    continue
            
            # ì œëª© êµ¬ì¡° ì¶”ì¶œ (ë§ˆí¬ë‹¤ìš´ ë³€í™˜ìš©)
            headings = []
            for level in range(1, 7):  # h1 to h6
                h_elements = await page.query_selector_all(f'h{level}')
                for element in h_elements:
                    text = await element.inner_text()
                    if text.strip():
                        headings.append({
                            'level': level,
                            'text': text.strip()
                        })
            
            # ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            markdown_content = self._convert_to_markdown(title, main_text, headings)
            
            return {
                'title': title,
                'html': html_content,
                'text': main_text,
                'markdown': markdown_content,
                'metadata': {
                    'title': title,
                    'description': meta_description,
                    'keywords': meta_keywords,
                    'og_title': og_title,
                    'og_description': og_description,
                    'url': url
                },
                'headings': headings
            }
            
        except Exception as e:
            logger.error(f"ì½˜í…ì¸  ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
            raise
    
    def _convert_to_markdown(self, title: str, text: str, headings: List[Dict]) -> str:
        """í…ìŠ¤íŠ¸ë¥¼ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        markdown_lines = []
        
        # ì œëª© ì¶”ê°€
        if title:
            markdown_lines.append(f"# {title}\n")
        
        # í—¤ë”© êµ¬ì¡°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë§ˆí¬ë‹¤ìš´ ìƒì„±
        if headings:
            current_text = text
            for heading in headings:
                level = heading['level']
                heading_text = heading['text']
                markdown_prefix = '#' * level
                markdown_lines.append(f"{markdown_prefix} {heading_text}\n")
        else:
            # í—¤ë”©ì´ ì—†ìœ¼ë©´ í…ìŠ¤íŠ¸ë¥¼ ë‹¨ë½ìœ¼ë¡œ ë¶„í• 
            paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
            for para in paragraphs:
                if len(para) > 20:  # ì¶©ë¶„í•œ ê¸¸ì´ì˜ ë‹¨ë½ë§Œ í¬í•¨
                    markdown_lines.append(f"{para}\n")
        
        return '\n'.join(markdown_lines)
    
    def _extract_hierarchy_from_headings(self, headings: List[Dict], url: str) -> Dict[str, Any]:
        """í—¤ë”© ë¦¬ìŠ¤íŠ¸ì—ì„œ ê³„ì¸µêµ¬ì¡° ì¶”ì¶œ"""
        hierarchy = {"depth1": "ì›¹í˜ì´ì§€", "depth2": {}, "depth3": {}}
        
        if not headings:
            return hierarchy
        
        current_h1 = None
        current_h2 = None
        
        for heading in headings:
            level = heading['level']
            text = heading['text']
            
            if level == 1:
                current_h1 = text
                hierarchy["depth1"] = current_h1
                if current_h1 not in hierarchy["depth2"]:
                    hierarchy["depth2"][current_h1] = []
                    
            elif level == 2:
                current_h2 = text
                if current_h1:
                    if current_h1 not in hierarchy["depth2"]:
                        hierarchy["depth2"][current_h1] = []
                    hierarchy["depth2"][current_h1].append(current_h2)
                else:
                    hierarchy["depth2"]["ê¸°íƒ€"] = hierarchy["depth2"].get("ê¸°íƒ€", [])
                    hierarchy["depth2"]["ê¸°íƒ€"].append(current_h2)
                    
            elif level == 3:
                depth3_key = current_h2 or current_h1 or "ê¸°íƒ€"
                if depth3_key not in hierarchy["depth3"]:
                    hierarchy["depth3"][depth3_key] = []
                hierarchy["depth3"][depth3_key].append(text)
        
        return hierarchy
    
    def _calculate_quality_score(self, content_data: Dict, strategy: CrawlStrategy) -> float:
        """í¬ë¡¤ë§ ê²°ê³¼ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        score = 45  # Playwright ê¸°ë³¸ ì ìˆ˜ (ë¸Œë¼ìš°ì € ê¸°ë°˜)
        
        # í…ìŠ¤íŠ¸ ê¸¸ì´ ì ìˆ˜ (0-25ì )
        text_length = len(content_data.get('text', ''))
        if text_length > 5000:
            score += 25
        elif text_length > 2000:
            score += 20
        elif text_length > 500:
            score += 15
        elif text_length > 100:
            score += 10
        
        # êµ¬ì¡°ì  ìš”ì†Œ ì ìˆ˜ (0-15ì )
        headings = content_data.get('headings', [])
        if headings:
            score += min(len(headings) * 2, 10)  # í—¤ë”© ê°œìˆ˜ì— ë”°ë¥¸ ì ìˆ˜
            h1_count = sum(1 for h in headings if h['level'] == 1)
            if h1_count > 0:
                score += 5
        
        # ë©”íƒ€ë°ì´í„° í’ˆì§ˆ (0-10ì )
        metadata = content_data.get('metadata', {})
        if metadata.get('title'):
            score += 3
        if metadata.get('description'):
            score += 3
        if metadata.get('og_title') or metadata.get('og_description'):
            score += 2
        if metadata.get('keywords'):
            score += 2
        
        # JavaScript ë Œë”ë§ ë³´ë„ˆìŠ¤ (0-5ì )
        score += 5  # PlaywrightëŠ” í•­ìƒ JS ë Œë”ë§
        
        return min(score, 100.0)
    
    async def crawl(self, url: str, strategy: CrawlStrategy) -> CrawlResult:
        """Playwrightë¥¼ ì‚¬ìš©í•œ ì›¹í˜ì´ì§€ í¬ë¡¤ë§"""
        if not self.is_initialized or not self.context:
            raise RuntimeError("Playwright ì—”ì§„ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        logger.info(f"ğŸ­ Playwrightë¡œ í¬ë¡¤ë§ ì‹œì‘: {url}")
        
        page = None
        try:
            # ìƒˆ í˜ì´ì§€ ìƒì„±
            page = await self.context.new_page()
            
            # í˜ì´ì§€ ì´ë²¤íŠ¸ ë¦¬ìŠ¤ë„ˆ ì„¤ì • (ì„ íƒì )
            if strategy.anti_bot_mode:
                # ì•ˆí‹°ë´‡ ëª¨ë“œì—ì„œëŠ” ë” ìì—°ìŠ¤ëŸ¬ìš´ í–‰ë™ ì‹œë®¬ë ˆì´ì…˜
                await page.set_extra_http_headers({
                    'sec-ch-ua': '"Not_A Brand";v="8", "Chromium";v="120"',
                    'sec-ch-ua-mobile': '?0',
                    'sec-ch-ua-platform': '"macOS"',
                    'sec-fetch-dest': 'document',
                    'sec-fetch-mode': 'navigate',
                    'sec-fetch-site': 'none',
                    'sec-fetch-user': '?1',
                })
            
            # í˜ì´ì§€ ë¡œë“œ
            logger.info(f"ğŸ­ í˜ì´ì§€ ë¡œë“œ ì¤‘: {url}")
            await page.goto(url, timeout=strategy.timeout * 1000, wait_until='domcontentloaded')
            
            # ì½˜í…ì¸  ë¡œë”© ëŒ€ê¸°
            await self._wait_for_content_load(page, strategy)
            
            # ë¬´í•œìŠ¤í¬ë¡¤ ì²˜ë¦¬ (í•„ìš”í•œ ê²½ìš°)
            if hasattr(strategy, 'handle_infinite_scroll') and strategy.handle_infinite_scroll:
                await self._handle_infinite_scroll(page)
            
            # ì½˜í…ì¸  ì¶”ì¶œ
            content_data = await self._extract_content(page, url)
            
            # ê³„ì¸µêµ¬ì¡° ì¶”ì¶œ
            hierarchy = self._extract_hierarchy_from_headings(content_data.get('headings', []), url)
            
            # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
            quality_score = self._calculate_quality_score(content_data, strategy)
            
            # ê²°ê³¼ ê°ì²´ ìƒì„±
            crawl_result = CrawlResult(
                url=url,
                title=content_data['title'],
                text=content_data['markdown'],
                hierarchy=hierarchy,
                metadata={
                    "crawler_used": "playwright",
                    "processing_time": f"{strategy.timeout}s",
                    "content_quality": "high" if quality_score > 80 else "medium" if quality_score > 60 else "low",
                    "extraction_confidence": quality_score / 100,
                    "playwright_metadata": content_data['metadata'],
                    "html_length": len(content_data['html']),
                    "markdown_length": len(content_data['markdown']),
                    "text_length": len(content_data['text']),
                    "headings_count": len(content_data.get('headings', [])),
                    "quality_score": quality_score,
                    "javascript_rendered": True,
                    "anti_bot_mode": strategy.anti_bot_mode
                },
                status="complete",
                timestamp=datetime.now()
            )
            
            logger.info(f"âœ… Playwright í¬ë¡¤ë§ ì„±ê³µ: {url} (í’ˆì§ˆ: {quality_score:.1f}/100)")
            return crawl_result
            
        except PlaywrightTimeoutError as e:
            logger.error(f"âŒ Playwright í¬ë¡¤ë§ íƒ€ì„ì•„ì›ƒ: {url} - {e}")
            return CrawlResult(
                url=url,
                title="",
                text="",
                hierarchy={},
                metadata={
                    "crawler_used": "playwright",
                    "error_type": "TimeoutError",
                    "processing_time": "0s"
                },
                status="failed",
                timestamp=datetime.now(),
                error=f"í˜ì´ì§€ ë¡œë“œ íƒ€ì„ì•„ì›ƒ: {str(e)}"
            )
            
        except Exception as e:
            logger.error(f"âŒ Playwright í¬ë¡¤ë§ ì‹¤íŒ¨: {url} - {e}")
            return CrawlResult(
                url=url,
                title="",
                text="",
                hierarchy={},
                metadata={
                    "crawler_used": "playwright",
                    "error_type": type(e).__name__,
                    "processing_time": "0s"
                },
                status="failed",
                timestamp=datetime.now(),
                error=str(e)
            )
        finally:
            # í˜ì´ì§€ ì •ë¦¬
            if page:
                try:
                    await page.close()
                except Exception:
                    pass 