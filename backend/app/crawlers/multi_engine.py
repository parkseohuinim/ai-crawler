import logging
import asyncio
from typing import Dict, List, Optional, Any
from datetime import datetime
from urllib.parse import urlparse
import re

from .base import BaseCrawler, CrawlResult, CrawlStrategy
from .firecrawl_engine import FirecrawlEngine
from .requests_engine import RequestsEngine
from .crawl4ai_engine import Crawl4AIEngine
from .playwright_engine import PlaywrightEngine

# MCP í´ë¼ì´ì–¸íŠ¸ í†µí•©
from ..mcp import MCPClient, MCPToolsManager, CrawlingStrategyManager

logger = logging.getLogger(__name__)

class MultiEngineCrawler:
    """ë‹¤ì¤‘ í¬ë¡¤ë§ ì—”ì§„ í†µí•© ê´€ë¦¬ì (MCP ê¸°ë°˜ AI ë¶„ì„ í†µí•©)"""
    
    def __init__(self):
        self.engines: Dict[str, BaseCrawler] = {}
        self.is_initialized = False
        
        # MCP í´ë¼ì´ì–¸íŠ¸ ë° ê´€ë¦¬ìë“¤
        self.mcp_client = MCPClient()
        self.mcp_tools_manager = None
        self.strategy_manager = None
        
        # í¬ë¡¤ë§ ì „ëµ ì„¤ì • (ì‚¬ì´íŠ¸ ìœ í˜•ë³„) - Phase 2 ì—…ë°ì´íŠ¸
        self.crawler_strategies = {
            "complex_spa": {
                "primary": "crawl4ai",  # AI ê¸°ë°˜ SPA í¬ë¡¤ë§
                "fallback": ["firecrawl", "playwright", "requests"],
                "characteristics": ["React/Vue", "ë¬´í•œìŠ¤í¬ë¡¤", "ë³µì¡í•œ JS"]
            },
            "ai_analysis_needed": {
                "primary": "crawl4ai",  # LLM ì¶”ì¶œ ì „ëµ ì‚¬ìš©
                "fallback": ["firecrawl", "playwright", "requests"],
                "characteristics": ["ë³µì¡í•œ êµ¬ì¡°", "ì˜ë¯¸ì  ì¶”ì¶œ", "AI ë¶„ë¥˜ í•„ìš”"]
            },
            "anti_bot_heavy": {
                "primary": "playwright",  # ë¸Œë¼ìš°ì € ê¸°ë°˜ ìš°íšŒ
                "fallback": ["firecrawl", "crawl4ai", "requests"],
                "characteristics": ["Cloudflare", "reCAPTCHA", "ê°•í•œ ë´‡ ì°¨ë‹¨"]
            },
            "standard_dynamic": {
                "primary": "playwright",  # ë¸Œë¼ìš°ì € ìë™í™”
                "fallback": ["crawl4ai", "firecrawl", "requests"],
                "characteristics": ["í‘œì¤€ ë™ì ì‚¬ì´íŠ¸", "ë¡œê·¸ì¸ í•„ìš”", "ì„¸ë°€í•œ ì œì–´"]
            },
            "simple_static": {
                "primary": "requests",  # ë¹ ë¥¸ ì²˜ë¦¬
                "fallback": ["crawl4ai", "firecrawl", "playwright"],
                "characteristics": ["ì •ì  HTML", "ë¹ ë¥¸ ì²˜ë¦¬", "ë‹¨ìˆœ êµ¬ì¡°"]
            }
        }
    
    async def initialize(self):
        """ëª¨ë“  í¬ë¡¤ë§ ì—”ì§„ ë° MCP í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        logger.info("ğŸ”§ í¬ë¡¤ë§ ì—”ì§„ë“¤ ë° MCP í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹œì‘...")
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ì—”ì§„ë“¤ ë“±ë¡ (Phase 2 - ëª¨ë“  ì—”ì§„ í™œì„±í™”)
        self.engines = {
            "requests": RequestsEngine(),        # ê¸°ë³¸ HTTP í¬ë¡¤ëŸ¬
            "firecrawl": FirecrawlEngine(),      # í”„ë¦¬ë¯¸ì—„ ì„œë¹„ìŠ¤
            "crawl4ai": Crawl4AIEngine(),        # AI ê¸°ë°˜ í¬ë¡¤ëŸ¬
            "playwright": PlaywrightEngine(),    # ë¸Œë¼ìš°ì € ìë™í™”
        }
        
        # ê° ì—”ì§„ ì´ˆê¸°í™”
        failed_engines = []
        for name, engine in self.engines.items():
            try:
                await engine.initialize()
                logger.info(f"âœ… {name} ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                logger.error(f"âŒ {name} ì—”ì§„ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                # ì‹¤íŒ¨í•œ ì—”ì§„ì„ ë³„ë„ ë¦¬ìŠ¤íŠ¸ì— ê¸°ë¡
                failed_engines.append(name)
        
        # ì‹¤íŒ¨í•œ ì—”ì§„ë“¤ì„ ë”•ì…”ë„ˆë¦¬ì—ì„œ ì œê±°
        for name in failed_engines:
            del self.engines[name]
        
        # MCP ê´€ë¦¬ìë“¤ ì´ˆê¸°í™”
        self.mcp_tools_manager = MCPToolsManager(self.mcp_client)
        self.strategy_manager = CrawlingStrategyManager(self.mcp_client)
        
        self.is_initialized = True
        logger.info(f"ğŸš€ ì´ {len(self.engines)}ê°œ ì—”ì§„ + MCP í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def cleanup(self):
        """ëª¨ë“  ì—”ì§„ ì •ë¦¬"""
        logger.info("ğŸ”„ í¬ë¡¤ë§ ì—”ì§„ ì •ë¦¬ ì‹œì‘...")
        
        for name, engine in self.engines.items():
            try:
                await engine.cleanup()
                logger.info(f"âœ… {name} ì—”ì§„ ì •ë¦¬ ì™„ë£Œ")
            except Exception as e:
                logger.error(f"âŒ {name} ì—”ì§„ ì •ë¦¬ ì‹¤íŒ¨: {e}")
        
        self.is_initialized = False
        logger.info("ğŸ ëª¨ë“  ì—”ì§„ ì •ë¦¬ ì™„ë£Œ")
    
    def _validate_url(self, url: str) -> tuple[bool, str]:
        """URL ìœ íš¨ì„± ê²€ì‚¬"""
        try:
            # ê¸°ë³¸ URL í˜•ì‹ ê²€ì‚¬
            if not url or not isinstance(url, str):
                return False, "URLì´ ë¹„ì–´ìˆê±°ë‚˜ ë¬¸ìì—´ì´ ì•„ë‹™ë‹ˆë‹¤"
            
            # URL íŒŒì‹±
            parsed = urlparse(url.strip())
            
            # ìŠ¤í‚¤ë§ˆ ê²€ì‚¬
            if not parsed.scheme or parsed.scheme.lower() not in ['http', 'https']:
                return False, f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ìŠ¤í‚¤ë§ˆ: {parsed.scheme}"
            
            # ë„ë©”ì¸ ê²€ì‚¬
            if not parsed.netloc:
                return False, "ë„ë©”ì¸ì´ ì—†ìŠµë‹ˆë‹¤"
            
            # ë„ë©”ì¸ í˜•ì‹ ê²€ì‚¬ (ê¸°ë³¸ì ì¸ íŒ¨í„´)
            domain_pattern = r'^[a-zA-Z0-9]([a-zA-Z0-9\-]{0,61}[a-zA-Z0-9])?(\.[a-zA-Z0-9]([a-zA-Z0-9\-]{0,61}[a-zA-Z0-9])?)*$'
            if not re.match(domain_pattern, parsed.netloc.split(':')[0]):
                return False, f"ì˜ëª»ëœ ë„ë©”ì¸ í˜•ì‹: {parsed.netloc}"
            
            # ì•Œë ¤ì§„ ë¬¸ì œ URL íŒ¨í„´ë“¤
            known_issues = [
                'lineCombOrder/lineCombList.do',  # 404ë¥¼ ë°˜í™˜í•˜ëŠ” ê²ƒìœ¼ë¡œ ì•Œë ¤ì§„ íŒ¨í„´
                'javascript:',  # JavaScript ìŠ¤í‚¤ë§ˆ
                'mailto:',  # ì´ë©”ì¼ ë§í¬
                '#',  # ì•µì»¤ë§Œ ìˆëŠ” ë§í¬
            ]
            
            for issue in known_issues:
                if issue in url:
                    return False, f"ì•Œë ¤ì§„ ë¬¸ì œ URL íŒ¨í„´: {issue}"
            
            return True, "ìœ íš¨í•œ URL"
            
        except Exception as e:
            return False, f"URL ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {str(e)}"
    
    async def analyze_site_and_get_strategy(self, url: str, sample_html: str = "") -> Dict[str, Any]:
        """MCP ê¸°ë°˜ ì‚¬ì´íŠ¸ ë¶„ì„ í›„ í¬ë¡¤ë§ ì „ëµ ìƒì„±"""
        logger.info(f"ğŸ§  MCP ê¸°ë°˜ ì‚¬ì´íŠ¸ ë¶„ì„ ì‹œì‘: {url}")
        
        # ğŸ”§ ë””ë²„ê¹…: MCP ë„êµ¬ ë§¤ë‹ˆì € ìƒíƒœ í™•ì¸
        logger.info(f"ğŸ”§ DEBUG: MCP ë„êµ¬ ë§¤ë‹ˆì € ì´ˆê¸°í™” ìƒíƒœ: {self.mcp_tools_manager is not None}")
        logger.info(f"ğŸ”§ DEBUG: MCP í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ìƒíƒœ: {self.mcp_client is not None}")
        print(f"[DEBUG] MCP ë„êµ¬ ë§¤ë‹ˆì € ì´ˆê¸°í™” ìƒíƒœ: {self.mcp_tools_manager is not None}")
        print(f"[DEBUG] MCP í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ìƒíƒœ: {self.mcp_client is not None}")
        
        try:
            # ğŸ”§ ë””ë²„ê¹…: MCP ì—°ê²° ì‹œë„
            logger.info("ğŸ”§ DEBUG: MCP í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ì‹œë„ ì¤‘...")
            print(f"[DEBUG] MCP í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ì‹œë„ ì¤‘...")
            
            # MCP í´ë¼ì´ì–¸íŠ¸ë¡œ ì—°ê²°í•˜ì—¬ ë¶„ì„ ì‹¤í–‰
            async with self.mcp_client.connect():
                logger.info("ğŸ”§ DEBUG: MCP í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ì„±ê³µ")
                print(f"[DEBUG] MCP í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ì„±ê³µ")
                
                # ğŸ”§ ë””ë²„ê¹…: ì¢…í•© ë¶„ì„ ì‹¤í–‰ ì „
                logger.info(f"ğŸ”§ DEBUG: analyze_website_completely í˜¸ì¶œ ì‹œì‘ - URL: {url}, HTML ê¸¸ì´: {len(sample_html)}")
                
                # ì¢…í•© ë¶„ì„ ì‹¤í–‰ (ì‚¬ì´íŠ¸ ë¶„ì„ â†’ êµ¬ì¡° ê°ì§€ â†’ ì „ëµ ìƒì„±)
                complete_analysis = await self.mcp_tools_manager.analyze_website_completely(url, sample_html)
                
                # ğŸ”§ ë””ë²„ê¹…: ë¶„ì„ ê²°ê³¼ ìƒì„¸ ë¡œê·¸
                logger.info(f"ğŸ”§ DEBUG: analyze_website_completely ì™„ë£Œ")
                logger.info(f"ğŸ”§ DEBUG: ë¶„ì„ ê²°ê³¼ íƒ€ì…: {type(complete_analysis)}")
                logger.info(f"ğŸ”§ DEBUG: ë¶„ì„ ê²°ê³¼ í‚¤ë“¤: {list(complete_analysis.keys()) if isinstance(complete_analysis, dict) else 'Not a dict'}")
                
                if "error" in complete_analysis:
                    logger.warning(f"ğŸ”§ DEBUG: MCP ë¶„ì„ì—ì„œ ì—ëŸ¬ ê°ì§€: {complete_analysis['error']}")
                    logger.warning(f"MCP ë¶„ì„ ì‹¤íŒ¨, í´ë°± ì „ëµ ì‚¬ìš©: {complete_analysis['error']}")
                    fallback_result = self._get_fallback_strategy(url)
                    logger.info(f"ğŸ”§ DEBUG: í´ë°± ì „ëµ ê²°ê³¼: {fallback_result}")
                    return fallback_result
                
                # ğŸ”§ ë””ë²„ê¹…: ì„±ê³µì ì¸ MCP ë¶„ì„ ê²°ê³¼ ìƒì„¸ ë¡œê·¸
                logger.info(f"ğŸ”§ DEBUG: MCP ë¶„ì„ ì„±ê³µ!")
                print(f"[DEBUG] MCP ë¶„ì„ ì„±ê³µ!")
                if "crawling_strategy" in complete_analysis:
                    strategy = complete_analysis["crawling_strategy"]
                    logger.info(f"ğŸ”§ DEBUG: ì¶”ì²œ ì—”ì§„: {strategy.get('recommended_engine', 'None')}")
                    logger.info(f"ğŸ”§ DEBUG: í´ë°± ì—”ì§„ë“¤: {strategy.get('fallback_engines', [])}")
                    print(f"[DEBUG] ì¶”ì²œ ì—”ì§„: {strategy.get('recommended_engine', 'None')}, í´ë°± ì—”ì§„ë“¤: {strategy.get('fallback_engines', [])}")
                else:
                    logger.warning(f"ğŸ”§ DEBUG: crawling_strategy í‚¤ê°€ ì—†ìŒ! ì „ì²´ ê²°ê³¼: {complete_analysis}")
                    print(f"[DEBUG] crawling_strategy í‚¤ê°€ ì—†ìŒ! ì „ì²´ ê²°ê³¼: {complete_analysis}")
                
                logger.info(f"âœ… MCP ê¸°ë°˜ ë¶„ì„ ì™„ë£Œ: {url}")
                return complete_analysis
                
        except Exception as e:
            # ğŸ”§ ë””ë²„ê¹…: ì˜ˆì™¸ ìƒì„¸ ì •ë³´
            logger.error(f"ğŸ”§ DEBUG: MCP ë¶„ì„ ì¤‘ ì˜ˆì™¸ ë°œìƒ!")
            logger.error(f"ğŸ”§ DEBUG: ì˜ˆì™¸ íƒ€ì…: {type(e).__name__}")
            logger.error(f"ğŸ”§ DEBUG: ì˜ˆì™¸ ë©”ì‹œì§€: {str(e)}")
            logger.error(f"ğŸ”§ DEBUG: ì˜ˆì™¸ ìƒì„¸: {repr(e)}")
            print(f"[DEBUG] MCP ë¶„ì„ ì¤‘ ì˜ˆì™¸ ë°œìƒ! íƒ€ì…: {type(e).__name__}, ë©”ì‹œì§€: {str(e)}")
            
            logger.error(f"MCP ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
            fallback_result = self._get_fallback_strategy(url)
            logger.info(f"ğŸ”§ DEBUG: ì˜ˆì™¸ í›„ í´ë°± ì „ëµ ê²°ê³¼: {fallback_result}")
            print(f"[DEBUG] ì˜ˆì™¸ í›„ í´ë°± ì „ëµ ì‚¬ìš©")
            return fallback_result
    
    def _get_fallback_strategy(self, url: str) -> Dict[str, Any]:
        """MCP ì‹¤íŒ¨ ì‹œ ì‚¬ìš©í•  í´ë°± ì „ëµ (ê°œì„ ëœ íœ´ë¦¬ìŠ¤í‹± ê¸°ë°˜)"""
        domain = url.lower()
        
        logger.warning(f"âš ï¸ MCP ë¶„ì„ ì‹¤íŒ¨ - í´ë°± ì „ëµ ì‚¬ìš©: {url}")
        
        # ğŸ”§ ë””ë²„ê¹…: ë„ë©”ì¸ ë¶„ì„ ê³¼ì •
        logger.info(f"ğŸ”§ DEBUG: í´ë°± ì „ëµ - ë„ë©”ì¸ ë¶„ì„: {domain}")
        print(f"[DEBUG] í´ë°± ì „ëµ - ë„ë©”ì¸ ë¶„ì„: {domain}")
        
        # ê°œì„ ëœ íŒ¨í„´ ë§¤ì¹­
        strategy_type = None
        
        # ğŸ”§ ë””ë²„ê¹…: ê° íŒ¨í„´ ë§¤ì¹­ ê³¼ì •
        spa_keywords = ['react.dev', 'vue', 'angular', 'spa']
        shopping_keywords = ['shop.kt.com', 'shopping', 'ecommerce', 'store']
        security_keywords = ['cloudflare', 'protected', 'secure']
        dynamic_keywords = ['dynamic', 'app', 'portal']
        
        logger.info(f"ğŸ”§ DEBUG: SPA í‚¤ì›Œë“œ ì²´í¬: {spa_keywords}")
        print(f"[DEBUG] SPA í‚¤ì›Œë“œ ì²´í¬: {spa_keywords}")
        if any(keyword in domain for keyword in spa_keywords):
            strategy_type = "complex_spa"
            logger.info(f"ğŸ¯ í´ë°± ì „ëµ: SPA ì‚¬ì´íŠ¸ë¡œ íŒë‹¨ â†’ {strategy_type}")
            print(f"[DEBUG] SPA ì‚¬ì´íŠ¸ë¡œ íŒë‹¨ â†’ {strategy_type}")
        else:
            logger.info(f"ğŸ”§ DEBUG: SPA í‚¤ì›Œë“œ ë§¤ì¹­ ì‹¤íŒ¨")
            print(f"[DEBUG] SPA í‚¤ì›Œë“œ ë§¤ì¹­ ì‹¤íŒ¨")
        
        if not strategy_type:
            logger.info(f"ğŸ”§ DEBUG: ì‡¼í•‘ëª° í‚¤ì›Œë“œ ì²´í¬: {shopping_keywords}")
            if any(keyword in domain for keyword in shopping_keywords):
                strategy_type = "ai_analysis_needed"  # crawl4ai ìš°ì„ 
                logger.info(f"ğŸ¯ í´ë°± ì „ëµ: ì‡¼í•‘ëª°/AI í•„ìš” ì‚¬ì´íŠ¸ â†’ {strategy_type}")
            else:
                logger.info(f"ğŸ”§ DEBUG: ì‡¼í•‘ëª° í‚¤ì›Œë“œ ë§¤ì¹­ ì‹¤íŒ¨")
        
        if not strategy_type:
            logger.info(f"ğŸ”§ DEBUG: ë³´ì•ˆ í‚¤ì›Œë“œ ì²´í¬: {security_keywords}")
            if any(keyword in domain for keyword in security_keywords):
                strategy_type = "anti_bot_heavy"
                logger.info(f"ğŸ¯ í´ë°± ì „ëµ: ë´‡ ì°¨ë‹¨ ì‚¬ì´íŠ¸ â†’ {strategy_type}")
            else:
                logger.info(f"ğŸ”§ DEBUG: ë³´ì•ˆ í‚¤ì›Œë“œ ë§¤ì¹­ ì‹¤íŒ¨")
        
        if not strategy_type:
            logger.info(f"ğŸ”§ DEBUG: ë™ì  í‚¤ì›Œë“œ ì²´í¬: {dynamic_keywords}")
            if any(keyword in domain for keyword in dynamic_keywords):
                strategy_type = "standard_dynamic"
                logger.info(f"ğŸ¯ í´ë°± ì „ëµ: ë™ì  ì‚¬ì´íŠ¸ â†’ {strategy_type}")
            else:
                logger.info(f"ğŸ”§ DEBUG: ë™ì  í‚¤ì›Œë“œ ë§¤ì¹­ ì‹¤íŒ¨")
        
        if not strategy_type:
            strategy_type = "simple_static"
            logger.info(f"ğŸ¯ í´ë°± ì „ëµ: ë‹¨ìˆœ ì •ì  ì‚¬ì´íŠ¸ â†’ {strategy_type} (ê¸°ë³¸ê°’)")
        
        # ğŸ”§ ë””ë²„ê¹…: ì„ íƒëœ ì „ëµ ì •ë³´
        logger.info(f"ğŸ”§ DEBUG: ìµœì¢… ì„ íƒëœ ì „ëµ íƒ€ì…: {strategy_type}")
        
        if strategy_type not in self.crawler_strategies:
            logger.error(f"ğŸ”§ DEBUG: ì „ëµ íƒ€ì… '{strategy_type}'ì´ crawler_strategiesì— ì—†ìŒ!")
            logger.error(f"ğŸ”§ DEBUG: ì‚¬ìš© ê°€ëŠ¥í•œ ì „ëµë“¤: {list(self.crawler_strategies.keys())}")
            strategy_type = "simple_static"  # ì•ˆì „í•œ ê¸°ë³¸ê°’
        
        config = self.crawler_strategies[strategy_type]
        
        # ğŸ”§ ë””ë²„ê¹…: ì „ëµ ì„¤ì • ì •ë³´
        logger.info(f"ğŸ”§ DEBUG: ì „ëµ ì„¤ì •:")
        logger.info(f"   - primary: {config['primary']}")
        logger.info(f"   - fallback: {config['fallback']}")
        logger.info(f"   - characteristics: {config.get('characteristics', [])}")
        
        result = {
            "url": url,
            "crawling_strategy": {
                "recommended_engine": config["primary"],
                "fallback_engines": config["fallback"],
                "strategy_type": strategy_type
            },
            "is_fallback": True,
            "status": "fallback_strategy"
        }
        
        # ğŸ”§ ë””ë²„ê¹…: ìµœì¢… ê²°ê³¼
        logger.info(f"ğŸ”§ DEBUG: í´ë°± ì „ëµ ìµœì¢… ê²°ê³¼:")
        logger.info(f"   - recommended_engine: {result['crawling_strategy']['recommended_engine']}")
        logger.info(f"   - fallback_engines: {result['crawling_strategy']['fallback_engines']}")
        logger.info(f"   - strategy_type: {result['crawling_strategy']['strategy_type']}")
        print(f"[DEBUG] í´ë°± ì „ëµ ìµœì¢… ê²°ê³¼: ì¶”ì²œ={result['crawling_strategy']['recommended_engine']}, í´ë°±={result['crawling_strategy']['fallback_engines']}, íƒ€ì…={result['crawling_strategy']['strategy_type']}")
        
        return result
    
    def get_strategy_config(self, strategy_type: str) -> CrawlStrategy:
        """ì „ëµ íƒ€ì…ì— ë”°ë¥¸ í¬ë¡¤ë§ ì„¤ì • ë°˜í™˜"""
        if strategy_type not in self.crawler_strategies:
            strategy_type = "simple_static"
        
        config = self.crawler_strategies[strategy_type]
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ì—”ì§„ë§Œ í•„í„°ë§
        available_engines = [eng for eng in [config["primary"]] + config["fallback"] 
                           if eng in self.engines]
        
        if not available_engines:
            available_engines = list(self.engines.keys())
        
        # ì „ëµ íƒ€ì…ì— ë”°ë¥¸ ë™ì  íƒ€ì„ì•„ì›ƒ ì„¤ì •
        timeout_config = {
            "complex_spa": 60,  # SPAëŠ” ë¡œë”© ì‹œê°„ì´ ê¸¸ì–´ì„œ 60ì´ˆ
            "ai_analysis_needed": 45,  # AI ë¶„ì„ í•„ìš” ì‚¬ì´íŠ¸ëŠ” 45ì´ˆ
            "anti_bot_heavy": 60,  # ë´‡ ì°¨ë‹¨ ì‚¬ì´íŠ¸ëŠ” ìš°íšŒ ì‹œê°„ í•„ìš”í•´ì„œ 60ì´ˆ
            "standard_dynamic": 40,  # í‘œì¤€ ë™ì  ì‚¬ì´íŠ¸ëŠ” 40ì´ˆ
            "simple_static": 30  # ì •ì  ì‚¬ì´íŠ¸ëŠ” 30ì´ˆ
        }
        
        return CrawlStrategy(
            engine_priority=available_engines,
            timeout=timeout_config.get(strategy_type, 30),
            max_retries=3,
            wait_time=1.0
        )
    
    async def crawl_with_strategy(self, url: str, custom_strategy: Optional[CrawlStrategy] = None) -> CrawlResult:
        """MCP ê¸°ë°˜ ì§€ëŠ¥í˜• í¬ë¡¤ë§"""
        if not self.is_initialized:
            raise RuntimeError("í¬ë¡¤ëŸ¬ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        # ğŸ”§ ë””ë²„ê¹…: í˜„ì¬ ì‚¬ìš© ê°€ëŠ¥í•œ ì—”ì§„ë“¤ ì¶œë ¥
        logger.info(f"ğŸ”§ DEBUG: í˜„ì¬ ì´ˆê¸°í™”ëœ ì—”ì§„ë“¤: {list(self.engines.keys())}")
        print(f"[DEBUG] í˜„ì¬ ì´ˆê¸°í™”ëœ ì—”ì§„ë“¤: {list(self.engines.keys())}")
        
        # URL ìœ íš¨ì„± ê²€ì‚¬
        is_valid, validation_msg = self._validate_url(url)
        if not is_valid:
            logger.error(f"ğŸš« ìœ íš¨í•˜ì§€ ì•Šì€ URL: {url} - {validation_msg}")
            return CrawlResult(
                url=url,
                title="",
                text="",
                hierarchy={},
                metadata={"error": validation_msg, "validation_failed": True},
                status="failed",
                timestamp=datetime.now(),
                error=f"URL ìœ íš¨ì„± ê²€ì‚¬ ì‹¤íŒ¨: {validation_msg}"
            )
        
        logger.info(f"ğŸ” MCP ê¸°ë°˜ ì‚¬ì´íŠ¸ ë¶„ì„ ì‹œì‘: {url}")
        print(f"[DEBUG] ğŸ” MCP ê¸°ë°˜ ì‚¬ì´íŠ¸ ë¶„ì„ ì‹œì‘: {url}")
        
        # MCP ë¶„ì„ ê²°ê³¼
        analysis_result = None
        
        # ì „ëµ ê²°ì •
        if custom_strategy:
            strategy = custom_strategy
            logger.info("ğŸ‘¤ ì‚¬ìš©ì ì •ì˜ ì „ëµ ì‚¬ìš©")
            logger.info(f"ğŸ”§ DEBUG: ì‚¬ìš©ì ì •ì˜ ì „ëµ ì—”ì§„ë“¤: {strategy.engine_priority}")
            print(f"[DEBUG] ğŸ‘¤ ì‚¬ìš©ì ì •ì˜ ì „ëµ ì‚¬ìš©: {strategy.engine_priority}")
        else:
            # MCP ê¸°ë°˜ ì¢…í•© ë¶„ì„ ì‹¤í–‰
            logger.info(f"ğŸ” MCP ë¶„ì„ ì‹œì‘ for {url}")
            print(f"[DEBUG] ğŸ” MCP ë¶„ì„ ì‹œì‘ for {url}")
            analysis_result = await self.analyze_site_and_get_strategy(url)
            logger.info(f"ğŸ” MCP ë¶„ì„ ì™„ë£Œ - ê²°ê³¼ íƒ€ì…: {type(analysis_result)}")
            logger.debug(f"ğŸ” MCP ë¶„ì„ ê²°ê³¼ í‚¤ë“¤: {list(analysis_result.keys()) if isinstance(analysis_result, dict) else 'Not a dict'}")
            print(f"[DEBUG] ğŸ” MCP ë¶„ì„ ì™„ë£Œ - ê²°ê³¼ íƒ€ì…: {type(analysis_result)}")
            
            # ğŸ”§ ë””ë²„ê¹…: MCP ë¶„ì„ ê²°ê³¼ ì „ì²´ ì¶œë ¥
            logger.info(f"ğŸ”§ DEBUG: MCP ë¶„ì„ ê²°ê³¼ ì „ì²´: {analysis_result}")
            print(f"[DEBUG] ğŸ”§ MCP ë¶„ì„ ê²°ê³¼ ì „ì²´: {analysis_result}")
            
            # í´ë°± ì „ëµ í™•ì¸
            if analysis_result.get("is_fallback"):
                logger.warning(f"âš ï¸ í´ë°± ì „ëµ ê°ì§€! í´ë°± ì´ìœ : {analysis_result.get('status', 'Unknown')}")
                print(f"[DEBUG] âš ï¸ í´ë°± ì „ëµ ê°ì§€! í´ë°± ì´ìœ : {analysis_result.get('status', 'Unknown')}")
            else:
                print(f"[DEBUG] âœ… MCP ë¶„ì„ ì„±ê³µ (í´ë°± ì•„ë‹˜)")
            
            # ë¶„ì„ ê²°ê³¼ì—ì„œ ì „ëµ ì¶”ì¶œ
            crawling_strategy = analysis_result.get("crawling_strategy", {})
            recommended_crawler = crawling_strategy.get("recommended_engine", "requests")
            fallback_crawlers = crawling_strategy.get("fallback_engines", ["requests"])
            
            # ğŸ”§ ë””ë²„ê¹…: ì „ëµ ì¶”ì¶œ ê²°ê³¼
            logger.info(f"ğŸ”§ DEBUG: ì¶”ì¶œëœ ì „ëµ ì •ë³´:")
            logger.info(f"   - crawling_strategy: {crawling_strategy}")
            logger.info(f"   - recommended_engine: {recommended_crawler}")
            logger.info(f"   - fallback_engines: {fallback_crawlers}")
            print(f"[DEBUG] ğŸ”§ ì¶”ì¶œëœ ì „ëµ ì •ë³´:")
            print(f"[DEBUG]    - crawling_strategy: {crawling_strategy}")
            print(f"[DEBUG]    - recommended_engine: {recommended_crawler}")
            print(f"[DEBUG]    - fallback_engines: {fallback_crawlers}")
            
            # í•­ìƒ í‘œì‹œë˜ëŠ” MCP ë¶„ì„ ê²°ê³¼ (ì½˜ì†” ì§ì ‘ ì¶œë ¥)
            print(f"[MCP] ğŸ§  ë¶„ì„ ê²°ê³¼: ì¶”ì²œ={recommended_crawler}, í´ë°±={fallback_crawlers}")
            print(f"[MCP] ğŸ“‹ ìµœì¢… ì—”ì§„ ìš°ì„ ìˆœìœ„: {[recommended_crawler] + [c for c in fallback_crawlers if c != recommended_crawler]}")
            
            logger.info(f"ğŸ§  MCP ë¶„ì„ ê²°ê³¼:")
            logger.info(f"   - ì¶”ì²œ ì—”ì§„: {recommended_crawler}")
            logger.info(f"   - í´ë°± ì—”ì§„: {fallback_crawlers}")
            
            # CrawlStrategy ê°ì²´ ìƒì„±
            logger.debug(f"ğŸ”§ recommended_crawler: {recommended_crawler}")
            logger.debug(f"ğŸ”§ fallback_crawlers: {fallback_crawlers}")
            
            engine_priority = [recommended_crawler] + [c for c in fallback_crawlers if c != recommended_crawler]
            logger.info(f"ğŸ“‹ ìµœì¢… ì—”ì§„ ìš°ì„ ìˆœìœ„: {engine_priority}")
            logger.debug(f"ğŸ”§ ì—”ì§„ ìš°ì„ ìˆœìœ„ ìƒì„¸:")
            logger.debug(f"   - recommended_crawler: {recommended_crawler}")
            logger.debug(f"   - fallback_crawlers: {fallback_crawlers}")  
            logger.debug(f"   - ì¤‘ë³µ ì œê±° í›„: {[c for c in fallback_crawlers if c != recommended_crawler]}")
            logger.debug(f"   - ìµœì¢… ì¡°í•©: {engine_priority}")
            
            # ì‚¬ìš© ê°€ëŠ¥í•œ ì—”ì§„ë§Œ í•„í„°ë§
            available_engines = [eng for eng in engine_priority if eng in self.engines]
            unavailable_engines = [eng for eng in engine_priority if eng not in self.engines]
            
            # ğŸ”§ ë””ë²„ê¹…: ì—”ì§„ í•„í„°ë§ ê²°ê³¼
            logger.info(f"ğŸ”§ DEBUG: ì—”ì§„ í•„í„°ë§ ê²°ê³¼:")
            logger.info(f"   - ìš”ì²­ëœ ì—”ì§„ë“¤: {engine_priority}")
            logger.info(f"   - ì‚¬ìš© ê°€ëŠ¥í•œ ì—”ì§„ë“¤: {available_engines}")
            logger.info(f"   - ì‚¬ìš© ë¶ˆê°€ëŠ¥í•œ ì—”ì§„ë“¤: {unavailable_engines}")
            logger.info(f"   - í˜„ì¬ ì´ˆê¸°í™”ëœ ì—”ì§„ë“¤: {list(self.engines.keys())}")
            
            if unavailable_engines:
                logger.warning(f"âš ï¸ ì‚¬ìš© ë¶ˆê°€ ì—”ì§„ë“¤: {unavailable_engines}")
            
            # ğŸ”§ ì‚¬ìš© ê°€ëŠ¥í•œ ì—”ì§„ì´ ì—†ëŠ” ê²½ìš° ëª¨ë“  ì—”ì§„ ì‚¬ìš©
            if not available_engines:
                logger.warning(f"âš ï¸ ìš”ì²­ëœ ì—”ì§„ë“¤ì´ ëª¨ë‘ ì‚¬ìš© ë¶ˆê°€! ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  ì—”ì§„ ì‚¬ìš©")
                available_engines = list(self.engines.keys())
            
            print(f"[MCP] âœ… ì‚¬ìš© ê°€ëŠ¥í•œ ì—”ì§„ë“¤: {available_engines}")
            logger.info(f"âœ… ì‚¬ìš© ê°€ëŠ¥í•œ ì—”ì§„ë“¤: {available_engines}")
            logger.debug(f"ğŸ” self.engines.keys(): {list(self.engines.keys())}")
            
            strategy = CrawlStrategy(
                engine_priority=available_engines,
                timeout=30,
                max_retries=3,
                wait_time=1.0
            )
            
            print(f"[MCP] ğŸ¯ ìµœì¢… ì „ëµ: ìš°ì„ ìˆœìœ„={available_engines}")
            logger.info(f"ğŸ¯ MCP ì¶”ì²œ ì „ëµ: {recommended_crawler} (í´ë°±: {fallback_crawlers})")
            logger.info(f"ğŸ”§ ì‹¤ì œ ì‚¬ìš©í•  ìš°ì„ ìˆœìœ„: {available_engines}")
        
        # ìš°ì„ ìˆœìœ„ì— ë”°ë¼ ì—”ì§„ ì‹œë„
        last_error = None
        attempted_engines = []
        
        logger.info(f"ğŸ¯ ì—”ì§„ ìš°ì„ ìˆœìœ„: {strategy.engine_priority}")
        logger.info(f"ğŸ¬ í¬ë¡¤ë§ ì‹œì‘: ì´ {len(strategy.engine_priority)}ê°œ ì—”ì§„ ì‹œë„ ì˜ˆì •")
        print(f"[DEBUG] ğŸ¯ ì—”ì§„ ìš°ì„ ìˆœìœ„: {strategy.engine_priority}")
        print(f"[DEBUG] ğŸ¬ í¬ë¡¤ë§ ì‹œì‘: ì´ {len(strategy.engine_priority)}ê°œ ì—”ì§„ ì‹œë„ ì˜ˆì •")
        
        for i, engine_name in enumerate(strategy.engine_priority, 1):
            attempted_engines.append(engine_name)
            
            if engine_name not in self.engines:
                logger.warning(f"âš ï¸ [{i}/{len(strategy.engine_priority)}] ì—”ì§„ {engine_name} ì‚¬ìš© ë¶ˆê°€ (ë“±ë¡ë˜ì§€ ì•ŠìŒ)")
                logger.warning(f"ğŸ”§ DEBUG: í˜„ì¬ ë“±ë¡ëœ ì—”ì§„ë“¤: {list(self.engines.keys())}")
                continue
            
            engine = self.engines[engine_name]
            logger.info(f"ğŸš€ [{i}/{len(strategy.engine_priority)}] {engine_name} ì—”ì§„ìœ¼ë¡œ í¬ë¡¤ë§ ì‹œë„ ì¤‘...")
            print(f"[DEBUG] ğŸš€ [{i}/{len(strategy.engine_priority)}] {engine_name} ì—”ì§„ìœ¼ë¡œ í¬ë¡¤ë§ ì‹œë„ ì¤‘...")
            
            try:
                start_time = asyncio.get_event_loop().time()
                result = await engine.crawl_with_retry(url, strategy)
                end_time = asyncio.get_event_loop().time()
                execution_time = end_time - start_time
                
                logger.info(f"â±ï¸ {engine_name} ì—”ì§„ ì‹¤í–‰ ì‹œê°„: {execution_time:.2f}ì´ˆ")
                logger.info(f"ğŸ“Š {engine_name} ì—”ì§„ ê²°ê³¼: status={result.status}, title='{result.title}', text_length={len(result.text)}")
                
                if result.status == "complete":
                    logger.info(f"âœ… [{i}/{len(strategy.engine_priority)}] {engine_name} ì—”ì§„ìœ¼ë¡œ ì„±ê³µ!")
                    logger.info(f"ğŸ‰ ìµœì¢… ì„ íƒëœ ì—”ì§„: {engine_name}")
                    
                    # ì„±ê³µí•œ ì—”ì§„ ì •ë³´ë¥¼ ë©”íƒ€ë°ì´í„°ì— ì¶”ê°€
                    result.metadata["attempted_engines"] = attempted_engines
                    result.metadata["successful_engine_index"] = i
                    result.metadata["total_available_engines"] = len(strategy.engine_priority)
                    
                    # ì‹¤ì œ ì²˜ë¦¬ì‹œê°„ ì¶”ê°€ (ê¸°ì¡´ í•˜ë“œì½”ë”©ëœ ê°’ ë®ì–´ì“°ê¸°)
                    result.metadata["processing_time"] = f"{execution_time:.2f}s"
                    result.metadata["execution_time"] = execution_time
                    result.metadata["engine_used"] = engine_name
                    
                    # MCP í’ˆì§ˆ ê²€ì¦ ì‹¤í–‰ (ì˜¤ë¥˜ ì‹œ ë¬´ì‹œ)
                    if analysis_result and self.mcp_tools_manager:
                        try:
                            async with self.mcp_client.connect():
                                quality_result = await self.mcp_tools_manager.validate_crawling_quality(
                                    result.to_dict(), url
                                )
                                
                                # í’ˆì§ˆ ì •ë³´ë¥¼ ê²°ê³¼ì— ì¶”ê°€
                                if quality_result and "error" not in quality_result:
                                    result.metadata["mcp_quality_score"] = quality_result.get("quality_score", "N/A")
                                    result.metadata["quality_assessment"] = quality_result.get("assessment", {})
                                    logger.info(f"ğŸ“Š MCP í’ˆì§ˆ ì ìˆ˜: {quality_result.get('quality_score', 'N/A')}")
                                else:
                                    logger.debug("MCP í’ˆì§ˆ ê²€ì¦ ê²°ê³¼ ì—†ìŒ ë˜ëŠ” ì˜¤ë¥˜")
                        except Exception as e:
                            logger.debug(f"MCP í’ˆì§ˆ ê²€ì¦ ìŠ¤í‚µ (ì˜¤ë¥˜): {e}")
                            # í’ˆì§ˆ ê²€ì¦ ì‹¤íŒ¨í•´ë„ í¬ë¡¤ë§ ê²°ê³¼ì—ëŠ” ì˜í–¥ ì—†ìŒ
                    
                    # MCP ë¶„ì„ ì •ë³´ ì¶”ê°€
                    if analysis_result:
                        result.metadata["mcp_analysis"] = analysis_result
                        result.metadata["used_mcp_intelligence"] = True
                        
                        # ğŸ¯ ì‚¬ìš©ì ì¹œí™”ì ì¸ ì—”ì§„ ì„ íƒ ì´ìœ  ìƒì„±
                        engine_selection_reason = self._generate_engine_selection_explanation(
                            analysis_result, engine_name, attempted_engines
                        )
                        result.metadata["engine_selection_reason"] = engine_selection_reason
                    
                    return result
                else:
                    logger.warning(f"âš ï¸ [{i}/{len(strategy.engine_priority)}] {engine_name} ì—”ì§„ ë¶€ë¶„ ì‹¤íŒ¨: {result.error}")
                    last_error = result.error
                    
            except Exception as e:
                logger.error(f"âŒ [{i}/{len(strategy.engine_priority)}] {engine_name} ì—”ì§„ ì˜ˆì™¸ ë°œìƒ: {type(e).__name__}: {e}")
                last_error = str(e)
                continue
        
        # ëª¨ë“  ì—”ì§„ ì‹¤íŒ¨
        logger.error(f"ğŸ’¥ ëª¨ë“  ì—”ì§„ ì‹¤íŒ¨: {url}")
        logger.error(f"ğŸ” ì‹œë„í•œ ì—”ì§„ë“¤: {attempted_engines}")
        logger.error(f"ğŸ“ ë§ˆì§€ë§‰ ì˜¤ë¥˜: {last_error}")
        
        return CrawlResult(
            url=url,
            title="",
            text="",
            hierarchy={},
            metadata={
                "attempted_engines": attempted_engines,
                "total_available_engines": len(strategy.engine_priority),
                "final_error": str(last_error),
                "all_engines_failed": True
            },
            status="failed",
            timestamp=datetime.now(),
            error=f"ëª¨ë“  ì—”ì§„ ì‹¤íŒ¨: {last_error}"
        )
    
    async def bulk_crawl(self, urls: List[str], max_concurrent: int = 5) -> List[CrawlResult]:
        """ëŒ€ëŸ‰ URL ë³‘ë ¬ í¬ë¡¤ë§"""
        logger.info(f"ğŸ“¦ ëŒ€ëŸ‰ í¬ë¡¤ë§ ì‹œì‘: {len(urls)}ê°œ URL")
        
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def crawl_single(url: str) -> CrawlResult:
            async with semaphore:
                return await self.crawl_with_strategy(url)
        
        tasks = [crawl_single(url) for url in urls]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ì˜ˆì™¸ ì²˜ë¦¬
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                processed_results.append(CrawlResult(
                    url=urls[i],
                    title="",
                    text="",
                    hierarchy={},
                    metadata={"error": str(result)},
                    status="failed",
                    timestamp=datetime.now(),
                    error=str(result)
                ))
            else:
                processed_results.append(result)
        
        success_count = sum(1 for r in processed_results if r.status == "complete")
        logger.info(f"ğŸ“Š ëŒ€ëŸ‰ í¬ë¡¤ë§ ì™„ë£Œ: {success_count}/{len(urls)} ì„±ê³µ")
        
        return processed_results
    
    def _generate_engine_selection_explanation(self, analysis_result: Dict, selected_engine: str, attempted_engines: List[str]) -> Dict[str, Any]:
        """ì‚¬ìš©ì ì¹œí™”ì ì¸ ì—”ì§„ ì„ íƒ ì´ìœ  ìƒì„±"""
        try:
            site_analysis = analysis_result.get("site_analysis", {})
            crawling_strategy = analysis_result.get("crawling_strategy", {})
            
            # ì‚¬ì´íŠ¸ íŠ¹ì„± ë¶„ì„
            site_type = site_analysis.get("site_type", {}).get("type", "unknown")
            js_complexity = site_analysis.get("javascript_complexity", {})
            anti_bot = site_analysis.get("anti_bot_detection", {})
            
            # ê¸°ë³¸ ì •ë³´
            explanation = {
                "selected_engine": selected_engine,
                "confidence": crawling_strategy.get("confidence", 0),
                "analysis_method": "MCP AI ë¶„ì„" if not analysis_result.get("is_fallback") else "í´ë°± ì „ëµ",
                "site_characteristics": {},
                "selection_reasons": [],
                "technical_details": {},
                "fallback_engines": crawling_strategy.get("fallback_engines", [])
            }
            
            # ì‚¬ì´íŠ¸ íŠ¹ì„± ìš”ì•½
            explanation["site_characteristics"] = {
                "site_type": site_type,
                "javascript_level": js_complexity.get("level", "unknown"),
                "javascript_score": js_complexity.get("score", 0),
                "anti_bot_risk": anti_bot.get("risk_level", "unknown"),
                "requires_js": js_complexity.get("requires_js_execution", False)
            }
            
            # ì„ íƒ ì´ìœ  ìƒì„±
            reasons = []
            
            # ì‚¬ì´íŠ¸ íƒ€ì…ë³„ ì´ìœ 
            if site_type == "simple_static":
                reasons.append("ì •ì  ì›¹ì‚¬ì´íŠ¸ë¡œ ë¶„ë¥˜ë˜ì—ˆìŠµë‹ˆë‹¤")
            elif site_type == "complex_spa":
                reasons.append("ë³µì¡í•œ SPA(Single Page Application)ë¡œ ë¶„ì„ë˜ì—ˆìŠµë‹ˆë‹¤")
            elif site_type == "dynamic_content":
                reasons.append("ë™ì  ì½˜í…ì¸ ê°€ í¬í•¨ëœ ì‚¬ì´íŠ¸ë¡œ ë¶„ì„ë˜ì—ˆìŠµë‹ˆë‹¤")
            
            # JavaScript ë³µì¡ë„ ì´ìœ 
            js_level = js_complexity.get("level", "unknown")
            js_score = js_complexity.get("score", 0)
            
            if js_level == "high" and js_score > 70:
                reasons.append(f"JavaScript ë³µì¡ë„ê°€ ë†’ìŒ (ì ìˆ˜: {js_score}/100)")
                reasons.append("JavaScript ì‹¤í–‰ì´ í•„ìš”í•œ ë™ì  ì½˜í…ì¸ ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤")
            elif js_level == "medium":
                reasons.append(f"JavaScript ë³µì¡ë„ê°€ ë³´í†µ ìˆ˜ì¤€ (ì ìˆ˜: {js_score}/100)")
            elif js_level == "low":
                reasons.append(f"JavaScript ì‚¬ìš©ëŸ‰ì´ ì ìŒ (ì ìˆ˜: {js_score}/100)")
            
            # ì•ˆí‹°ë´‡ ìœ„í—˜ë„ ì´ìœ 
            anti_bot_risk = anti_bot.get("risk_level", "unknown")
            if anti_bot_risk == "high":
                reasons.append("ê°•ë ¥í•œ ë´‡ ì°¨ë‹¨ ì‹œìŠ¤í…œì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤")
            elif anti_bot_risk == "medium":
                reasons.append("ì¤‘ê°„ ìˆ˜ì¤€ì˜ ë´‡ ì°¨ë‹¨ ì‹œìŠ¤í…œì´ ìˆìŠµë‹ˆë‹¤")
            elif anti_bot_risk == "low":
                reasons.append("ë´‡ ì°¨ë‹¨ ìœ„í—˜ë„ê°€ ë‚®ìŠµë‹ˆë‹¤")
            
            # ì—”ì§„ë³„ ì„ íƒ ì´ìœ 
            if selected_engine == "crawl4ai":
                reasons.append("AI ê¸°ë°˜ ì½˜í…ì¸  ì¶”ì¶œì— ìµœì í™”ëœ ì—”ì§„ì…ë‹ˆë‹¤")
                if js_complexity.get("requires_js_execution"):
                    reasons.append("JavaScript ì‹¤í–‰ê³¼ LLM ê¸°ë°˜ ì¶”ì¶œì´ í•„ìš”í•©ë‹ˆë‹¤")
            elif selected_engine == "firecrawl":
                reasons.append("í”„ë¦¬ë¯¸ì—„ í¬ë¡¤ë§ ì„œë¹„ìŠ¤ë¡œ ì•ˆí‹°ë´‡ ìš°íšŒì— ê°•ë ¥í•©ë‹ˆë‹¤")
            elif selected_engine == "playwright":
                reasons.append("ë¸Œë¼ìš°ì € ìë™í™”ë¡œ ë³µì¡í•œ ì‚¬ì´íŠ¸ ì²˜ë¦¬ì— ì í•©í•©ë‹ˆë‹¤")
            elif selected_engine == "requests":
                reasons.append("ë‹¨ìˆœí•œ HTTP ìš”ì²­ìœ¼ë¡œ ë¹ ë¥¸ ì²˜ë¦¬ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤")
            
            explanation["selection_reasons"] = reasons
            
            # ê¸°ìˆ ì  ì„¸ë¶€ì‚¬í•­
            explanation["technical_details"] = {
                "mcp_reasoning": crawling_strategy.get("reasoning", ""),
                "script_count": site_analysis.get("site_type", {}).get("script_count", 0),
                "content_ratio": site_analysis.get("site_type", {}).get("content_ratio", 0),
                "attempted_engines": attempted_engines,
                "success_on_attempt": len(attempted_engines)
            }
            
            return explanation
            
        except Exception as e:
            logger.error(f"ì—”ì§„ ì„ íƒ ì´ìœ  ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            return {
                "selected_engine": selected_engine,
                "confidence": 0,
                "analysis_method": "ì˜¤ë¥˜ ë°œìƒ",
                "selection_reasons": [f"{selected_engine} ì—”ì§„ì´ ì„ íƒë˜ì—ˆìŠµë‹ˆë‹¤"],
                "error": str(e)
            }

    async def get_engine_status(self) -> Dict[str, Any]:
        """ëª¨ë“  ì—”ì§„ ìƒíƒœ ë°˜í™˜"""
        status = {}
        
        for name, engine in self.engines.items():
            try:
                status[name] = await engine.health_check()
            except Exception as e:
                status[name] = {
                    "name": name,
                    "error": str(e),
                    "initialized": False
                }
        
        return {
            "total_engines": len(self.engines),
            "initialized": self.is_initialized,
            "engines": status
        } 