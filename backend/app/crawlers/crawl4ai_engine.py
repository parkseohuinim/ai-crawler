import os
import logging
from typing import Dict, Any, Optional
from datetime import datetime
import json
import asyncio

from .base import BaseCrawler, CrawlResult, CrawlStrategy, EngineCapabilities

logger = logging.getLogger(__name__)

try:
    from crawl4ai import AsyncWebCrawler
    from crawl4ai.extraction_strategy import LLMExtractionStrategy, CosineStrategy
    from crawl4ai.chunking_strategy import RegexChunking
    CRAWL4AI_AVAILABLE = True
except ImportError:
    CRAWL4AI_AVAILABLE = False
    logger.warning("Crawl4AI ÎùºÏù¥Î∏åÎü¨Î¶¨Í∞Ä ÏÑ§ÏπòÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§")

class Crawl4AIEngine(BaseCrawler):
    """Crawl4AI Í∏∞Î∞ò ÌÅ¨Î°§ÎßÅ ÏóîÏßÑ - AI Í∏∞Î∞ò Ïä§ÎßàÌä∏ ÏΩòÌÖêÏ∏† Ï∂îÏ∂ú"""
    
    def __init__(self):
        super().__init__("crawl4ai")
        self.crawler = None
        self.openai_api_key = None
    
    async def initialize(self) -> None:
        """Crawl4AI ÌÅ¨Î°§Îü¨ Ï¥àÍ∏∞Ìôî"""
        if not CRAWL4AI_AVAILABLE:
            raise RuntimeError("Crawl4AI ÎùºÏù¥Î∏åÎü¨Î¶¨Í∞Ä ÏÑ§ÏπòÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§")
        
        # OpenAI API ÌÇ§ ÌôïÏù∏ (LLM Ï∂îÏ∂úÏö©)
        self.openai_api_key = os.getenv("OPENAI_API_KEY")
        if not self.openai_api_key:
            logger.warning("OPENAI_API_KEYÍ∞Ä ÏÑ§Ï†ïÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§. Í∏∞Î≥∏ Ï∂îÏ∂ú Ï†ÑÎûµÏùÑ ÏÇ¨Ïö©Ìï©ÎãàÎã§.")
        else:
            # API ÌÇ§ ÏùºÎ∂ÄÎßå Î°úÍπÖ (Î≥¥ÏïàÏÉÅ)
            masked_key = self.openai_api_key[:8] + "..." + self.openai_api_key[-4:] if len(self.openai_api_key) > 12 else "***"
            logger.info(f"ü§ñ OpenAI API ÌÇ§ Î°úÎìúÎê®: {masked_key}")
        
        try:
            # Crawl4AI ÎπÑÎèôÍ∏∞ ÌÅ¨Î°§Îü¨ Ï¥àÍ∏∞Ìôî (ÏµúÏã† Î≤ÑÏ†Ñ API ÏÇ¨Ïö©)
            self.crawler = AsyncWebCrawler(
                verbose=True,
                headless=True
            )
            
            # ÌÅ¨Î°§Îü¨ ÏãúÏûë (astart ÎåÄÏã† start ÏÇ¨Ïö©)
            await self.crawler.start()
            self.is_initialized = True
            logger.info("ü§ñ Crawl4AI ÏóîÏßÑ Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
            
        except Exception as e:
            logger.error(f"Crawl4AI Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
            raise
    
    async def cleanup(self) -> None:
        """Î¶¨ÏÜåÏä§ Ï†ïÎ¶¨"""
        if self.crawler:
            try:
                # aclose ÎåÄÏã† close ÏÇ¨Ïö©
                await self.crawler.close()
                logger.info("ü§ñ Crawl4AI ÌÅ¨Î°§Îü¨ Ï¢ÖÎ£å ÏôÑÎ£å")
            except Exception as e:
                logger.error(f"Crawl4AI ÌÅ¨Î°§Îü¨ Ï¢ÖÎ£å Ï§ë Ïò§Î•ò: {e}")
            finally:
                self.crawler = None
        
        self.is_initialized = False
        logger.info("ü§ñ Crawl4AI ÏóîÏßÑ Ï†ïÎ¶¨ ÏôÑÎ£å")
    
    def get_capabilities(self) -> Dict[str, Any]:
        """Crawl4AI ÏóîÏßÑÏùò Îä•Î†•"""
        return {
            EngineCapabilities.JAVASCRIPT_RENDERING: True,
            EngineCapabilities.ANTI_BOT_BYPASS: True,
            EngineCapabilities.PREMIUM_SERVICE: False,  # Ïò§ÌîàÏÜåÏä§
            EngineCapabilities.INFINITE_SCROLL: True,
            EngineCapabilities.BULK_PROCESSING: True,
            "supported_formats": ["markdown", "html", "structured_data"],
            "ai_features": ["llm_extraction", "semantic_chunking", "smart_filtering"],
            "rate_limits": "Î∏åÎùºÏö∞Ï†Ä Í∏∞Î∞ò (Î¨¥Ï†úÌïú)",
            "best_for": ["LLM ÌÜµÌï©", "Íµ¨Ï°∞ÌôîÎêú Îç∞Ïù¥ÌÑ∞", "AI Í∏∞Î∞ò Ï∂îÏ∂ú", "Î≥µÏû°Ìïú SPA"]
        }
    
    def _create_extraction_strategy(self, strategy: CrawlStrategy) -> Optional[Any]:
        """Ï∂îÏ∂ú Ï†ÑÎûµ ÏÉùÏÑ± (ÌòÑÏû¨ ÎπÑÌôúÏÑ±Ìôî - deprecated ÏóêÎü¨ Î∞©ÏßÄ)"""
        # OpenAI API ÌÇ§Í∞Ä ÏóÜÍ±∞ÎÇò LLM Ï†ÑÎûµÏóêÏÑú ÏóêÎü¨Í∞Ä Î∞úÏÉùÌïòÎØÄÎ°ú ÎπÑÌôúÏÑ±Ìôî
        logger.info("üí° LLM Ï∂îÏ∂ú Ï†ÑÎûµ ÎπÑÌôúÏÑ±Ìôî (Í∏∞Î≥∏ Ï∂îÏ∂ú ÏÇ¨Ïö©)")
        return None
        
        # ÏõêÎûò ÏΩîÎìúÎäî Ï£ºÏÑù Ï≤òÎ¶¨
        """
        if not self.openai_api_key:
            return None
        
        try:
            from crawl4ai.models import LLMConfig
            
            # LLM ÏÑ§Ï†ï ÏÉùÏÑ± (ÏµúÏã† API)
            llm_config = LLMConfig(
                provider="openai/gpt-4o-mini",  # ÎπÑÏö© Ìö®Ïú®Ï†ÅÏù∏ Î™®Îç∏
                api_token=self.openai_api_key
            )
            
            # LLM Í∏∞Î∞ò Ï∂îÏ∂ú Ï†ÑÎûµ ÏÉùÏÑ±
            llm_strategy = LLMExtractionStrategy(
                llm_config=llm_config,
                instruction="Ï£ºÏöî ÏΩòÌÖêÏ∏†Î•º ÎßàÌÅ¨Îã§Ïö¥ ÌòïÏãùÏúºÎ°ú Ï∂îÏ∂úÌïòÏÑ∏Ïöî.",
                extraction_type="block",
                apply_chunking=True,
                chunking_strategy=RegexChunking()
            )
            return llm_strategy
        except Exception as e:
            logger.error(f"LLM Ï∂îÏ∂ú Ï†ÑÎûµ ÏÉùÏÑ± Ïã§Ìå®: {e}")
            return None
        """
    
    def _extract_hierarchy_from_markdown(self, markdown_text: str, url: str) -> Dict[str, Any]:
        """ÎßàÌÅ¨Îã§Ïö¥ ÌÖçÏä§Ìä∏ÏóêÏÑú Í≥ÑÏ∏µÍµ¨Ï°∞ Ï∂îÏ∂ú"""
        hierarchy = {"depth1": "ÏõπÌéòÏù¥ÏßÄ", "depth2": {}, "depth3": {}}
        
        if not markdown_text:
            return hierarchy
        
        lines = markdown_text.split('\n')
        current_h1 = None
        current_h2 = None
        
        for line in lines:
            line = line.strip()
            
            if line.startswith('# ') and not line.startswith('## '):
                # H1 Ìó§Îçî
                current_h1 = line[2:].strip()
                hierarchy["depth1"] = current_h1
                if current_h1 not in hierarchy["depth2"]:
                    hierarchy["depth2"][current_h1] = []
                    
            elif line.startswith('## '):
                # H2 Ìó§Îçî
                current_h2 = line[3:].strip()
                if current_h1:
                    if current_h1 not in hierarchy["depth2"]:
                        hierarchy["depth2"][current_h1] = []
                    hierarchy["depth2"][current_h1].append(current_h2)
                else:
                    hierarchy["depth2"]["Í∏∞ÌÉÄ"] = hierarchy["depth2"].get("Í∏∞ÌÉÄ", [])
                    hierarchy["depth2"]["Í∏∞ÌÉÄ"].append(current_h2)
                    
            elif line.startswith('### '):
                # H3 Ìó§Îçî
                h3_title = line[4:].strip()
                depth3_key = current_h2 or current_h1 or "Í∏∞ÌÉÄ"
                if depth3_key not in hierarchy["depth3"]:
                    hierarchy["depth3"][depth3_key] = []
                hierarchy["depth3"][depth3_key].append(h3_title)
        
        return hierarchy
    
    def _calculate_quality_score(self, result_data: Dict, markdown_text: str) -> float:
        """ÌÅ¨Î°§ÎßÅ Í≤∞Í≥º ÌíàÏßà Ï†êÏàò Í≥ÑÏÇ∞"""
        score = 50  # Crawl4AI Í∏∞Î≥∏ Ï†êÏàò (AI Í∏∞Î∞ò)
        
        # ÌÖçÏä§Ìä∏ Í∏∏Ïù¥ Ï†êÏàò (0-25Ï†ê)
        text_length = len(markdown_text) if markdown_text else 0
        if text_length > 8000:
            score += 25
        elif text_length > 3000:
            score += 20
        elif text_length > 1000:
            score += 15
        elif text_length > 100:
            score += 10
        
        # Íµ¨Ï°∞Ï†Å ÏöîÏÜå Ï†êÏàò (0-15Ï†ê)
        if markdown_text:
            structure_score = 0
            if '# ' in markdown_text:
                structure_score += 4
            if '## ' in markdown_text:
                structure_score += 4
            if '- ' in markdown_text or '* ' in markdown_text:
                structure_score += 3
            if '[' in markdown_text and '](' in markdown_text:
                structure_score += 4
            score += structure_score
        
        # AI Ï∂îÏ∂ú ÏÇ¨Ïö© Î≥¥ÎÑàÏä§ (0-10Ï†ê)
        if result_data.get("extracted_content"):
            score += 10
        elif result_data.get("llm_extraction_strategy"):
            score += 5
        
        return min(score, 100.0)
    
    async def crawl(self, url: str, strategy: CrawlStrategy) -> CrawlResult:
        """Crawl4AIÎ•º ÏÇ¨Ïö©Ìïú ÏõπÌéòÏù¥ÏßÄ ÌÅ¨Î°§ÎßÅ"""
        if not self.is_initialized or not self.crawler:
            raise RuntimeError("Crawl4AI ÏóîÏßÑÏù¥ Ï¥àÍ∏∞ÌôîÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§")
        
        logger.info(f"ü§ñ Crawl4AIÎ°ú ÌÅ¨Î°§ÎßÅ ÏãúÏûë: {url}")
        
        try:
            # Ï∂îÏ∂ú Ï†ÑÎûµ ÏÉùÏÑ±
            extraction_strategy = self._create_extraction_strategy(strategy)
            
            # Crawl4AI ÌÅ¨Î°§ÎßÅ ÏòµÏÖò
            crawl_options = {
                "word_count_threshold": 10,  # ÏµúÏÜå Îã®Ïñ¥ Ïàò
                "only_text": False,  # HTMLÎèÑ Ìï®Íªò Î∞òÌôò
                "bypass_cache": True,  # Ï∫êÏãú Ïö∞Ìöå
                "remove_overlay_elements": True,  # Ïò§Î≤ÑÎ†àÏù¥ Ï†úÍ±∞
                "simulate_user": True,  # ÏÇ¨Ïö©Ïûê ÏãúÎÆ¨Î†àÏù¥ÏÖò
            }
            
            # CSS ÏÑ†ÌÉùÏûê ÏÑ§Ï†ï - Google Í∞ôÏùÄ ÏÇ¨Ïù¥Ìä∏Î•º ÏúÑÌï¥ Îçî Ìè¨Í¥ÑÏ†ÅÏúºÎ°ú ÏÑ§Ï†ï
            css_selector = ""  # Ï†ÑÏ≤¥ ÌéòÏù¥ÏßÄ ÌÅ¨Î°§ÎßÅ (Ï†úÌïú ÏóÜÏùå)
            
            # üîß Google Í∞ôÏùÄ JavaScript ÏùòÏ°¥ ÏÇ¨Ïù¥Ìä∏ Í∞êÏßÄ
            is_js_heavy_site = any(domain in url.lower() for domain in [
                'google.com', 'gmail.com', 'youtube.com', 
                'facebook.com', 'twitter.com', 'instagram.com',
                'linkedin.com', 'reddit.com'
            ])
            
            # ÎåÄÍ∏∞ Ï°∞Í±¥ ÏÑ§Ï†ï - JavaScript ÏùòÏ°¥ ÏÇ¨Ïù¥Ìä∏Îäî Îçî Ïò§Îûò ÎåÄÍ∏∞
            if is_js_heavy_site:
                wait_for = "networkidle"  # ÎÑ§Ìä∏ÏõåÌÅ¨ ÌôúÎèôÏù¥ Î©àÏ∂ú ÎïåÍπåÏßÄ ÎåÄÍ∏∞
                crawl_options["delay_before_return_html"] = 5  # 5Ï¥à Ï∂îÍ∞Ä ÎåÄÍ∏∞
                crawl_options["simulate_user"] = True
                crawl_options["override_navigator"] = True
                logger.info(f"ü§ñ JavaScript ÏùòÏ°¥ ÏÇ¨Ïù¥Ìä∏ Í∞êÏßÄ: {url} - ÌôïÏû•Îêú ÎåÄÍ∏∞ ÏÑ§Ï†ï Ï†ÅÏö©")
            else:
                wait_for = "domcontentloaded"
                crawl_options["delay_before_return_html"] = 2  # Í∏∞Î≥∏ 2Ï¥à ÎåÄÍ∏∞
            
            if strategy.anti_bot_mode:
                wait_for = "networkidle"
                crawl_options["simulate_user"] = True
                crawl_options["override_navigator"] = True
            
            # LLM Ï∂îÏ∂ú Ï†ÑÎûµ ÏÇ¨Ïö© Ïó¨Î∂Ä
            if extraction_strategy:
                crawl_options["extraction_strategy"] = extraction_strategy
                logger.info("ü§ñ LLM Ï∂îÏ∂ú Ï†ÑÎûµ ÌôúÏÑ±Ìôî")
            
            # ÌÅ¨Î°§ÎßÅ Ïã§Ìñâ
            logger.info(f"ü§ñ Crawl4AI ÏòµÏÖò: {crawl_options}")
            result = await self.crawler.arun(
                url=url,
                css_selector=css_selector,
                wait_for=wait_for,
                **crawl_options
            )
            
            # Í≤∞Í≥º Ï≤òÎ¶¨
            if not result.success:
                error_msg = f"ÌÅ¨Î°§ÎßÅ Ïã§Ìå®: {result.error_message or 'Ïïå Ïàò ÏóÜÎäî Ïò§Î•ò'}"
                raise Exception(error_msg)
            
            # ÎßàÌÅ¨Îã§Ïö¥ ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú
            markdown_text = result.markdown or result.cleaned_html or ""
            html_content = result.html or ""
            
            # Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Ï∂îÏ∂ú
            metadata = {
                "title": result.metadata.get("title", "Ï†úÎ™© ÏóÜÏùå") if result.metadata else "Ï†úÎ™© ÏóÜÏùå",
                "description": result.metadata.get("description", "") if result.metadata else "",
                "keywords": result.metadata.get("keywords", "") if result.metadata else "",
            }
            
            # Ï∂îÏ∂úÎêú ÏΩòÌÖêÏ∏† ÌôïÏù∏
            extracted_content = None
            if hasattr(result, 'extracted_content') and result.extracted_content:
                extracted_content = result.extracted_content
                logger.info("ü§ñ LLM Ï∂îÏ∂ú ÏΩòÌÖêÏ∏† Í∞êÏßÄÎê®")
            
            # Í≥ÑÏ∏µÍµ¨Ï°∞ Ï∂îÏ∂ú
            hierarchy = self._extract_hierarchy_from_markdown(markdown_text, url)
            
            # ÌíàÏßà Ï†êÏàò Í≥ÑÏÇ∞
            result_data = {
                "extracted_content": extracted_content,
                "llm_extraction_strategy": extraction_strategy is not None,
                "metadata": metadata
            }
            quality_score = self._calculate_quality_score(result_data, markdown_text)
            
            # Í≤∞Í≥º Í∞ùÏ≤¥ ÏÉùÏÑ±
            crawl_result = CrawlResult(
                url=url,
                title=metadata["title"],
                text=markdown_text,
                hierarchy=hierarchy,
                metadata={
                    "crawler_used": "crawl4ai",
                    "processing_time": f"{strategy.timeout}s",
                    "content_quality": "high" if quality_score > 85 else "medium" if quality_score > 60 else "low",
                    "extraction_confidence": quality_score / 100,
                    "crawl4ai_metadata": metadata,
                    "html_length": len(html_content),
                    "markdown_length": len(markdown_text),
                    "quality_score": quality_score,
                    "ai_extraction_used": extraction_strategy is not None,
                    "extracted_content_available": extracted_content is not None,
                    "success": result.success,
                    "crawl4ai_screenshot": result.screenshot if hasattr(result, 'screenshot') else None
                },
                status="complete",
                timestamp=datetime.now()
            )
            
            logger.info(f"‚úÖ Crawl4AI ÌÅ¨Î°§ÎßÅ ÏÑ±Í≥µ: {url} (ÌíàÏßà: {quality_score:.1f}/100)")
            return crawl_result
            
        except Exception as e:
            logger.error(f"‚ùå Crawl4AI ÌÅ¨Î°§ÎßÅ Ïã§Ìå®: {url} - {e}")
            return CrawlResult(
                url=url,
                title="",
                text="",
                hierarchy={},
                metadata={
                    "crawler_used": "crawl4ai",
                    "error_type": type(e).__name__,
                    "processing_time": "0s"
                },
                status="failed",
                timestamp=datetime.now(),
                error=str(e)
            ) 