import os
import logging
from typing import Dict, Any
from datetime import datetime
import json
import asyncio

from .base import BaseCrawler, CrawlResult, CrawlStrategy, EngineCapabilities

logger = logging.getLogger(__name__)

try:
    from firecrawl import FirecrawlApp
    FIRECRAWL_AVAILABLE = True
except ImportError:
    FIRECRAWL_AVAILABLE = False
    logger.warning("Firecrawl ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

class FirecrawlEngine(BaseCrawler):
    """Firecrawl ê¸°ë°˜ í¬ë¡¤ë§ ì—”ì§„"""
    
    def __init__(self):
        super().__init__("firecrawl")
        self.client = None
        self.api_key = None
    
    async def initialize(self) -> None:
        """Firecrawl í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        if not FIRECRAWL_AVAILABLE:
            raise RuntimeError("Firecrawl ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        # API í‚¤ í™•ì¸
        self.api_key = os.getenv("FIRECRAWL_API_KEY")
        if not self.api_key:
            logger.warning("FIRECRAWL_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            # ê°œë°œìš© ë”ë¯¸ í‚¤ ë˜ëŠ” ë¬´ë£Œ ë²„ì „ ì‚¬ìš©
            self.api_key = "fc-dummy-key-for-development"
        else:
            # API í‚¤ ì¼ë¶€ë§Œ ë¡œê¹… (ë³´ì•ˆìƒ)
            masked_key = self.api_key[:8] + "..." + self.api_key[-4:] if len(self.api_key) > 12 else "***"
            logger.info(f"ğŸ”¥ Firecrawl API í‚¤ ë¡œë“œë¨: {masked_key}")
        
        try:
            self.client = FirecrawlApp(api_key=self.api_key)
            self.is_initialized = True
            logger.info("ğŸ”¥ Firecrawl í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.error(f"Firecrawl ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    async def cleanup(self) -> None:
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        self.client = None
        self.is_initialized = False
        logger.info("ğŸ”¥ Firecrawl ì—”ì§„ ì •ë¦¬ ì™„ë£Œ")
    
    def get_capabilities(self) -> Dict[str, Any]:
        """Firecrawl ì—”ì§„ì˜ ëŠ¥ë ¥"""
        return {
            EngineCapabilities.JAVASCRIPT_RENDERING: True,
            EngineCapabilities.ANTI_BOT_BYPASS: True,
            EngineCapabilities.PREMIUM_SERVICE: True,
            EngineCapabilities.INFINITE_SCROLL: True,
            EngineCapabilities.BULK_PROCESSING: False,  # API ì œí•œìœ¼ë¡œ ì¸í•œ ë‹¨ì¼ ì²˜ë¦¬ ê¶Œì¥
            "supported_formats": ["markdown", "html", "text"],
            "rate_limits": "ë†’ìŒ (í”„ë¦¬ë¯¸ì—„ ì„œë¹„ìŠ¤)",
            "best_for": ["SPA", "ì•ˆí‹°ë´‡ ì‚¬ì´íŠ¸", "ë³µì¡í•œ JS", "ë¬´í•œìŠ¤í¬ë¡¤"]
        }
    
    def _extract_hierarchy_from_markdown(self, markdown_text: str, url: str) -> Dict[str, Any]:
        """ë§ˆí¬ë‹¤ìš´ í…ìŠ¤íŠ¸ì—ì„œ ê³„ì¸µêµ¬ì¡° ì¶”ì¶œ"""
        hierarchy = {"depth1": "ì›¹í˜ì´ì§€", "depth2": {}, "depth3": {}}
        
        if not markdown_text:
            return hierarchy
        
        lines = markdown_text.split('\n')
        current_h1 = None
        current_h2 = None
        
        for line in lines:
            line = line.strip()
            
            if line.startswith('# ') and not line.startswith('## '):
                # H1 í—¤ë”
                current_h1 = line[2:].strip()
                hierarchy["depth1"] = current_h1
                if current_h1 not in hierarchy["depth2"]:
                    hierarchy["depth2"][current_h1] = []
                    
            elif line.startswith('## '):
                # H2 í—¤ë”
                current_h2 = line[3:].strip()
                if current_h1:
                    if current_h1 not in hierarchy["depth2"]:
                        hierarchy["depth2"][current_h1] = []
                    hierarchy["depth2"][current_h1].append(current_h2)
                else:
                    hierarchy["depth2"]["ê¸°íƒ€"] = hierarchy["depth2"].get("ê¸°íƒ€", [])
                    hierarchy["depth2"]["ê¸°íƒ€"].append(current_h2)
                    
            elif line.startswith('### '):
                # H3 í—¤ë”
                h3_title = line[4:].strip()
                depth3_key = current_h2 or current_h1 or "ê¸°íƒ€"
                if depth3_key not in hierarchy["depth3"]:
                    hierarchy["depth3"][depth3_key] = []
                hierarchy["depth3"][depth3_key].append(h3_title)
        
        return hierarchy
    
    def _calculate_quality_score(self, result_data: Dict, markdown_text: str) -> float:
        """í¬ë¡¤ë§ ê²°ê³¼ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        score = 40  # ê¸°ë³¸ ì„±ê³µ ì ìˆ˜ (Firecrawlì´ ì‘ë‹µì„ ë°˜í™˜í–ˆìœ¼ë¯€ë¡œ)
        
        # í…ìŠ¤íŠ¸ ê¸¸ì´ ì ìˆ˜ (0-30ì )
        text_length = len(markdown_text) if markdown_text else 0
        if text_length > 5000:
            score += 30
        elif text_length > 1000:
            score += 20
        elif text_length > 100:
            score += 10
        
        # êµ¬ì¡°ì  ìš”ì†Œ ì ìˆ˜ (0-20ì )
        if markdown_text:
            structure_score = 0
            if '# ' in markdown_text:
                structure_score += 5
            if '## ' in markdown_text:
                structure_score += 5
            if '- ' in markdown_text or '* ' in markdown_text:
                structure_score += 5
            if '[' in markdown_text and '](' in markdown_text:
                structure_score += 5
            score += structure_score
        
        # ë©”íƒ€ë°ì´í„° í’ˆì§ˆ (0-10ì )
        if result_data.get("metadata"):
            metadata = result_data["metadata"]
            if metadata.get("title"):
                score += 3
            if metadata.get("description"):
                score += 3
            if metadata.get("keywords"):
                score += 2
            if metadata.get("ogTitle") or metadata.get("ogDescription"):
                score += 2
        
        return min(score, 100.0)
    
    async def crawl(self, url: str, strategy: CrawlStrategy) -> CrawlResult:
        """Firecrawlì„ ì‚¬ìš©í•œ ì›¹í˜ì´ì§€ í¬ë¡¤ë§"""
        if not self.is_initialized or not self.client:
            raise RuntimeError("Firecrawl ì—”ì§„ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        logger.info(f"ğŸ”¥ Firecrawlë¡œ í¬ë¡¤ë§ ì‹œì‘: {url}")
        
        try:
            # Firecrawl v1 API ì˜µì…˜ ì„¤ì • (2025ë…„ ìµœì‹  ë²„ì „)
            scrape_params = {
                "formats": ["markdown", "html"],
                "onlyMainContent": True,  # ë©”ì¸ ì½˜í…ì¸ ë§Œ ì¶”ì¶œ
            }
            
            # ì•ˆí‹°ë´‡ ëª¨ë“œê°€ í™œì„±í™”ëœ ê²½ìš° ì¶”ê°€ ì˜µì…˜
            if strategy.anti_bot_mode:
                scrape_params["waitFor"] = 5000  # ë” ì˜¤ë˜ ëŒ€ê¸°
            
            # ë¹„ë™ê¸° ì²˜ë¦¬ë¥¼ ìœ„í•´ ë™ê¸° í˜¸ì¶œì„ ìŠ¤ë ˆë“œí’€ì—ì„œ ì‹¤í–‰
            logger.info(f"ğŸ”¥ Firecrawl v1 scrape_url íŒŒë¼ë¯¸í„°: {scrape_params}")
            loop = asyncio.get_event_loop()
            
            # Firecrawl v1 API 2025ë…„ ë²„ì „ - ìƒˆë¡œìš´ ScrapeResponse ê°ì²´ ì‚¬ìš©
            scrape_response = await loop.run_in_executor(
                None, 
                lambda: self.client.scrape_url(url=url, **scrape_params)
            )
            
            # ë””ë²„ê¹…ì„ ìœ„í•œ ê²°ê³¼ ë¡œê¹…
            logger.info(f"ğŸ”¥ Firecrawl ì‘ë‹µ íƒ€ì…: {type(scrape_response)}")
            logger.info(f"ğŸ”¥ Firecrawl ì‘ë‹µ ì†ì„±ë“¤: {dir(scrape_response) if scrape_response else 'None'}")
            
            if not scrape_response:
                raise Exception("Firecrawl í¬ë¡¤ë§ ì‹¤íŒ¨: ì‘ë‹µ ì—†ìŒ")
            
            # 2025ë…„ ìµœì‹  SDK - ScrapeResponse ê°ì²´ì—ì„œ ì§ì ‘ ì†ì„± ì ‘ê·¼
            try:
                # success ì†ì„± í™•ì¸ (ìˆëŠ” ê²½ìš°)
                if hasattr(scrape_response, 'success') and not scrape_response.success:
                    error_msg = getattr(scrape_response, 'error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')
                    raise Exception(f"Firecrawl í¬ë¡¤ë§ ì‹¤íŒ¨: {error_msg}")
                
                # data ì†ì„±ì—ì„œ ì‹¤ì œ í¬ë¡¤ë§ ê²°ê³¼ ì¶”ì¶œ
                if hasattr(scrape_response, 'data'):
                    result_data = scrape_response.data
                elif hasattr(scrape_response, 'content'):
                    # ì§ì ‘ contentì— ì ‘ê·¼í•˜ëŠ” ê²½ìš°
                    result_data = {
                        'markdown': getattr(scrape_response, 'markdown', ''),
                        'html': getattr(scrape_response, 'html', ''),
                        'metadata': getattr(scrape_response, 'metadata', {})
                    }
                else:
                    # ì‘ë‹µ ê°ì²´ ìì²´ê°€ ë°ì´í„°ì¸ ê²½ìš°
                    result_data = {
                        'markdown': getattr(scrape_response, 'markdown', ''),
                        'html': getattr(scrape_response, 'html', ''),
                        'metadata': getattr(scrape_response, 'metadata', {})
                    }
                
                logger.info(f"ğŸ”¥ ì¶”ì¶œëœ ë°ì´í„° í‚¤ë“¤: {list(result_data.keys()) if isinstance(result_data, dict) else 'Not a dict'}")
                
                # ë§ˆí¬ë‹¤ìš´ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                markdown_text = result_data.get("markdown", result_data.get("content", ""))
                html_content = result_data.get("html", "")
                
                # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
                metadata = result_data.get("metadata", {})
                title = metadata.get("title", metadata.get("ogTitle", "ì œëª© ì—†ìŒ"))
                
                # ê³„ì¸µêµ¬ì¡° ì¶”ì¶œ
                hierarchy = self._extract_hierarchy_from_markdown(markdown_text, url)
                
                # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
                quality_score = self._calculate_quality_score(result_data, markdown_text)
                
                # ê²°ê³¼ ê°ì²´ ìƒì„±
                crawl_result = CrawlResult(
                    url=url,
                    title=title,
                    text=markdown_text,
                    hierarchy=hierarchy,
                    metadata={
                        "crawler_used": "firecrawl",
                        "processing_time": f"{strategy.timeout}s",
                        "content_quality": "high" if quality_score > 80 else "medium" if quality_score > 50 else "low",
                        "extraction_confidence": quality_score / 100,
                        "firecrawl_metadata": metadata,
                        "html_length": len(html_content),
                        "markdown_length": len(markdown_text),
                        "quality_score": quality_score
                    },
                    status="complete",
                    timestamp=datetime.now()
                )
                
                logger.info(f"âœ… Firecrawl í¬ë¡¤ë§ ì„±ê³µ: {url} (í’ˆì§ˆ: {quality_score:.1f}/100)")
                return crawl_result
                
            except AttributeError as e:
                logger.error(f"ğŸ”¥ Firecrawl ì‘ë‹µ ê°ì²´ ì†ì„± ì ‘ê·¼ ì˜¤ë¥˜: {e}")
                # ì‘ë‹µ ê°ì²´ì˜ ì‹¤ì œ êµ¬ì¡°ë¥¼ í™•ì¸í•˜ê¸° ìœ„í•œ ì¶”ê°€ ë¡œê¹…
                logger.error(f"ğŸ”¥ ì‘ë‹µ ê°ì²´ íƒ€ì…: {type(scrape_response)}")
                logger.error(f"ğŸ”¥ ì‘ë‹µ ê°ì²´ ë‚´ìš©: {scrape_response}")
                raise Exception(f"Firecrawl ì‘ë‹µ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            
        except Exception as e:
            logger.error(f"âŒ Firecrawl í¬ë¡¤ë§ ì‹¤íŒ¨: {url} - {e}")
            return CrawlResult(
                url=url,
                title="",
                text="",
                hierarchy={},
                metadata={
                    "crawler_used": "firecrawl",
                    "error_type": type(e).__name__,
                    "processing_time": "0s"
                },
                status="failed",
                timestamp=datetime.now(),
                error=str(e)
            ) 