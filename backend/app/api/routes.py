from fastapi import APIRouter, HTTPException, BackgroundTasks
from fastapi.responses import FileResponse
from pydantic import BaseModel, HttpUrl
from typing import List, Optional, Dict, Any
import logging
import json
import os
import uuid
from datetime import datetime
import asyncio
from ..crawlers.multi_engine import MultiEngineCrawler
from ..crawlers.base import CrawlStrategy, CrawlResult
from ..utils.natural_language_parser import nl_parser, SelectiveCrawlingIntent
from ..utils.error_formatter import format_crawling_error, get_simple_error_message
# Models are defined in this file directly

# ì „ì—­ í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ëŠ” ì˜ì¡´ì„± ì£¼ì…ìœ¼ë¡œ ì²˜ë¦¬
crawler_instance = None

def set_crawler_instance(instance):
    """í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ì„¤ì •"""
    global crawler_instance
    crawler_instance = instance

logger = logging.getLogger(__name__)

router = APIRouter(tags=["crawler"])

# Request Models
class SingleCrawlRequest(BaseModel):
    url: HttpUrl
    engine: Optional[str] = None  # íŠ¹ì • ì—”ì§„ ê°•ì œ ì§€ì •
    timeout: Optional[int] = 30
    anti_bot_mode: Optional[bool] = False
    job_id: Optional[str] = None  # WebSocket ì§„í–‰ë¥  ì¶”ì ìš©
    clean_text: Optional[bool] = True  # í…ìŠ¤íŠ¸ í›„ì²˜ë¦¬ ì ìš© ì—¬ë¶€

class BulkCrawlRequest(BaseModel):
    urls: List[HttpUrl]
    max_concurrent: Optional[int] = 5
    timeout: Optional[int] = 30
    clean_text: Optional[bool] = True  # í…ìŠ¤íŠ¸ í›„ì²˜ë¦¬ ì ìš© ì—¬ë¶€

class SmartCrawlRequest(BaseModel):
    text: str  # ìì—°ì–´ ì…ë ¥ (URL + ì¶”ì¶œ ìš”ì²­)
    timeout: Optional[int] = 30
    clean_text: Optional[bool] = True

# ğŸ¯ í†µí•© ìš”ì²­ ëª¨ë¸
class UnifiedCrawlRequest(BaseModel):
    text: str  # ëª¨ë“  í˜•íƒœì˜ ì…ë ¥ (URL, ìì—°ì–´, ë©€í‹° URL ë“±)
    engine: Optional[str] = None
    timeout: Optional[int] = 30
    clean_text: Optional[bool] = True
    job_id: Optional[str] = None

class CrawlResponse(BaseModel):
    url: str
    title: str
    text: str
    hierarchy: Dict[str, Any]
    metadata: Dict[str, Any]
    status: str
    timestamp: str
    error: Optional[str] = None

class SelectiveCrawlResponse(BaseModel):
    url: str
    target_content: str  # ì¶”ì¶œí•œ ì½˜í…ì¸  íƒ€ì…
    extracted_data: Dict[str, Any]  # ì‹¤ì œ ì¶”ì¶œëœ ë°ì´í„°
    # ğŸ”§ ë‹¨ì¼ í¬ë¡¤ë§ê³¼ ì¼ê´€ì„±ì„ ìœ„í•œ ì¶”ê°€ í•„ë“œ
    title: Optional[str] = None  # í˜ì´ì§€ ì œëª©
    full_text: Optional[str] = None  # í›„ì²˜ë¦¬ëœ ì „ì²´ í…ìŠ¤íŠ¸
    hierarchy: Optional[Dict[str, Any]] = None  # êµ¬ì¡°í™”ëœ ì •ë³´
    metadata: Dict[str, Any]
    status: str
    timestamp: str
    error: Optional[str] = None

# ğŸ¯ í†µí•© ì‘ë‹µ ëª¨ë¸ (ëª¨ë“  í¬ë¡¤ë§ íƒ€ì…ì„ í¬ê´„)
class UnifiedCrawlResponse(BaseModel):
    request_type: str  # "single", "bulk", "selective"
    input_text: str   # ì›ë³¸ ì…ë ¥
    status: str       # "complete", "processing", "failed"
    
    # ë‹¨ì¼ ê²°ê³¼ (single, selective)
    result: Optional[CrawlResponse] = None
    
    # ë‹¤ì¤‘ ê²°ê³¼ (bulk)
    results: Optional[List[CrawlResponse]] = None
    total_urls: Optional[int] = None
    successful_urls: Optional[int] = None
    failed_urls: Optional[int] = None
    job_id: Optional[str] = None
    
    # ê³µí†µ ë©”íƒ€ë°ì´í„°
    metadata: Dict[str, Any]
    timestamp: str
    error: Optional[str] = None

# ì§„í–‰ ì¤‘ì¸ ì‘ì—… ì¶”ì 
active_jobs = {}

@router.post("/crawl/single", response_model=CrawlResponse)
async def crawl_single_url(request: SingleCrawlRequest):
    """ë‹¨ì¼ URL í¬ë¡¤ë§"""
    global crawler_instance
    
    if not crawler_instance or not crawler_instance.is_initialized:
        raise HTTPException(status_code=503, detail="í¬ë¡¤ë§ ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    url = str(request.url)
    job_id = request.job_id or str(uuid.uuid4())[:8]
    
    logger.info(f"ğŸ“¡ ë‹¨ì¼ í¬ë¡¤ë§ ìš”ì²­: {url} [Job: {job_id}]")
    
    # WebSocket ì§„í–‰ë¥  ì „ì†¡ í•¨ìˆ˜ import
    from ..api.websocket import send_crawling_progress, send_crawling_complete, send_crawling_error
    
    try:
        # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸: ì‹œì‘
        await send_crawling_progress(job_id, "initializing", 10, "í¬ë¡¤ë§ ì‹œì‘ ì¤‘...")
        
        # ì‚¬ìš©ì ì •ì˜ ì „ëµ ìƒì„±
        from ..crawlers.base import CrawlStrategy
        
        if request.engine:
            # íŠ¹ì • ì—”ì§„ ê°•ì œ ì§€ì •
            if request.engine not in crawler_instance.engines:
                await send_crawling_error(job_id, f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì—”ì§„: {request.engine}")
                raise HTTPException(status_code=400, detail=f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì—”ì§„: {request.engine}")
            strategy = CrawlStrategy(
                engine_priority=[request.engine],
                timeout=request.timeout,
                anti_bot_mode=request.anti_bot_mode
            )
            await send_crawling_progress(job_id, "strategy", 25, f"ì—”ì§„ ì„¤ì •: {request.engine}")
        else:
            # ìë™ ì „ëµ ì„ íƒ
            strategy = None
            await send_crawling_progress(job_id, "strategy", 25, "ìµœì  ì—”ì§„ ìë™ ì„ íƒ ì¤‘...")
        
        # í¬ë¡¤ë§ ì‹¤í–‰
        logger.info(f"ğŸš€ API: crawl_with_strategy í˜¸ì¶œ ì‹œì‘ - URL: {url}")
        logger.info(f"ğŸš€ API: ì‚¬ìš©ì ì •ì˜ ì „ëµ: {strategy.engine_priority if strategy else 'ìë™ ì„ íƒ'}")
        logger.info(f"ğŸš€ API: í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ í™•ì¸ - engines: {list(crawler_instance.engines.keys())}")
        
        await send_crawling_progress(job_id, "crawling", 50, "ì›¹ í˜ì´ì§€ ì ‘ê·¼ ë° ë°ì´í„° ì¶”ì¶œ ì¤‘...")
        
        result = await crawler_instance.crawl_with_strategy(url, strategy)
        
        logger.info(f"ğŸš€ API: crawl_with_strategy ì™„ë£Œ - ê²°ê³¼: {result.status}")
        
        # ğŸ”§ í¬ë¡¤ë§ ì‹¤íŒ¨ ì‹œ HTTP ì—ëŸ¬ ì‘ë‹µ
        if result.status == "failed":
            raw_error = result.error or "í¬ë¡¤ë§ ì‹¤íŒ¨ (ì›ì¸ ë¶ˆëª…)"
            attempted_engines = result.metadata.get("attempted_engines", [])
            
            # ì‚¬ìš©ì ì¹œí™”ì ì¸ ì—ëŸ¬ ë©”ì‹œì§€ ìƒì„±
            user_friendly_error = format_crawling_error(raw_error, url, attempted_engines)
            simple_error = get_simple_error_message(raw_error)
            
            logger.error(f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {url} - {raw_error}")
            await send_crawling_error(job_id, simple_error)
            
            # ì‹¤íŒ¨ ê²°ê³¼ë„ íŒŒì¼ë¡œ ì €ì¥ (ë””ë²„ê¹…ìš©)
            result_file = f"results/failed_crawl_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            os.makedirs("results", exist_ok=True)
            
            result_dict = {
                "url": result.url,
                "title": result.title,
                "text": result.text,
                "hierarchy": result.hierarchy,
                "metadata": result.metadata,
                "status": result.status,
                "timestamp": result.timestamp.isoformat(),
                "error": raw_error,  # ì›ë³¸ ì—ëŸ¬ëŠ” íŒŒì¼ì— ì €ì¥
                "user_error": simple_error  # ì‚¬ìš©ì ì¹œí™”ì  ì—ëŸ¬ë„ ì €ì¥
            }
            
            with open(result_file, 'w', encoding='utf-8') as f:
                json.dump(result_dict, f, ensure_ascii=False, indent=2)
            
            raise HTTPException(
                status_code=422,  # Unprocessable Entity
                detail={
                    "message": simple_error,  # ì‚¬ìš©ì ì¹œí™”ì ì¸ ë©”ì‹œì§€
                    "url": url,
                    "error": simple_error,  # í”„ë¡ íŠ¸ì—”ë“œì—ì„œ í‘œì‹œí•  ë©”ì‹œì§€
                    "detailed_error": user_friendly_error,  # ìƒì„¸ ì •ë³´ (í•„ìš”ì‹œ)
                    "attempted_engines": attempted_engines,
                    "debug_file": result_file
                }
            )
        
        await send_crawling_progress(job_id, "processing", 80, "ë°ì´í„° ì²˜ë¦¬ ë° í’ˆì§ˆ ë¶„ì„ ì¤‘...")
        
        # í›„ì²˜ë¦¬ ì ìš©
        if request.clean_text:
            from ..utils.text_processor import post_process_crawl_result
            result = post_process_crawl_result(result, clean_text=True)
            logger.info(f"ğŸ§¹ í…ìŠ¤íŠ¸ í›„ì²˜ë¦¬ ì ìš© ì™„ë£Œ - ì••ì¶•ë¥ : {result.metadata.get('text_reduction_ratio', 1.0):.2f}")
        
        # ê²°ê³¼ ì €ì¥ (ì„±ê³µí•œ ê²½ìš°)
        result_file = f"results/single_crawl_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        os.makedirs("results", exist_ok=True)
        
        result_dict = {
            "url": result.url,
            "title": result.title,
            "text": result.text,
            "hierarchy": result.hierarchy,
            "metadata": result.metadata,
            "status": result.status,
            "timestamp": result.timestamp.isoformat(),
            "error": result.error
        }
        
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(result_dict, f, ensure_ascii=False, indent=2)
        
        logger.info(f"âœ… ë‹¨ì¼ í¬ë¡¤ë§ ì™„ë£Œ: {url} - í’ˆì§ˆ: {result.metadata.get('quality_score', 0):.1f}/100")
        
        response = CrawlResponse(
            url=result.url,
            title=result.title,
            text=result.text,
            hierarchy=result.hierarchy,
            metadata=result.metadata,
            status=result.status,
            timestamp=result.timestamp.isoformat(),
            error=result.error
        )
        
        # WebSocket ì™„ë£Œ ì•Œë¦¼
        await send_crawling_complete(job_id, {
            "status": result.status,
            "quality_score": result.metadata.get('quality_score', 0),
            "engine_used": result.metadata.get('engine_used', 'unknown'),
            "title": result.title,
            "text_length": len(result.text),
            "response": response.dict()
        })
        
        return response
        
    except HTTPException:
        # HTTPExceptionì€ ê·¸ëŒ€ë¡œ ì¬ë°œìƒ
        raise
    except Exception as e:
        raw_error = str(e)
        simple_error = get_simple_error_message(raw_error)
        
        logger.error(f"âŒ ë‹¨ì¼ í¬ë¡¤ë§ ì‹¤íŒ¨: {url} - {raw_error}")
        await send_crawling_error(job_id, simple_error)
        raise HTTPException(status_code=500, detail=simple_error)

@router.post("/crawl/bulk")
async def crawl_bulk_urls(request: BulkCrawlRequest, background_tasks: BackgroundTasks):
    """ëŒ€ëŸ‰ URL í¬ë¡¤ë§ (ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬)"""
    global crawler_instance
    
    if not crawler_instance or not crawler_instance.is_initialized:
        raise HTTPException(status_code=503, detail="í¬ë¡¤ë§ ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    urls = [str(url) for url in request.urls]
    job_id = f"bulk_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    
    logger.info(f"ğŸ“¦ ëŒ€ëŸ‰ í¬ë¡¤ë§ ìš”ì²­: {len(urls)}ê°œ URL, ì‘ì—… ID: {job_id}")
    
    # ì‘ì—… ìƒíƒœ ì´ˆê¸°í™”
    active_jobs[job_id] = {
        "status": "started",
        "total_urls": len(urls),
        "completed": 0,
        "success": 0,
        "failed": 0,
        "start_time": datetime.now(),
        "progress": 0
    }
    
    async def process_bulk_crawl():
        """ë°±ê·¸ë¼ìš´ë“œì—ì„œ ëŒ€ëŸ‰ í¬ë¡¤ë§ ì²˜ë¦¬"""
        global crawler_instance, active_jobs
        
        # ğŸ”§ WebSocket í•¨ìˆ˜ë“¤ì„ ë‚´ë¶€ì—ì„œ import
        from ..api.websocket import send_crawling_progress, send_crawling_complete, send_crawling_error
        
        if not crawler_instance or not crawler_instance.is_initialized:
            logger.error(f"âŒ í¬ë¡¤ë§ ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ: {job_id}")
            await send_crawling_error(job_id, "í¬ë¡¤ë§ ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            return
        
        try:
            await send_crawling_progress(job_id, "starting", 5, f"ğŸš€ {len(urls)}ê°œ URL í¬ë¡¤ë§ ì‹œì‘...")
            
            # ê°œë³„ URL í¬ë¡¤ë§ í•¨ìˆ˜ (ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ í¬í•¨)
            completed_count = 0
            success_count = 0
            results = []
            
            semaphore = asyncio.Semaphore(request.max_concurrent)
            
            async def crawl_single_with_progress(url: str, index: int) -> CrawlResult:
                nonlocal completed_count, success_count
                
                async with semaphore:
                    try:
                        await send_crawling_progress(
                            job_id, 
                            "crawling", 
                            10 + int((completed_count / len(urls)) * 80),  # 10-90% ë²”ìœ„
                            f"ğŸ“¡ í¬ë¡¤ë§ ì¤‘: {url[:50]}{'...' if len(url) > 50 else ''}"
                        )
                        
                        result = await crawler_instance.crawl_with_strategy(url)
                        
                        # í›„ì²˜ë¦¬ ì ìš© (ìš”ì²­ëœ ê²½ìš°)
                        if request.clean_text:
                            from ..utils.text_processor import post_process_crawl_result
                            result = post_process_crawl_result(result, clean_text=True)
                        
                        completed_count += 1
                        if result.status == "complete":
                            success_count += 1
                        
                        # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                        progress = 10 + int((completed_count / len(urls)) * 80)
                        await send_crawling_progress(
                            job_id,
                            "processing",
                            progress,
                            f"âœ… ì™„ë£Œ: {completed_count}/{len(urls)} (ì„±ê³µ: {success_count})"
                        )
                        
                        # ì‘ì—… ìƒíƒœ ì—…ë°ì´íŠ¸
                        active_jobs[job_id].update({
                            "completed": completed_count,
                            "success": success_count,
                            "failed": completed_count - success_count,
                            "progress": progress
                        })
                        
                        return result
                        
                    except Exception as e:
                        completed_count += 1
                        logger.error(f"âŒ URL í¬ë¡¤ë§ ì‹¤íŒ¨: {url} - {e}")
                        
                        # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ (ì‹¤íŒ¨ í¬í•¨)
                        progress = 10 + int((completed_count / len(urls)) * 80)
                        await send_crawling_progress(
                            job_id,
                            "processing",
                            progress,
                            f"âš ï¸ ì§„í–‰: {completed_count}/{len(urls)} (ì„±ê³µ: {success_count})"
                        )
                        
                        # ì‘ì—… ìƒíƒœ ì—…ë°ì´íŠ¸
                        active_jobs[job_id].update({
                            "completed": completed_count,
                            "success": success_count,
                            "failed": completed_count - success_count,
                            "progress": progress
                        })
                        
                        # ì‹¤íŒ¨í•œ ê²°ê³¼ ë°˜í™˜
                        return CrawlResult(
                            url=url,
                            title="",
                            text="",
                            hierarchy={},
                            metadata={"error": str(e)},
                            status="failed",
                            timestamp=datetime.now(),
                            error=str(e)
                        )
            
            # ëª¨ë“  URLì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬
            tasks = [crawl_single_with_progress(url, i) for i, url in enumerate(urls)]
            results = await asyncio.gather(*tasks)
            
            await send_crawling_progress(job_id, "finalizing", 95, "ğŸ“Š ê²°ê³¼ ì €ì¥ ì¤‘...")
            
            # ê²°ê³¼ íŒŒì¼ ì €ì¥
            result_file = f"results/bulk_crawl_{job_id}.json"
            os.makedirs("results", exist_ok=True)
            
            results_data = []
            
            for result in results:
                result_dict = {
                    "url": result.url,
                    "title": result.title,
                    "text": result.text,
                    "hierarchy": result.hierarchy,
                    "metadata": result.metadata,
                    "status": result.status,
                    "timestamp": result.timestamp.isoformat(),
                    "error": result.error
                }
                results_data.append(result_dict)
            
            # ê²°ê³¼ ìš”ì•½
            summary = {
                "job_id": job_id,
                "total_urls": len(urls),
                "successful": success_count,
                "failed": len(urls) - success_count,
                "success_rate": (success_count / len(urls)) * 100,
                "start_time": active_jobs[job_id]["start_time"].isoformat(),
                "end_time": datetime.now().isoformat(),
                "results": results_data
            }
            
            with open(result_file, 'w', encoding='utf-8') as f:
                json.dump(summary, f, ensure_ascii=False, indent=2)
            
            # ì‘ì—… ìƒíƒœ ì—…ë°ì´íŠ¸
            active_jobs[job_id].update({
                "status": "completed",
                "completed": len(urls),
                "success": success_count,
                "failed": len(urls) - success_count,
                "end_time": datetime.now(),
                "result_file": result_file,
                "progress": 100
            })
            
            await send_crawling_progress(job_id, "completed", 100, f"ğŸ‰ ì™„ë£Œ! ì„±ê³µ: {success_count}/{len(urls)}")
            
            logger.info(f"ğŸ“Š ëŒ€ëŸ‰ í¬ë¡¤ë§ ì™„ë£Œ: {job_id} - ì„±ê³µë¥ : {(success_count/len(urls)*100):.1f}%")
            
        except Exception as e:
            logger.error(f"ğŸ’¥ ëŒ€ëŸ‰ í¬ë¡¤ë§ ì‹¤íŒ¨: {job_id} - {e}")
            active_jobs[job_id].update({
                "status": "failed",
                "error": str(e),
                "end_time": datetime.now()
            })
            await send_crawling_error(job_id, f"ëŒ€ëŸ‰ í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}")
    
    # ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì‹œì‘
    background_tasks.add_task(process_bulk_crawl)
    
    return {
        "job_id": job_id,
        "message": f"{len(urls)}ê°œ URLì˜ ëŒ€ëŸ‰ í¬ë¡¤ë§ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤",
        "status": "started",
        "total_urls": len(urls)
    }

@router.get("/jobs/{job_id}/status")
async def get_job_status(job_id: str):
    """ì‘ì—… ì§„í–‰ ìƒíƒœ ì¡°íšŒ"""
    if job_id not in active_jobs:
        raise HTTPException(status_code=404, detail="ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    
    job_info = active_jobs[job_id].copy()
    
    # datetime ê°ì²´ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜ (ì´ë¯¸ ë¬¸ìì—´ì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ìœ ì§€)
    if "start_time" in job_info and hasattr(job_info["start_time"], 'isoformat'):
        job_info["start_time"] = job_info["start_time"].isoformat()
    if "end_time" in job_info and hasattr(job_info["end_time"], 'isoformat'):
        job_info["end_time"] = job_info["end_time"].isoformat()
    
    return job_info

@router.get("/jobs/{job_id}/download")
async def download_job_result(job_id: str):
    """ì‘ì—… ê²°ê³¼ íŒŒì¼ ë‹¤ìš´ë¡œë“œ"""
    if job_id not in active_jobs:
        raise HTTPException(status_code=404, detail="ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    
    job_info = active_jobs[job_id]
    
    if job_info["status"] != "completed":
        raise HTTPException(status_code=400, detail="ì‘ì—…ì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    if "result_file" not in job_info:
        raise HTTPException(status_code=404, detail="ê²°ê³¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    
    result_file = job_info["result_file"]
    
    if not os.path.exists(result_file):
        raise HTTPException(status_code=404, detail="ê²°ê³¼ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
    
    return FileResponse(
        path=result_file,
        filename=f"crawl_results_{job_id}.json",
        media_type="application/json"
    )

@router.get("/jobs/{job_id}/results")
async def get_job_results(job_id: str):
    """ì‘ì—… ê²°ê³¼ ë°ì´í„° ì¡°íšŒ (JSON í˜•íƒœ)"""
    if job_id not in active_jobs:
        raise HTTPException(status_code=404, detail="ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    
    job_info = active_jobs[job_id]
    
    if job_info["status"] != "completed":
        raise HTTPException(status_code=400, detail="ì‘ì—…ì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    if "result_file" not in job_info:
        raise HTTPException(status_code=404, detail="ê²°ê³¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    
    result_file = job_info["result_file"]
    
    if not os.path.exists(result_file):
        raise HTTPException(status_code=404, detail="ê²°ê³¼ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
    
    try:
        with open(result_file, 'r', encoding='utf-8') as f:
            file_data = json.load(f)
        
        # í”„ë¡ íŠ¸ì—”ë“œê°€ ê¸°ëŒ€í•˜ëŠ” êµ¬ì¡°ë¡œ ë³€í™˜
        formatted_data = {
            "summary": {
                "job_id": file_data.get("job_id"),
                "total_urls": file_data.get("total_urls"),
                "successful_urls": file_data.get("successful"),
                "failed_urls": file_data.get("failed"),
                "success_rate": file_data.get("success_rate"),
                "status": "completed",
                "start_time": file_data.get("start_time"),
                "end_time": file_data.get("end_time")
            },
            "results": file_data.get("results", [])
        }
        
        return formatted_data
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ê²°ê³¼ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {str(e)}")

@router.get("/engines/status")
async def get_engines_status():
    """ëª¨ë“  í¬ë¡¤ë§ ì—”ì§„ ìƒíƒœ ì¡°íšŒ"""
    global crawler_instance
    
    if not crawler_instance:
        raise HTTPException(status_code=503, detail="í¬ë¡¤ë§ ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    status = await crawler_instance.get_engine_status()
    return status

@router.get("/jobs/active")
async def get_active_jobs():
    """í˜„ì¬ í™œì„± ì‘ì—… ëª©ë¡ ì¡°íšŒ"""
    active_job_list = []
    
    for job_id, job_info in active_jobs.items():
        job_copy = job_info.copy()
        
        # datetime ê°ì²´ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜ (ì´ë¯¸ ë¬¸ìì—´ì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ìœ ì§€)
        if "start_time" in job_copy and hasattr(job_copy["start_time"], 'isoformat'):
            job_copy["start_time"] = job_copy["start_time"].isoformat()
        if "end_time" in job_copy and hasattr(job_copy["end_time"], 'isoformat'):
            job_copy["end_time"] = job_copy["end_time"].isoformat()
        
        job_copy["job_id"] = job_id
        active_job_list.append(job_copy)
    
    return {
        "total_jobs": len(active_job_list),
        "jobs": active_job_list
    }

@router.delete("/jobs/{job_id}")
async def cancel_job(job_id: str):
    """ì‘ì—… ì·¨ì†Œ (ì§„í–‰ ì¤‘ì¸ ì‘ì—…ì€ ì™„ë£Œê¹Œì§€ ê¸°ë‹¤ë¦¼)"""
    if job_id not in active_jobs:
        raise HTTPException(status_code=404, detail="ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    
    job_info = active_jobs[job_id]
    
    if job_info["status"] in ["completed", "failed"]:
        # ì™„ë£Œëœ ì‘ì—…ì€ ê¸°ë¡ì—ì„œ ì œê±°
        del active_jobs[job_id]
        return {"message": f"ì‘ì—… {job_id}ê°€ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤"}
    else:
        return {"message": f"ì‘ì—… {job_id}ëŠ” ì§„í–‰ ì¤‘ì´ë¯€ë¡œ ì™„ë£Œ í›„ ì œê±°ë©ë‹ˆë‹¤"}

@router.post("/test/simple")
async def test_simple_crawl():
    """ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ í¬ë¡¤ë§"""
    test_url = "https://httpbin.org/html"
    
    request = SingleCrawlRequest(url=test_url)
    result = await crawl_single_url(request)
    
    return {
        "message": "í…ŒìŠ¤íŠ¸ í¬ë¡¤ë§ ì™„ë£Œ",
        "test_url": test_url,
        "result_summary": {
            "status": result.status,
            "title": result.title,
            "text_length": len(result.text),
            "crawler_used": result.metadata.get("crawler_used"),
            "quality_score": result.metadata.get("quality_score")
        }
    }

@router.post("/crawl/smart", response_model=SelectiveCrawlResponse)
async def smart_natural_crawl(request: SmartCrawlRequest):
    """ìì—°ì–´ ê¸°ë°˜ ìŠ¤ë§ˆíŠ¸ í¬ë¡¤ë§ (ì„ íƒì  ì¶”ì¶œ)"""
    global crawler_instance
    
    if not crawler_instance or not crawler_instance.is_initialized:
        raise HTTPException(status_code=503, detail="í¬ë¡¤ë§ ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    job_id = str(uuid.uuid4())[:8]
    logger.info(f"ğŸ§  ìŠ¤ë§ˆíŠ¸ í¬ë¡¤ë§ ìš”ì²­: {request.text} [Job: {job_id}]")
    
    # WebSocket ì§„í–‰ë¥  ì „ì†¡ í•¨ìˆ˜ import
    from ..api.websocket import send_crawling_progress, send_crawling_complete, send_crawling_error
    
    try:
        # 1ë‹¨ê³„: ìì—°ì–´ íŒŒì‹±
        await send_crawling_progress(job_id, "parsing", 15, "ğŸ” ìì—°ì–´ ë¶„ì„ ì¤‘...")
        
        intent = nl_parser.parse_selective_request(request.text)
        validation = nl_parser.validate_intent(intent)
        
        if not validation["is_valid"]:
            error_msg = validation["message"]
            await send_crawling_error(job_id, error_msg)
            raise HTTPException(status_code=400, detail=error_msg)
        
        # í”¼ë“œë°± ë©”ì‹œì§€
        await send_crawling_progress(
            job_id, "intent_confirmed", 30, 
            validation["message"]
        )
        
        # 2ë‹¨ê³„: URL í¬ë¡¤ë§
        url = intent.urls[0]  # ì²« ë²ˆì§¸ URL ì‚¬ìš©
        await send_crawling_progress(
            job_id, "crawling", 50, 
            f"ğŸ“¡ í¬ë¡¤ë§ ì‹œì‘: {url}"
        )
        
        # ê¸°ë³¸ í¬ë¡¤ë§ ì‹¤í–‰
        result = await crawler_instance.crawl_with_strategy(
            url=intent.urls[0],
            custom_strategy=None  # ìë™ ì „ëµ ì„ íƒ
        )
        
        if not result or result.status != "complete":
            error_msg = f"í¬ë¡¤ë§ ì‹¤íŒ¨: {result.error if result else 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜'}"
            await send_crawling_error(job_id, error_msg)
            raise HTTPException(status_code=500, detail=error_msg)
        
        # ğŸ§¹ í›„ì²˜ë¦¬ ì ìš© (ë‹¨ì¼/ë©€í‹° í¬ë¡¤ë§ê³¼ ë™ì¼í•˜ê²Œ)
        if request.clean_text:
            await send_crawling_progress(
                job_id, "post_processing", 65, 
                "ğŸ§¹ í…ìŠ¤íŠ¸ í›„ì²˜ë¦¬ ì ìš© ì¤‘..."
            )
            
            from ..utils.text_processor import post_process_crawl_result
            result = post_process_crawl_result(result, clean_text=True)
            logger.info(f"ğŸ§¹ ìŠ¤ë§ˆíŠ¸ í¬ë¡¤ë§ í›„ì²˜ë¦¬ ì™„ë£Œ - ì••ì¶•ë¥ : {result.metadata.get('text_reduction_ratio', 1.0):.2f}")
        
        # 3ë‹¨ê³„: ì„ íƒì  ì½˜í…ì¸  ì¶”ì¶œ
        await send_crawling_progress(
            job_id, "extracting", 75, 
            f"ğŸ¯ '{intent.target_content}' ì¶”ì¶œ ì¤‘..."
        )
        
        # MCP í´ë¼ì´ì–¸íŠ¸ë¥¼ í†µí•œ ì„ íƒì  ì¶”ì¶œ
        extraction_result = await crawler_instance.mcp_client.extract_selective_content(
            html_content=result.text,  # í¬ë¡¤ë§ëœ ë‚´ìš©
            target_content=intent.target_content,
            url=url
        )
        
        if "error" in extraction_result:
            error_msg = f"ì¶”ì¶œ ì‹¤íŒ¨: {extraction_result['error']}"
            await send_crawling_error(job_id, error_msg)
            raise HTTPException(status_code=500, detail=error_msg)
        
        # 4ë‹¨ê³„: ê²°ê³¼ ì €ì¥
        await send_crawling_progress(job_id, "saving", 90, "ğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘...")
        
        result_file = f"results/smart_crawl_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        os.makedirs("results", exist_ok=True)
        
        response_data = SelectiveCrawlResponse(
            url=url,
            target_content=intent.target_content,
            extracted_data=extraction_result.get("extracted_data", {}),
            # ğŸ”§ ë‹¨ì¼ í¬ë¡¤ë§ê³¼ ì¼ê´€ì„±ì„ ìœ„í•œ ì¶”ê°€ ì •ë³´
            title=result.title,
            full_text=result.text,  # í›„ì²˜ë¦¬ëœ ì „ì²´ í…ìŠ¤íŠ¸
            hierarchy=result.hierarchy,
            metadata={
                # ğŸ”§ í¬ë¡¤ë§ ì—”ì§„ ì •ë³´ - result.metadataì—ì„œ ì˜¬ë°”ë¥´ê²Œ ê°€ì ¸ì˜¤ê¸°
                "engine_used": result.metadata.get("engine_used") or result.metadata.get("crawler_used", "unknown"),
                "crawler_used": result.metadata.get("crawler_used", "unknown"),
                
                # ğŸ”§ ì²˜ë¦¬ì‹œê°„ ì •ë³´
                "processing_time": result.metadata.get("processing_time", "N/A"),
                "execution_time": result.metadata.get("execution_time"),
                
                # ğŸ”§ í’ˆì§ˆ ì •ë³´ - extraction_resultì™€ original_crawling_metadataì—ì„œ ìµœì ê°’ ì„ íƒ
                "quality_score": result.metadata.get("quality_score") or extraction_result.get("quality_score", 0),
                "content_quality": result.metadata.get("content_quality", "medium"),
                "confidence": result.metadata.get("confidence") or result.metadata.get("extraction_confidence") or (result.metadata.get("quality_score", 0) / 100.0) or 0.5,
                "extraction_confidence": result.metadata.get("extraction_confidence") or result.metadata.get("confidence") or (result.metadata.get("quality_score", 0) / 100.0) or 0.5,
                
                # ğŸ”§ ì„ íƒì  í¬ë¡¤ë§ íŠ¹í™” ì •ë³´
                "crawling_mode": "selective",
                "target_content": intent.target_content,
                "extraction_type": intent.extraction_type,
                "selective_crawling_mode": True,
                "extracted_data": extraction_result.get("extracted_data", {}),
                
                # ğŸ”§ ê¸°íƒ€ ë©”íƒ€ë°ì´í„°
                "intent_confidence": intent.confidence,
                "raw_request": intent.raw_request,
                "text_length": len(result.text),
                "post_processing_applied": request.clean_text,
                
                # ğŸ”§ ì›ë³¸ í¬ë¡¤ë§ ë©”íƒ€ë°ì´í„° í¬í•¨
                "original_crawling_metadata": result.metadata
            },
            status="complete",
            timestamp=datetime.now().isoformat()
        )
        
        # ê²°ê³¼ íŒŒì¼ ì €ì¥
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(response_data.dict(), f, ensure_ascii=False, indent=2)
        
        # ì™„ë£Œ ì•Œë¦¼
        await send_crawling_complete(job_id, {
            "status": "complete",
            "target_content": intent.target_content,
            "extraction_quality": extraction_result.get("quality_score", 0.0),
            "url": url,
            "response": response_data.dict()
        })
        
        logger.info(f"âœ… ìŠ¤ë§ˆíŠ¸ í¬ë¡¤ë§ ì™„ë£Œ: {url} -> {intent.target_content}")
        return response_data
        
    except HTTPException:
        # ì´ë¯¸ ì²˜ë¦¬ëœ HTTP ì˜ˆì™¸ëŠ” ë‹¤ì‹œ ë°œìƒ
        raise
    except Exception as e:
        logger.error(f"âŒ ìŠ¤ë§ˆíŠ¸ í¬ë¡¤ë§ ì‹¤íŒ¨: {request.text} - {e}")
        await send_crawling_error(job_id, str(e))
        raise HTTPException(status_code=500, detail=f"ìŠ¤ë§ˆíŠ¸ í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}")

@router.post("/parse/intent")
async def parse_natural_language_intent(request: SmartCrawlRequest):
    """ìì—°ì–´ ì˜ë„ íŒŒì‹± (í…ŒìŠ¤íŠ¸ìš©)"""
    try:
        intent = nl_parser.parse_selective_request(request.text)
        validation = nl_parser.validate_intent(intent)
        
        return {
            "raw_text": request.text,
            "parsed_intent": {
                "urls": intent.urls,
                "target_content": intent.target_content,
                "confidence": intent.confidence,
                "extraction_type": intent.extraction_type
            },
            "validation": validation,
            "suggestions": validation.get("suggestions", [])
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì˜ë„ íŒŒì‹± ì‹¤íŒ¨: {str(e)}")

# ğŸ¯ í†µí•© í¬ë¡¤ë§ ì—”ë“œí¬ì¸íŠ¸
@router.post("/crawl/unified", response_model=UnifiedCrawlResponse)
async def unified_crawl(request: UnifiedCrawlRequest):
    """
    í†µí•© í¬ë¡¤ë§ ì—”ë“œí¬ì¸íŠ¸
    ëª¨ë“  í˜•íƒœì˜ ì…ë ¥ì„ ë°›ì•„ì„œ ì˜ë„ë¥¼ ë¶„ì„í•˜ê³  ì ì ˆí•œ ì²˜ë¦¬ ë°©ì‹ìœ¼ë¡œ ë¼ìš°íŒ…
    """
    global crawler_instance
    
    if not crawler_instance or not crawler_instance.is_initialized:
        raise HTTPException(status_code=503, detail="í¬ë¡¤ë§ ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    job_id = request.job_id or str(uuid.uuid4())[:8]
    logger.info(f"ğŸ¯ í†µí•© í¬ë¡¤ë§ ìš”ì²­: {request.text} [Job: {job_id}]")
    
    # WebSocket ì§„í–‰ë¥  ì „ì†¡ í•¨ìˆ˜ import
    from ..api.websocket import send_crawling_progress, send_crawling_complete, send_crawling_error
    
    try:
        # 1ë‹¨ê³„: í†µí•© ì˜ë„ ë¶„ì„
        await send_crawling_progress(job_id, "analyzing", 10, "ğŸ§  ì…ë ¥ ë¶„ì„ ì¤‘...")
        
        intent = nl_parser.analyze_unified_intent(request.text)
        
        logger.info(f"ğŸ¯ ì˜ë„ ë¶„ì„ ê²°ê³¼: {intent.request_type} (ì‹ ë¢°ë„: {intent.confidence:.2f})")
        
        # 2ë‹¨ê³„: ì˜ë„ì— ë”°ë¥¸ ë¼ìš°íŒ…
        await send_crawling_progress(
            job_id, "routing", 20, 
            f"ğŸ“ ì²˜ë¦¬ ë°©ì‹ ê²°ì •: {intent.request_type}"
        )
        
        if intent.request_type == "invalid":
            error_msg = intent.metadata.get("error", "ìœ íš¨í•˜ì§€ ì•Šì€ ìš”ì²­ì…ë‹ˆë‹¤")
            await send_crawling_error(job_id, error_msg)
            raise HTTPException(status_code=400, detail=error_msg)
        
        elif intent.request_type == "single":
            # ë‹¨ì¼ URL í¬ë¡¤ë§ìœ¼ë¡œ ë¼ìš°íŒ…
            result = await _handle_single_crawl_internal(
                intent.urls[0], request.engine, request.timeout, 
                request.clean_text, job_id
            )
            
            return UnifiedCrawlResponse(
                request_type="single",
                input_text=request.text,
                result=result,
                metadata={
                    "intent_confidence": intent.confidence,
                    "processing_route": "single_crawl",
                    **intent.metadata
                },
                status="complete",
                timestamp=datetime.now().isoformat()
            )
        
        elif intent.request_type == "bulk":
            # ë©€í‹° URL í¬ë¡¤ë§ìœ¼ë¡œ ë¼ìš°íŒ… (ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬)
            # ğŸ”§ ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬ ì‹œì‘ë§Œ í•˜ê³  ì¦‰ì‹œ ì‘ë‹µ
            await _handle_bulk_crawl_internal(
                intent.urls, request.timeout, request.clean_text, job_id
            )
            
            return UnifiedCrawlResponse(
                request_type="bulk",
                input_text=request.text,
                results=None,  # ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬ ì¤‘ì´ë¯€ë¡œ None
                total_urls=len(intent.urls),
                successful_urls=0,  # ì•„ì§ ì²˜ë¦¬ ì¤‘
                failed_urls=0,      # ì•„ì§ ì²˜ë¦¬ ì¤‘
                job_id=job_id,
                metadata={
                    "intent_confidence": intent.confidence,
                    "processing_route": "bulk_crawl",
                    "url_count": len(intent.urls),
                    "background_processing": True,
                    **intent.metadata
                },
                status="processing",  # ğŸ”§ ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬ ì¤‘ì´ë¯€ë¡œ processing
                timestamp=datetime.now().isoformat()
            )
        
        elif intent.request_type == "selective":
            # ì„ íƒì  í¬ë¡¤ë§ìœ¼ë¡œ ë¼ìš°íŒ…
            selective_result = await _handle_selective_crawl_internal(
                intent.urls[0], intent.target_content, request.timeout,
                request.clean_text, job_id
            )
            
            return UnifiedCrawlResponse(
                request_type="selective",
                input_text=request.text,
                result=selective_result,
                metadata={
                    "intent_confidence": intent.confidence,
                    "processing_route": "selective_crawl",
                    "target_content": intent.target_content,
                    **intent.metadata
                },
                status="complete",
                timestamp=datetime.now().isoformat()
            )
        
        elif intent.request_type == "search":
            # ê²€ìƒ‰ í¬ë¡¤ë§ (ë¯¸ë˜ ê¸°ëŠ¥)
            await send_crawling_error(
                job_id, 
                f"'{intent.platform}'ì—ì„œ '{intent.search_query}' ê²€ìƒ‰ ê¸°ëŠ¥ì€ ì•„ì§ êµ¬í˜„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"
            )
            raise HTTPException(
                status_code=501, 
                detail=f"í”Œë«í¼ ê²€ìƒ‰ ê¸°ëŠ¥ì€ ì•„ì§ êµ¬í˜„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. (ìš”ì²­: {intent.platform} - {intent.search_query})"
            )
        
        else:
            # ì•Œ ìˆ˜ ì—†ëŠ” ìš”ì²­ íƒ€ì…
            await send_crawling_error(job_id, f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ìš”ì²­ íƒ€ì…: {intent.request_type}")
            raise HTTPException(status_code=400, detail=f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ìš”ì²­ íƒ€ì…: {intent.request_type}")
        
    except HTTPException:
        # ì´ë¯¸ ì²˜ë¦¬ëœ HTTP ì˜ˆì™¸ëŠ” ë‹¤ì‹œ ë°œìƒ
        raise
    except Exception as e:
        raw_error = str(e)
        simple_error = get_simple_error_message(raw_error)
        
        logger.error(f"âŒ í†µí•© í¬ë¡¤ë§ ì‹¤íŒ¨: {request.text} - {raw_error}")
        await send_crawling_error(job_id, simple_error)
        raise HTTPException(status_code=500, detail=simple_error)

# ğŸ”§ ë‚´ë¶€ ì²˜ë¦¬ í•¨ìˆ˜ë“¤ (ê¸°ì¡´ ë¡œì§ ì¬ì‚¬ìš©)
async def _handle_single_crawl_internal(
    url: str, engine: Optional[str], timeout: int, 
    clean_text: bool, job_id: str
) -> CrawlResponse:
    """ë‹¨ì¼ í¬ë¡¤ë§ ë‚´ë¶€ ì²˜ë¦¬"""
    # ê¸°ì¡´ crawl_single_url ë¡œì§ ì¬ì‚¬ìš©
    single_request = SingleCrawlRequest(
        url=url, engine=engine, timeout=timeout, 
        clean_text=clean_text, job_id=job_id
    )
    
    try:
        return await crawl_single_url(single_request)
    except HTTPException as e:
        # HTTPExceptionì˜ ê²½ìš° ìƒì„¸ ì •ë³´ë¥¼ í¬í•¨í•œ ì‘ë‹µ ìƒì„±
        if e.status_code == 422:  # í¬ë¡¤ë§ ì‹¤íŒ¨
            error_detail = e.detail
            if isinstance(error_detail, dict):
                error_msg = error_detail.get("error", "í¬ë¡¤ë§ ì‹¤íŒ¨")
                attempted_engines = error_detail.get("attempted_engines", [])
            else:
                error_msg = str(error_detail)
                attempted_engines = []
            
            # ì‹¤íŒ¨ ì‘ë‹µì„ CrawlResponse í˜•íƒœë¡œ ë³€í™˜
            return CrawlResponse(
                url=url,
                title="",
                text="",
                hierarchy={},
                metadata={
                    "error": error_msg,
                    "attempted_engines": attempted_engines,
                    "all_engines_failed": True,
                    "failure_reason": "crawling_failed"
                },
                status="failed",
                timestamp=datetime.now().isoformat(),
                error=error_msg  # ì´ë¯¸ í¬ë§·ëœ ì‚¬ìš©ì ì¹œí™”ì  ë©”ì‹œì§€
            )
        else:
            # ë‹¤ë¥¸ HTTP ì—ëŸ¬ëŠ” ê·¸ëŒ€ë¡œ ì¬ë°œìƒ
            raise

async def _handle_bulk_crawl_internal(
    urls: List[str], timeout: int, clean_text: bool, job_id: str
) -> None:
    """ë©€í‹° í¬ë¡¤ë§ ë‚´ë¶€ ì²˜ë¦¬ - ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬ë§Œ ì‹œì‘"""
    global crawler_instance, active_jobs
    
    if not crawler_instance or not crawler_instance.is_initialized:
        raise HTTPException(status_code=503, detail="í¬ë¡¤ë§ ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    # WebSocket ì§„í–‰ë¥  ì „ì†¡ í•¨ìˆ˜ import
    from ..api.websocket import send_crawling_progress, send_crawling_complete, send_crawling_error
    
    # ì‘ì—… ìƒíƒœ ì´ˆê¸°í™”
    active_jobs[job_id] = {
        "status": "processing",
        "total_urls": len(urls),
        "completed": 0,
        "success": 0,
        "failed": 0,
        "results": [],
        "start_time": datetime.now().isoformat(),
        "urls": urls
    }
    
    logger.info(f"ğŸš€ í†µí•© ëŒ€ëŸ‰ í¬ë¡¤ë§ ì‹œì‘: {len(urls)}ê°œ URL [Job: {job_id}]")
    
    # ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì •ì˜
    async def process_bulk_crawl():
        """ë°±ê·¸ë¼ìš´ë“œ ëŒ€ëŸ‰ í¬ë¡¤ë§ ì²˜ë¦¬"""
        logger.info(f"ğŸ”¥ process_bulk_crawl ì‹œì‘: {job_id}")
        try:
            logger.info(f"ğŸ“¡ WebSocket ì§„í–‰ë¥  ì „ì†¡ ì‹œë„: {job_id}")
            await send_crawling_progress(job_id, "starting", 5, f"ğŸš€ {len(urls)}ê°œ URL í¬ë¡¤ë§ ì‹œì‘")
            
            # ë™ì‹œ í¬ë¡¤ë§ í•¨ìˆ˜
            async def crawl_single_with_progress(url: str, index: int):
                try:
                    logger.info(f"ğŸ” ê°œë³„ í¬ë¡¤ë§ ì‹œì‘: {url} [{index+1}/{len(urls)}]")
                    
                    # ê°œë³„ í¬ë¡¤ë§ ì‹¤í–‰
                    single_request = SingleCrawlRequest(
                        url=url, 
                        timeout=timeout,
                        clean_text=clean_text,
                        job_id=f"{job_id}-{index}"  # ê°œë³„ ì‘ì—… ID
                    )
                    
                    result = await crawl_single_url(single_request)
                    
                    # ì„±ê³µ ì²˜ë¦¬
                    active_jobs[job_id]["completed"] += 1
                    active_jobs[job_id]["success"] += 1
                    active_jobs[job_id]["results"].append(result.dict())
                    
                    # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                    progress = int((active_jobs[job_id]["completed"] / len(urls)) * 100)
                    await send_crawling_progress(
                        job_id, "processing", progress,
                        f"âœ… {active_jobs[job_id]['completed']}/{len(urls)} ì™„ë£Œ"
                    )
                    
                    logger.info(f"âœ… ê°œë³„ í¬ë¡¤ë§ ì„±ê³µ: {url}")
                    return result
                    
                except Exception as e:
                    # ì‹¤íŒ¨ ì²˜ë¦¬
                    active_jobs[job_id]["completed"] += 1
                    active_jobs[job_id]["failed"] += 1
                    
                    error_result = CrawlResponse(
                        url=url,
                        title="",
                        text="",
                        hierarchy={},
                        metadata={"error": str(e)},
                        status="failed",
                        timestamp=datetime.now().isoformat(),
                        error=str(e)
                    )
                    active_jobs[job_id]["results"].append(error_result.dict())
                    
                    # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                    progress = int((active_jobs[job_id]["completed"] / len(urls)) * 100)
                    await send_crawling_progress(
                        job_id, "processing", progress,
                        f"âŒ {active_jobs[job_id]['completed']}/{len(urls)} ì™„ë£Œ (ì‹¤íŒ¨: {url})"
                    )
                    
                    logger.error(f"âŒ ê°œë³„ í¬ë¡¤ë§ ì‹¤íŒ¨: {url} - {e}")
                    return error_result
            
            # ë™ì‹œ ì‹¤í–‰ (ìµœëŒ€ 3ê°œ)
            semaphore = asyncio.Semaphore(3)
            
            async def crawl_with_semaphore(url: str, index: int):
                async with semaphore:
                    return await crawl_single_with_progress(url, index)
            
            # ëª¨ë“  URL ë™ì‹œ í¬ë¡¤ë§
            tasks = [crawl_with_semaphore(url, i) for i, url in enumerate(urls)]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # íŒŒì¼ë¡œ ê²°ê³¼ ì €ì¥
            await send_crawling_progress(job_id, "saving", 95, "ğŸ’¾ ê²°ê³¼ íŒŒì¼ ì €ì¥ ì¤‘...")
            
            result_file = f"results/unified_bulk_crawl_{job_id}.json"
            os.makedirs("results", exist_ok=True)
            
            # ê²°ê³¼ ìš”ì•½ ìƒì„±
            summary = {
                "job_id": job_id,
                "crawl_type": "unified_bulk",
                "total_urls": len(urls),
                "successful": active_jobs[job_id]["success"],
                "failed": active_jobs[job_id]["failed"],
                "success_rate": (active_jobs[job_id]["success"] / len(urls)) * 100,
                "start_time": active_jobs[job_id]["start_time"],
                "end_time": datetime.now().isoformat(),
                "results": active_jobs[job_id]["results"]
            }
            
            with open(result_file, 'w', encoding='utf-8') as f:
                json.dump(summary, f, ensure_ascii=False, indent=2)
            
            # ì™„ë£Œ ì²˜ë¦¬
            active_jobs[job_id]["status"] = "completed"
            active_jobs[job_id]["end_time"] = datetime.now().isoformat()
            active_jobs[job_id]["result_file"] = result_file
            
            logger.info(f"ğŸ’¾ í†µí•© ë©€í‹° í¬ë¡¤ë§ ê²°ê³¼ ì €ì¥: {result_file}")
            
            # ìµœì¢… ì™„ë£Œ ì•Œë¦¼
            await send_crawling_complete(job_id, {
                "status": "completed",
                "total_urls": len(urls),
                "successful": active_jobs[job_id]["success"],
                "failed": active_jobs[job_id]["failed"],
                "results": active_jobs[job_id]["results"],
                "result_file": result_file
            })
            
            logger.info(f"ğŸ‰ í†µí•© ëŒ€ëŸ‰ í¬ë¡¤ë§ ì™„ë£Œ: {job_id} (ì„±ê³µ: {active_jobs[job_id]['success']}, ì‹¤íŒ¨: {active_jobs[job_id]['failed']})")
            
        except Exception as e:
            # ì „ì²´ ì‘ì—… ì‹¤íŒ¨
            active_jobs[job_id]["status"] = "failed"
            active_jobs[job_id]["error"] = str(e)
            active_jobs[job_id]["end_time"] = datetime.now().isoformat()
            
            await send_crawling_error(job_id, f"ëŒ€ëŸ‰ í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}")
            logger.error(f"ğŸ’¥ í†µí•© ëŒ€ëŸ‰ í¬ë¡¤ë§ ì „ì²´ ì‹¤íŒ¨: {job_id} - {e}")
    
    # ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì‹œì‘ (asyncio.ensure_future ì‚¬ìš©)
    import asyncio
    asyncio.ensure_future(process_bulk_crawl())

async def _handle_selective_crawl_internal(
    url: str, target_content: str, timeout: int, 
    clean_text: bool, job_id: str
) -> CrawlResponse:
    """ì„ íƒì  í¬ë¡¤ë§ ë‚´ë¶€ ì²˜ë¦¬ - í†µì¼ëœ CrawlResponse ë°˜í™˜"""
    # ğŸ”§ job_idë¥¼ í¬í•¨í•œ ìŠ¤ë§ˆíŠ¸ í¬ë¡¤ë§ ìš”ì²­ ìƒì„±
    smart_request = SmartCrawlRequest(
        text=f"{url}ì˜ {target_content} ì¶”ì¶œí•´ì¤˜",
        timeout=timeout, 
        clean_text=clean_text
    )
    
    # ğŸ”§ ê¸°ì¡´ smart_natural_crawl ë¡œì§ì„ ì§ì ‘ í˜¸ì¶œí•˜ë˜ job_id ì „ë‹¬
    global crawler_instance
    
    if not crawler_instance or not crawler_instance.is_initialized:
        raise HTTPException(status_code=503, detail="í¬ë¡¤ë§ ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    # WebSocket ì§„í–‰ë¥  ì „ì†¡ í•¨ìˆ˜ import
    from ..api.websocket import send_crawling_progress, send_crawling_complete, send_crawling_error
    
    try:
        # 1ë‹¨ê³„: ìì—°ì–´ íŒŒì‹±
        await send_crawling_progress(job_id, "parsing", 30, "ğŸ” ìì—°ì–´ ì˜ë„ ë¶„ì„ ì¤‘...")
        
        intent = nl_parser.parse_selective_request(smart_request.text)
        validation = nl_parser.validate_intent(intent)
        
        if not validation["is_valid"]:
            error_msg = validation["message"]
            await send_crawling_error(job_id, error_msg)
            raise HTTPException(status_code=400, detail=error_msg)
        
        # 2ë‹¨ê³„: í¬ë¡¤ë§ ì‹¤í–‰
        await send_crawling_progress(job_id, "crawling", 50, f"ğŸ•·ï¸ {url} í¬ë¡¤ë§ ì¤‘...")
        
        result = await crawler_instance.crawl_with_strategy(
            url=intent.urls[0],
            custom_strategy=None  # ìë™ ì „ëµ ì„ íƒ
        )
        
        if not result or result.status != "complete":
            error_msg = f"í¬ë¡¤ë§ ì‹¤íŒ¨: {result.error if result else 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜'}"
            await send_crawling_error(job_id, error_msg)
            raise HTTPException(status_code=500, detail=error_msg)
        
        # 3ë‹¨ê³„: ì„ íƒì  ì¶”ì¶œ
        await send_crawling_progress(job_id, "extracting", 70, f"ğŸ¯ '{intent.target_content}' ì¶”ì¶œ ì¤‘...")
        
        # MCP í´ë¼ì´ì–¸íŠ¸ë¥¼ í†µí•œ ì„ íƒì  ì¶”ì¶œ
        extraction_result = await crawler_instance.mcp_client.extract_selective_content(
            html_content=result.text,  # í¬ë¡¤ë§ëœ ë‚´ìš©
            target_content=intent.target_content,
            url=url
        )
        
        if "error" in extraction_result:
            error_msg = f"ì¶”ì¶œ ì‹¤íŒ¨: {extraction_result['error']}"
            await send_crawling_error(job_id, error_msg)
            raise HTTPException(status_code=500, detail=error_msg)
        
        # 4ë‹¨ê³„: í›„ì²˜ë¦¬ ì ìš©
        await send_crawling_progress(job_id, "processing", 80, "ğŸ§¹ í…ìŠ¤íŠ¸ í›„ì²˜ë¦¬ ì¤‘...")
        
        if clean_text:
            from ..utils.text_processor import post_process_crawl_result
            result = post_process_crawl_result(result, clean_text=True)
        
        # 5ë‹¨ê³„: í†µì¼ëœ CrawlResponse ìƒì„±
        await send_crawling_progress(job_id, "saving", 90, "ğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘...")
        
        # ğŸ”§ í†µì¼ëœ CrawlResponse í˜•íƒœë¡œ ë°˜í™˜
        response_data = CrawlResponse(
            url=url,
            title=result.title,
            text=result.text,  # í›„ì²˜ë¦¬ëœ ì „ì²´ í…ìŠ¤íŠ¸
            hierarchy=result.hierarchy,
            metadata={
                # ğŸ”§ í¬ë¡¤ë§ ì—”ì§„ ì •ë³´ - result.metadataì—ì„œ ì˜¬ë°”ë¥´ê²Œ ê°€ì ¸ì˜¤ê¸°
                "engine_used": result.metadata.get("engine_used") or result.metadata.get("crawler_used", "unknown"),
                "crawler_used": result.metadata.get("crawler_used", "unknown"),
                
                # ğŸ”§ ì²˜ë¦¬ì‹œê°„ ì •ë³´
                "processing_time": result.metadata.get("processing_time", "N/A"),
                "execution_time": result.metadata.get("execution_time"),
                
                # ğŸ”§ í’ˆì§ˆ ì •ë³´ - extraction_resultì™€ original_crawling_metadataì—ì„œ ìµœì ê°’ ì„ íƒ
                "quality_score": result.metadata.get("quality_score") or extraction_result.get("quality_score", 0),
                "content_quality": result.metadata.get("content_quality", "medium"),
                # ğŸ”§ ì˜ë„ ë¶„ì„ ì‹ ë¢°ë„ì™€ í¬ë¡¤ë§ ì‹ ë¢°ë„ë¥¼ ê²°í•©í•˜ì—¬ ìµœì¢… ì‹ ë¢°ë„ ê³„ì‚°
                "confidence": min(
                    intent.confidence,  # ì˜ë„ ë¶„ì„ ì‹ ë¢°ë„
                    result.metadata.get("confidence") or result.metadata.get("extraction_confidence") or (result.metadata.get("quality_score", 0) / 100.0) or 0.5  # í¬ë¡¤ë§ ì‹ ë¢°ë„
                ),
                "extraction_confidence": result.metadata.get("extraction_confidence") or result.metadata.get("confidence") or (result.metadata.get("quality_score", 0) / 100.0) or 0.5,
                
                # ğŸ”§ ì„ íƒì  í¬ë¡¤ë§ íŠ¹í™” ì •ë³´
                "crawling_mode": "selective",
                "target_content": intent.target_content,
                "extraction_type": intent.extraction_type,
                "selective_crawling_mode": True,
                "extracted_data": extraction_result.get("extracted_data", {}),
                
                # ğŸ”§ ê¸°íƒ€ ë©”íƒ€ë°ì´í„°
                "intent_confidence": intent.confidence,
                "raw_request": intent.raw_request,
                "text_length": len(result.text),
                "post_processing_applied": clean_text,
                
                # ğŸ”§ ì›ë³¸ í¬ë¡¤ë§ ë©”íƒ€ë°ì´í„° í¬í•¨
                "original_crawling_metadata": result.metadata
            },
            status="complete",
            timestamp=datetime.now().isoformat()
        )
        
        # ê²°ê³¼ íŒŒì¼ ì €ì¥
        result_file = f"results/selective_crawl_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        os.makedirs("results", exist_ok=True)
        
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(response_data.dict(), f, ensure_ascii=False, indent=2)
        
        # ì™„ë£Œ ì•Œë¦¼
        await send_crawling_complete(job_id, {
            "status": "complete",
            "target_content": intent.target_content,
            "extraction_quality": extraction_result.get("quality_score", 0.0),
            "url": url,
            "response": response_data.dict()
        })
        
        logger.info(f"âœ… ì„ íƒì  í¬ë¡¤ë§ ì™„ë£Œ: {url} -> {intent.target_content}")
        return response_data
        
    except HTTPException:
        # ì´ë¯¸ ì²˜ë¦¬ëœ HTTP ì˜ˆì™¸ëŠ” ë‹¤ì‹œ ë°œìƒ
        raise
    except Exception as e:
        logger.error(f"âŒ ì„ íƒì  í¬ë¡¤ë§ ì‹¤íŒ¨: {smart_request.text} - {e}")
        await send_crawling_error(job_id, str(e))
        raise HTTPException(status_code=500, detail=f"ì„ íƒì  í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}") 