import re
import logging
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
from urllib.parse import urlparse

logger = logging.getLogger(__name__)

@dataclass
class SelectiveCrawlingIntent:
    """ì„ íƒì  í¬ë¡¤ë§ ì˜ë„"""
    urls: List[str]
    target_content: str  # "ì œëª©", "ê°€ê²©", "ë¦¬ë·°", "ë³¸ë¬¸" ë“±
    raw_request: str
    confidence: float
    extraction_type: str  # "selective", "full"

# ğŸ¯ í†µí•© ì˜ë„ ë¶„ì„ ê²°ê³¼
@dataclass  
class UnifiedIntent:
    """í†µí•© ì˜ë„ ë¶„ì„ ê²°ê³¼"""
    request_type: str    # "single", "bulk", "selective", "search"
    urls: List[str]      # ì¶”ì¶œëœ URLë“¤
    target_content: Optional[str] = None  # ì„ íƒì  ì¶”ì¶œ íƒ€ê²Ÿ
    search_query: Optional[str] = None    # ê²€ìƒ‰ ì¿¼ë¦¬
    platform: Optional[str] = None       # í”Œë«í¼ (ì¿ íŒ¡, ë„¤ì´ë²„ ë“±)
    confidence: float = 0.0              # ë¶„ì„ ì‹ ë¢°ë„
    raw_request: str = ""                # ì›ë³¸ ìš”ì²­
    metadata: Dict[str, Any] = None      # ì¶”ê°€ ë©”íƒ€ë°ì´í„°
    
    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}

class NaturalLanguageParser:
    """ìì—°ì–´ í¬ë¡¤ë§ ìš”ì²­ íŒŒì„œ"""
    
    def __init__(self):
        # ì½˜í…ì¸  íƒ€ì… ë§¤í•‘
        self.content_type_patterns = {
            "ì œëª©": ["ì œëª©", "íƒ€ì´í‹€", "title", "í—¤ë“œë¼ì¸", "headline"],
            "ê°€ê²©": ["ê°€ê²©", "price", "ë¹„ìš©", "cost", "ê¸ˆì•¡", "ìš”ê¸ˆ"],
            "ë³¸ë¬¸": ["ë³¸ë¬¸", "ë‚´ìš©", "content", "ê¸€", "article", "í…ìŠ¤íŠ¸", "text"],
            "ë¦¬ë·°": ["ë¦¬ë·°", "review", "í›„ê¸°", "í‰ê°€", "ëŒ“ê¸€", "comment"],
            "ìš”ì•½": ["ìš”ì•½", "summary", "ê°œìš”", "í•µì‹¬", "ì •ë¦¬"],
            "ì´ë¯¸ì§€": ["ì´ë¯¸ì§€", "image", "ì‚¬ì§„", "photo", "ê·¸ë¦¼", "picture"],
            "ë§í¬": ["ë§í¬", "link", "url", "ì£¼ì†Œ"],
            "ë‚ ì§œ": ["ë‚ ì§œ", "date", "ì‹œê°„", "time", "ì‘ì„±ì¼"]
        }
        
        # URL íŒ¨í„´
        self.url_pattern = re.compile(
            r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
        )
        
        # ê°„ë‹¨í•œ ë„ë©”ì¸ íŒ¨í„´ (http ì—†ì´)
        self.domain_pattern = re.compile(
            r'(?:www\.)?[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?(?:\.[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?)*\.[a-zA-Z]{2,}'
        )
    
    def extract_urls(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ URL ì¶”ì¶œ"""
        urls = []
        
        # ì™„ì „í•œ URL ì¶”ì¶œ (http/https í¬í•¨)
        full_urls = self.url_pattern.findall(text)
        urls.extend(full_urls)
        
        # ë„ë©”ì¸ë§Œ ìˆëŠ” ê²½ìš° (www.test.com í˜•íƒœ)
        domain_matches = self.domain_pattern.findall(text)
        for domain in domain_matches:
            # ì´ë¯¸ full_urlsì— ìˆëŠ”ì§€ í™•ì¸
            if not any(domain in url for url in full_urls):
                # http í”„ë¡œí† ì½œ ì¶”ê°€
                if not domain.startswith('www.'):
                    domain = 'www.' + domain
                urls.append('https://' + domain)
        
        # ì¤‘ë³µ ì œê±°
        return list(set(urls))
    
    def detect_target_content(self, text: str) -> tuple[str, float]:
        """íƒ€ê²Ÿ ì½˜í…ì¸  íƒ€ì… ê°ì§€"""
        text_lower = text.lower()
        
        best_match = "ì „ì²´"
        max_confidence = 0.0
        
        for content_type, keywords in self.content_type_patterns.items():
            confidence = 0.0
            
            for keyword in keywords:
                if keyword in text_lower:
                    # í‚¤ì›Œë“œê°€ "ë§Œ" ê³¼ í•¨ê»˜ ì‚¬ìš©ë˜ë©´ ë†’ì€ ì‹ ë¢°ë„
                    if f"{keyword}ë§Œ" in text_lower or f"{keyword} ë§Œ" in text_lower:
                        confidence = max(confidence, 0.8)  # ìµœëŒ€ê°’ìœ¼ë¡œ ì„¤ì • (ì¤‘ë³µ ë°©ì§€)
                    # ì¼ë°˜ì ì¸ í‚¤ì›Œë“œ ë§¤ì¹­
                    elif keyword in text_lower:
                        confidence = max(confidence, 0.5)  # ìµœëŒ€ê°’ìœ¼ë¡œ ì„¤ì • (ì¤‘ë³µ ë°©ì§€)
            
            # íŠ¹ì • íŒ¨í„´ë“¤ì— ëŒ€í•œ ë³´ë„ˆìŠ¤
            if content_type == "ì œëª©" and any(word in text_lower for word in ["ì¶”ì¶œ", "ê°€ì ¸", "ë½‘ì•„"]):
                confidence = min(1.0, confidence + 0.2)  # ë³´ë„ˆìŠ¤ ì¶”ê°€í•˜ë˜ 1.0 ì œí•œ
            
            if confidence > max_confidence:
                max_confidence = confidence
                best_match = content_type
        
        # ğŸ”§ ì‹ ë¢°ë„ë¥¼ 0.0~1.0 ë²”ìœ„ë¡œ ì œí•œ
        max_confidence = min(1.0, max(0.0, max_confidence))
        
        return best_match, max_confidence
    
    def parse_selective_request(self, text: str) -> SelectiveCrawlingIntent:
        """ì„ íƒì  í¬ë¡¤ë§ ìš”ì²­ íŒŒì‹±"""
        logger.info(f"ğŸ” ìì—°ì–´ íŒŒì‹± ì‹œì‘: {text}")
        
        # URL ì¶”ì¶œ
        urls = self.extract_urls(text)
        logger.info(f"ğŸ“ ì¶”ì¶œëœ URLë“¤: {urls}")
        
        # íƒ€ê²Ÿ ì½˜í…ì¸  ê°ì§€
        target_content, confidence = self.detect_target_content(text)
        logger.info(f"ğŸ¯ íƒ€ê²Ÿ ì½˜í…ì¸ : {target_content} (ì‹ ë¢°ë„: {confidence:.2f})")
        
        # ì¶”ì¶œ íƒ€ì… ê²°ì •
        extraction_type = "selective" if urls and target_content != "ì „ì²´" else "full"
        
        intent = SelectiveCrawlingIntent(
            urls=urls,
            target_content=target_content,
            raw_request=text,
            confidence=confidence,
            extraction_type=extraction_type
        )
        
        logger.info(f"âœ… íŒŒì‹± ì™„ë£Œ: {extraction_type} í¬ë¡¤ë§, {len(urls)}ê°œ URL")
        return intent
    
    def validate_intent(self, intent: SelectiveCrawlingIntent) -> Dict[str, Any]:
        """ì˜ë„ ê²€ì¦ ë° í”¼ë“œë°± ìƒì„±"""
        if not intent.urls:
            return {
                "is_valid": False,
                "message": "URLì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. 'https://example.comì˜ ì œëª© ì¶”ì¶œí•´ì¤˜' í˜•ì‹ìœ¼ë¡œ ì…ë ¥í•´ì£¼ì„¸ìš”.",
                "suggestions": [
                    "URLì„ í¬í•¨í•´ì£¼ì„¸ìš” (ì˜ˆ: https://naver.com)",
                    "www.ë„ë©”ì¸.com í˜•ì‹ë„ ê°€ëŠ¥í•©ë‹ˆë‹¤",
                    "ì¶”ì¶œí•˜ê³  ì‹¶ì€ ë‚´ìš©ì„ ëª…ì‹œí•´ì£¼ì„¸ìš” (ì œëª©, ê°€ê²©, ë³¸ë¬¸ ë“±)"
                ]
            }
        
        if not intent.target_content:
            return {
                "is_valid": False,
                "message": "ì¶”ì¶œí•  ì½˜í…ì¸ ë¥¼ ì§€ì •í•´ì£¼ì„¸ìš”.",
                "suggestions": [
                    "ì œëª©ë§Œ ì¶”ì¶œí•´ì¤˜",
                    "ê°€ê²© ì •ë³´ ê°€ì ¸ì™€ì¤˜", 
                    "ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œí•´ì¤˜"
                ]
            }
        
        # ì„±ê³µì ì¸ ê²€ì¦
        return {
            "is_valid": True,
            "message": f"âœ… {intent.urls[0]}ì—ì„œ '{intent.target_content}' ì¶”ì¶œì„ ì‹œì‘í•©ë‹ˆë‹¤. (ì‹ ë¢°ë„: {intent.confidence:.1f})",
            "confidence": intent.confidence,
            "url": intent.urls[0],
            "target": intent.target_content
        }

    # ğŸ¯ í†µí•© ì˜ë„ ë¶„ì„ ë©”ì„œë“œ
    def analyze_unified_intent(self, text: str) -> UnifiedIntent:
        """
        ëª¨ë“  í˜•íƒœì˜ ì…ë ¥ì„ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ ì²˜ë¦¬ ë°©ì‹ì„ ê²°ì •
        
        Args:
            text: ì‚¬ìš©ì ì…ë ¥ (URL, ìì—°ì–´, ë©€í‹° URL ë“±)
            
        Returns:
            UnifiedIntent: í†µí•© ì˜ë„ ë¶„ì„ ê²°ê³¼
        """
        logger.info(f"ğŸ§  í†µí•© ì˜ë„ ë¶„ì„ ì‹œì‘: {text}")
        
        # 1. URL ì¶”ì¶œ
        urls = self.extract_urls(text)
        url_count = len(urls)
        
        # 2. ìì—°ì–´ íŒ¨í„´ ë¶„ì„
        has_extraction_keywords = any(keyword in text for keyword in self.content_type_patterns.keys())
        has_search_keywords = any(keyword in text for keyword in ["ì°¾ì•„ì¤˜", "ê²€ìƒ‰", "ì°¾ê¸°", "ì•Œì•„ë´"])
        has_platform_keywords = any(platform in text for platform in ["ì¿ íŒ¡", "ë„¤ì´ë²„", "êµ¬ê¸€", "ì•„ë§ˆì¡´"])
        
        # 3. ì˜ë„ ê²°ì • ë¡œì§
        if url_count == 0:
            # URLì´ ì—†ëŠ” ê²½ìš°
            if has_platform_keywords and has_search_keywords:
                # "ì¿ íŒ¡ì—ì„œ ì½œë¼ ì°¾ì•„ì¤˜" íŒ¨í„´
                return self._analyze_search_intent(text)
            else:
                # ìœ íš¨í•˜ì§€ ì•Šì€ ìš”ì²­
                return UnifiedIntent(
                    request_type="invalid",
                    urls=[],
                    confidence=0.0,
                    raw_request=text,
                    metadata={"error": "URL ë˜ëŠ” ê²€ìƒ‰ ì˜ë„ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}
                )
        
        elif url_count == 1:
            # ë‹¨ì¼ URL
            if has_extraction_keywords:
                # "naver.comì˜ ì œëª©ë§Œ ì¶”ì¶œí•´ì¤˜" íŒ¨í„´
                return self._analyze_selective_intent(text, urls)
            else:
                # "https://example.com" ë‹¨ìˆœ URL
                return UnifiedIntent(
                    request_type="single",
                    urls=urls,
                    confidence=0.9,
                    raw_request=text,
                    metadata={"processing_type": "full_crawl"}
                )
        
        else:
            # ë©€í‹° URL
            if has_extraction_keywords:
                # ì—¬ëŸ¬ URLì—ì„œ ì„ íƒì  ì¶”ì¶œ (ë³µì¡í•œ ì¼€ì´ìŠ¤)
                return self._analyze_bulk_selective_intent(text, urls)
            else:
                # ë‹¨ìˆœ ë©€í‹° URL í¬ë¡¤ë§
                return UnifiedIntent(
                    request_type="bulk", 
                    urls=urls,
                    confidence=0.8,
                    raw_request=text,
                    metadata={"processing_type": "bulk_crawl", "url_count": url_count}
                )
    
    def _analyze_selective_intent(self, text: str, urls: List[str]) -> UnifiedIntent:
        """ì„ íƒì  í¬ë¡¤ë§ ì˜ë„ ë¶„ì„"""
        # ê¸°ì¡´ selective parsing ë¡œì§ ì¬ì‚¬ìš©
        selective_intent = self.parse_selective_request(text)
        
        return UnifiedIntent(
            request_type="selective",
            urls=urls,
            target_content=selective_intent.target_content,
            confidence=selective_intent.confidence,
            raw_request=text,
            metadata={
                "extraction_type": selective_intent.extraction_type,
                "processing_type": "selective_crawl"
            }
        )
    
    def _analyze_search_intent(self, text: str) -> UnifiedIntent:
        """ê²€ìƒ‰ í¬ë¡¤ë§ ì˜ë„ ë¶„ì„ (ë¯¸ë˜ ê¸°ëŠ¥)"""
        # í”Œë«í¼ ì¶”ì¶œ
        platform_patterns = {
            "ì¿ íŒ¡": r"ì¿ íŒ¡",
            "ë„¤ì´ë²„": r"ë„¤ì´ë²„",
            "êµ¬ê¸€": r"êµ¬ê¸€",
            "ì•„ë§ˆì¡´": r"ì•„ë§ˆì¡´"
        }
        
        detected_platform = None
        for platform, pattern in platform_patterns.items():
            if re.search(pattern, text, re.IGNORECASE):
                detected_platform = platform
                break
        
        # ê²€ìƒ‰ ì¿¼ë¦¬ ì¶”ì¶œ (ê°„ë‹¨í•œ íŒ¨í„´)
        search_patterns = [
            r"ì—ì„œ\s+(.+?)\s+ì°¾ì•„ì¤˜",
            r"ì—ì„œ\s+(.+?)\s+ê²€ìƒ‰",
            r"(.+?)\s+ì •ë³´\s+ì°¾ì•„ì¤˜"
        ]
        
        search_query = None
        for pattern in search_patterns:
            match = re.search(pattern, text)
            if match:
                search_query = match.group(1).strip()
                break
        
        return UnifiedIntent(
            request_type="search",
            urls=[],  # ê²€ìƒ‰ì€ URLì´ ì—†ìŒ
            search_query=search_query,
            platform=detected_platform,
            confidence=0.7 if detected_platform and search_query else 0.3,
            raw_request=text,
            metadata={
                "processing_type": "platform_search",
                "requires_implementation": True  # ì•„ì§ êµ¬í˜„ë˜ì§€ ì•ŠìŒ
            }
        )
    
    def _analyze_bulk_selective_intent(self, text: str, urls: List[str]) -> UnifiedIntent:
        """ë©€í‹° URL ì„ íƒì  ì¶”ì¶œ ì˜ë„ ë¶„ì„"""
        # ì¶”ì¶œ íƒ€ê²Ÿ ë¶„ì„
        target_content = None
        confidence = 0.6
        
        for keyword, content_type in self.content_type_patterns.items():
            if keyword in text:
                target_content = content_type
                confidence += 0.2
                break
        
        return UnifiedIntent(
            request_type="bulk_selective",  # ìƒˆë¡œìš´ íƒ€ì…
            urls=urls,
            target_content=target_content,
            confidence=min(confidence, 1.0),
            raw_request=text,
            metadata={
                "processing_type": "bulk_selective_crawl",
                "url_count": len(urls),
                "requires_implementation": True  # ë³µì¡í•œ ì¼€ì´ìŠ¤, ì¶”í›„ êµ¬í˜„
            }
        )

# ì „ì—­ íŒŒì„œ ì¸ìŠ¤í„´ìŠ¤
nl_parser = NaturalLanguageParser() 