"""
MCP ë„êµ¬ ê´€ë¦¬ì
MCP í´ë¼ì´ì–¸íŠ¸ë¥¼ í†µí•´ ë‹¤ì–‘í•œ ë¶„ì„ ë„êµ¬ë“¤ì„ í¸ë¦¬í•˜ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì¸í„°í˜ì´ìŠ¤
"""

import logging
from typing import Dict, Any, Optional
from .client import MCPClient

logger = logging.getLogger(__name__)

class MCPToolsManager:
    """MCP ë„êµ¬ë“¤ì„ ê´€ë¦¬í•˜ëŠ” ë§¤ë‹ˆì € í´ë˜ìŠ¤"""
    
    def __init__(self, mcp_client: MCPClient):
        """
        MCP ë„êµ¬ ë§¤ë‹ˆì € ì´ˆê¸°í™”
        
        Args:
            mcp_client: MCP í´ë¼ì´ì–¸íŠ¸ ì¸ìŠ¤í„´ìŠ¤
        """
        self.mcp_client = mcp_client
        self._available_tools = None
    
    async def get_available_tools(self) -> Dict[str, Any]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡ ì¡°íšŒ"""
        if self._available_tools is None:
            tools_list = await self.mcp_client.list_tools()
            self._available_tools = {tool["name"]: tool for tool in tools_list}
            logger.info(f"ì‚¬ìš© ê°€ëŠ¥í•œ MCP ë„êµ¬: {list(self._available_tools.keys())}")
        
        return self._available_tools
    
    async def analyze_website_completely(self, url: str, sample_html: str = "") -> Dict[str, Any]:
        """
        ì›¹ì‚¬ì´íŠ¸ ì¢…í•© ë¶„ì„ ì›Œí¬í”Œë¡œìš°
        ì‚¬ì´íŠ¸ ë¶„ì„ â†’ êµ¬ì¡° ê°ì§€ â†’ ì „ëµ ìƒì„±ì˜ ì „ì²´ ê³¼ì • ì‹¤í–‰
        
        Args:
            url: ë¶„ì„í•  ì›¹ì‚¬ì´íŠ¸ URL
            sample_html: ì„ íƒì  HTML ìƒ˜í”Œ
            
        Returns:
            ì „ì²´ ë¶„ì„ ê²°ê³¼
        """
        logger.info(f"ì›¹ì‚¬ì´íŠ¸ ì¢…í•© ë¶„ì„ ì‹œì‘: {url}")
        
        # ğŸ”§ ë””ë²„ê¹…: ì…ë ¥ ë§¤ê°œë³€ìˆ˜ í™•ì¸
        logger.info(f"ğŸ”§ DEBUG: ë¶„ì„ ë§¤ê°œë³€ìˆ˜:")
        logger.info(f"   - URL: {url}")
        logger.info(f"   - Sample HTML ê¸¸ì´: {len(sample_html)}")
        logger.info(f"   - MCP í´ë¼ì´ì–¸íŠ¸ ìƒíƒœ: {self.mcp_client is not None}")
        
        try:
            # 1ë‹¨ê³„: ì‚¬ì´íŠ¸ ë¶„ì„ ë° í¬ë¡¤ëŸ¬ ì„ íƒ
            logger.info("1ë‹¨ê³„: ì‚¬ì´íŠ¸ ë¶„ì„ ì¤‘...")
            logger.info(f"ğŸ”§ DEBUG: mcp_client.analyze_site í˜¸ì¶œ ì‹œì‘")
            
            site_analysis = await self.mcp_client.analyze_site(url, sample_html)
            
            logger.info(f"ğŸ”§ DEBUG: mcp_client.analyze_site ì™„ë£Œ")
            logger.info(f"ğŸ”§ DEBUG: site_analysis íƒ€ì…: {type(site_analysis)}")
            logger.info(f"ğŸ”§ DEBUG: site_analysis í‚¤ë“¤: {list(site_analysis.keys()) if isinstance(site_analysis, dict) else 'Not a dict'}")
            
            if "error" in site_analysis:
                logger.error(f"ğŸ”§ DEBUG: ì‚¬ì´íŠ¸ ë¶„ì„ì—ì„œ ì—ëŸ¬ ê°ì§€: {site_analysis['error']}")
                logger.error(f"ì‚¬ì´íŠ¸ ë¶„ì„ ì‹¤íŒ¨: {site_analysis['error']}")
                return {"error": "ì‚¬ì´íŠ¸ ë¶„ì„ ì‹¤íŒ¨", "details": site_analysis}
            
            logger.info(f"ğŸ”§ DEBUG: ì‚¬ì´íŠ¸ ë¶„ì„ ì„±ê³µ!")
            if "recommended_crawler" in site_analysis:
                logger.info(f"ğŸ”§ DEBUG: ì¶”ì²œ í¬ë¡¤ëŸ¬: {site_analysis['recommended_crawler']}")
            
            # 2ë‹¨ê³„: ì½˜í…ì¸  êµ¬ì¡° ë¶„ì„
            logger.info("2ë‹¨ê³„: ì½˜í…ì¸  êµ¬ì¡° ë¶„ì„ ì¤‘...")
            logger.info(f"ğŸ”§ DEBUG: mcp_client.detect_structure í˜¸ì¶œ ì‹œì‘")
            
            if sample_html:
                structure_analysis = await self.mcp_client.detect_structure(sample_html, url)
            else:
                # HTML ìƒ˜í”Œì´ ì—†ìœ¼ë©´ ê¸°ë³¸ êµ¬ì¡°ë¡œ ì§„í–‰
                logger.info(f"ğŸ”§ DEBUG: HTML ìƒ˜í”Œì´ ì—†ì–´ì„œ ê¸°ë³¸ êµ¬ì¡° ì‚¬ìš©")
                structure_analysis = await self.mcp_client.detect_structure(
                    "<html><body><p>ê¸°ë³¸ êµ¬ì¡°</p></body></html>", url
                )
            
            logger.info(f"ğŸ”§ DEBUG: mcp_client.detect_structure ì™„ë£Œ")
            logger.info(f"ğŸ”§ DEBUG: structure_analysis íƒ€ì…: {type(structure_analysis)}")
            logger.info(f"ğŸ”§ DEBUG: structure_analysis í‚¤ë“¤: {list(structure_analysis.keys()) if isinstance(structure_analysis, dict) else 'Not a dict'}")
            
            if "error" in structure_analysis:
                logger.warning(f"ğŸ”§ DEBUG: êµ¬ì¡° ë¶„ì„ì—ì„œ ì—ëŸ¬ ê°ì§€: {structure_analysis['error']}")
                logger.warning(f"êµ¬ì¡° ë¶„ì„ ì‹¤íŒ¨, ê¸°ë³¸ êµ¬ì¡° ì‚¬ìš©: {structure_analysis['error']}")
                structure_analysis = {"basic_structure": True}
            
            # 3ë‹¨ê³„: í¬ë¡¤ë§ ì „ëµ ìƒì„±
            logger.info("3ë‹¨ê³„: í¬ë¡¤ë§ ì „ëµ ìƒì„± ì¤‘...")
            logger.info(f"ğŸ”§ DEBUG: mcp_client.generate_strategy í˜¸ì¶œ ì‹œì‘")
            
            strategy = await self.mcp_client.generate_strategy(site_analysis, structure_analysis)
            
            logger.info(f"ğŸ”§ DEBUG: mcp_client.generate_strategy ì™„ë£Œ")
            logger.info(f"ğŸ”§ DEBUG: strategy íƒ€ì…: {type(strategy)}")
            logger.info(f"ğŸ”§ DEBUG: strategy í‚¤ë“¤: {list(strategy.keys()) if isinstance(strategy, dict) else 'Not a dict'}")
            
            if "error" in strategy:
                logger.error(f"ğŸ”§ DEBUG: ì „ëµ ìƒì„±ì—ì„œ ì—ëŸ¬ ê°ì§€: {strategy['error']}")
                logger.error(f"ì „ëµ ìƒì„± ì‹¤íŒ¨: {strategy['error']}")
                return {"error": "ì „ëµ ìƒì„± ì‹¤íŒ¨", "details": strategy}
            
            logger.info(f"ğŸ”§ DEBUG: ì „ëµ ìƒì„± ì„±ê³µ!")
            if "recommended_engine" in strategy:
                logger.info(f"ğŸ”§ DEBUG: ì „ëµì—ì„œ ì¶”ì²œ ì—”ì§„: {strategy['recommended_engine']}")
            if "fallback_engines" in strategy:
                logger.info(f"ğŸ”§ DEBUG: ì „ëµì—ì„œ í´ë°± ì—”ì§„ë“¤: {strategy['fallback_engines']}")
            
            # ê²°ê³¼ ì¢…í•©
            complete_analysis = {
                "url": url,
                "site_analysis": site_analysis,
                "structure_analysis": structure_analysis,
                "crawling_strategy": strategy,
                "status": "success",
                "workflow_completed": True
            }
            
            # ğŸ”§ ë””ë²„ê¹…: ìµœì¢… ê²°ê³¼ í™•ì¸
            logger.info(f"ğŸ”§ DEBUG: ìµœì¢… ê²°ê³¼ êµ¬ì„± ì™„ë£Œ")
            logger.info(f"ğŸ”§ DEBUG: complete_analysis í‚¤ë“¤: {list(complete_analysis.keys())}")
            
            logger.info(f"ì›¹ì‚¬ì´íŠ¸ ì¢…í•© ë¶„ì„ ì™„ë£Œ: {url}")
            return complete_analysis
            
        except Exception as e:
            # ğŸ”§ ë””ë²„ê¹…: ì˜ˆì™¸ ìƒì„¸ ì •ë³´
            logger.error(f"ğŸ”§ DEBUG: analyze_website_completelyì—ì„œ ì˜ˆì™¸ ë°œìƒ!")
            logger.error(f"ğŸ”§ DEBUG: ì˜ˆì™¸ íƒ€ì…: {type(e).__name__}")
            logger.error(f"ğŸ”§ DEBUG: ì˜ˆì™¸ ë©”ì‹œì§€: {str(e)}")
            logger.error(f"ğŸ”§ DEBUG: ì˜ˆì™¸ ìƒì„¸: {repr(e)}")
            
            logger.error(f"ì›¹ì‚¬ì´íŠ¸ ì¢…í•© ë¶„ì„ ì˜¤ë¥˜: {e}")
            return {
                "error": f"ì¢…í•© ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                "url": url,
                "status": "failed"
            }
    
    async def validate_crawling_quality(self, extracted_data: Dict[str, Any], url: str, 
                                      expected_quality: float = 70.0) -> Dict[str, Any]:
        """
        í¬ë¡¤ë§ ê²°ê³¼ í’ˆì§ˆ ê²€ì¦
        
        Args:
            extracted_data: í¬ë¡¤ë§ìœ¼ë¡œ ì¶”ì¶œëœ ë°ì´í„°
            url: ì›ë³¸ URL
            expected_quality: ê¸°ëŒ€ í’ˆì§ˆ ì ìˆ˜
            
        Returns:
            í’ˆì§ˆ ê²€ì¦ ê²°ê³¼
        """
        logger.info(f"í¬ë¡¤ë§ í’ˆì§ˆ ê²€ì¦ ì‹œì‘: {url}")
        
        try:
            validation_result = await self.mcp_client.validate_result(
                extracted_data, url, expected_quality
            )
            
            if "error" not in validation_result:
                logger.info(f"í’ˆì§ˆ ê²€ì¦ ì™„ë£Œ: {url}, ì ìˆ˜: {validation_result.get('quality_score', 'N/A')}")
            else:
                logger.warning(f"í’ˆì§ˆ ê²€ì¦ ì‹¤íŒ¨: {validation_result['error']}")
            
            return validation_result
            
        except Exception as e:
            logger.error(f"í’ˆì§ˆ ê²€ì¦ ì˜¤ë¥˜: {e}")
            return {
                "error": f"í’ˆì§ˆ ê²€ì¦ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                "url": url,
                "status": "failed"
            }
    
    async def get_optimal_crawler_for_url(self, url: str, sample_html: str = "") -> str:
        """
        URLì— ëŒ€í•œ ìµœì  í¬ë¡¤ëŸ¬ ì¶”ì²œ
        
        Args:
            url: ë¶„ì„í•  URL
            sample_html: ì„ íƒì  HTML ìƒ˜í”Œ
            
        Returns:
            ì¶”ì²œ í¬ë¡¤ëŸ¬ ì´ë¦„ (firecrawl, crawl4ai, playwright, requests)
        """
        try:
            analysis = await self.mcp_client.analyze_site(url, sample_html)
            
            if "error" in analysis:
                logger.warning(f"í¬ë¡¤ëŸ¬ ì¶”ì²œ ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©: {analysis['error']}")
                return "requests"  # ê¸°ë³¸ê°’
            
            recommended_crawler = analysis.get("recommended_crawler", "requests")
            logger.info(f"URL {url}ì— ëŒ€í•œ ì¶”ì²œ í¬ë¡¤ëŸ¬: {recommended_crawler}")
            
            return recommended_crawler
            
        except Exception as e:
            logger.error(f"í¬ë¡¤ëŸ¬ ì¶”ì²œ ì˜¤ë¥˜: {e}")
            return "requests"  # ê¸°ë³¸ê°’ 