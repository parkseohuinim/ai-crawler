"""
í¬ë¡¤ë§ ì „ëµ ê´€ë¦¬ì
MCP ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í¬ë¡¤ë§ ì „ëµì„ ìˆ˜ë¦½í•˜ê³  ê´€ë¦¬
"""

import logging
from typing import Dict, Any, List, Optional
from .client import MCPClient

logger = logging.getLogger(__name__)

class CrawlingStrategyManager:
    """í¬ë¡¤ë§ ì „ëµì„ ê´€ë¦¬í•˜ëŠ” ë§¤ë‹ˆì € í´ë˜ìŠ¤"""
    
    # í¬ë¡¤ëŸ¬ë³„ íŠ¹ì„± ì •ì˜ (PROJECT_SPECIFICATION.md ê¸°ë°˜)
    CRAWLER_CHARACTERISTICS = {
        "firecrawl": {
            "strengths": ["SPA", "ì•ˆí‹°ë´‡ ìš°íšŒ", "ë³µì¡í•œ JS", "React/Vue"],
            "weaknesses": ["ë¹„ìš©", "API ì œí•œ"],
            "use_cases": ["complex_spa", "anti_bot_heavy"]
        },
        "crawl4ai": {
            "strengths": ["AI ë¶„ì„", "ì˜ë¯¸ì  ì¶”ì¶œ", "êµ¬ì¡°í™”"],
            "weaknesses": ["ì†ë„", "ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰"],
            "use_cases": ["ai_analysis_needed", "complex_structure"]
        },
        "playwright": {
            "strengths": ["ì •ë°€ ì œì–´", "ë¡œê·¸ì¸", "ì¸í„°ë™ì…˜"],
            "weaknesses": ["ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰", "ë³µì¡ì„±"],
            "use_cases": ["standard_dynamic", "login_required"]
        },
        "requests": {
            "strengths": ["ì†ë„", "ë‹¨ìˆœí•¨", "ì•ˆì •ì„±"],
            "weaknesses": ["JS ì²˜ë¦¬ ë¶ˆê°€", "ë™ì  ì½˜í…ì¸ "],
            "use_cases": ["simple_static", "api_endpoints"]
        }
    }
    
    def __init__(self, mcp_client: MCPClient):
        """
        í¬ë¡¤ë§ ì „ëµ ë§¤ë‹ˆì € ì´ˆê¸°í™”
        
        Args:
            mcp_client: MCP í´ë¼ì´ì–¸íŠ¸ ì¸ìŠ¤í„´ìŠ¤
        """
        self.mcp_client = mcp_client
        self._strategy_cache = {}
    
    async def create_crawling_strategy(self, url: str, site_analysis: Dict[str, Any] = None,
                                     structure_analysis: Dict[str, Any] = None) -> Dict[str, Any]:
        """
        URLì— ëŒ€í•œ ì¢…í•©ì ì¸ í¬ë¡¤ë§ ì „ëµ ìƒì„±
        
        Args:
            url: ëŒ€ìƒ URL
            site_analysis: ì‚¬ì´íŠ¸ ë¶„ì„ ê²°ê³¼ (ì—†ìœ¼ë©´ ìë™ ë¶„ì„)
            structure_analysis: êµ¬ì¡° ë¶„ì„ ê²°ê³¼ (ì—†ìœ¼ë©´ ìë™ ë¶„ì„)
            
        Returns:
            ì™„ì „í•œ í¬ë¡¤ë§ ì „ëµ
        """
        logger.info(f"í¬ë¡¤ë§ ì „ëµ ìƒì„± ì‹œì‘: {url}")
        
        try:
            # ğŸš¨ ìºì‹œ ë¹„í™œì„±í™” (ë””ë²„ê¹…ìš©) - ê° URLë§ˆë‹¤ ìƒˆë¡œìš´ ì „ëµ ìƒì„±
            # if url in self._strategy_cache:
            #     logger.info(f"ìºì‹œëœ ì „ëµ ì‚¬ìš©: {url}")
            #     return self._strategy_cache[url]
            
            logger.info(f"ğŸ”„ ìƒˆë¡œìš´ ì „ëµ ìƒì„± (ìºì‹œ ë¹„í™œì„±í™”): {url}")
            
            # ì‚¬ì´íŠ¸ ë¶„ì„ì´ ì—†ìœ¼ë©´ ì‹¤í–‰
            if not site_analysis:
                logger.info("ì‚¬ì´íŠ¸ ë¶„ì„ ì‹¤í–‰ ì¤‘...")
                site_analysis = await self.mcp_client.analyze_site(url)
                
                if "error" in site_analysis:
                    logger.error(f"ì‚¬ì´íŠ¸ ë¶„ì„ ì‹¤íŒ¨: {site_analysis['error']}")
                    return self._create_fallback_strategy(url)
            
            # êµ¬ì¡° ë¶„ì„ì´ ì—†ìœ¼ë©´ ì‹¤í–‰
            if not structure_analysis:
                logger.info("êµ¬ì¡° ë¶„ì„ ì‹¤í–‰ ì¤‘...")
                structure_analysis = await self.mcp_client.detect_structure("", url)
                
                if "error" in structure_analysis:
                    logger.warning(f"êµ¬ì¡° ë¶„ì„ ì‹¤íŒ¨, ê¸°ë³¸ êµ¬ì¡° ì‚¬ìš©: {structure_analysis['error']}")
                    structure_analysis = {"basic_structure": True}
            
            # MCPë¥¼ í†µí•œ ì „ëµ ìƒì„±
            logger.info("MCP ì „ëµ ìƒì„± ì‹¤í–‰ ì¤‘...")
            mcp_strategy = await self.mcp_client.generate_strategy(site_analysis, structure_analysis)
            
            if "error" in mcp_strategy:
                logger.error(f"MCP ì „ëµ ìƒì„± ì‹¤íŒ¨: {mcp_strategy['error']}")
                return self._create_fallback_strategy(url)
            
            # ì „ëµ ë³´ê°• ë° ìµœì í™”
            enhanced_strategy = self._enhance_strategy(url, site_analysis, structure_analysis, mcp_strategy)
            
            # ìºì‹œì— ì €ì¥
            # self._strategy_cache[url] = enhanced_strategy
            
            logger.info(f"í¬ë¡¤ë§ ì „ëµ ìƒì„± ì™„ë£Œ: {url}")
            return enhanced_strategy
            
        except Exception as e:
            logger.error(f"í¬ë¡¤ë§ ì „ëµ ìƒì„± ì˜¤ë¥˜: {e}")
            return self._create_fallback_strategy(url)
    
    def _enhance_strategy(self, url: str, site_analysis: Dict[str, Any], 
                         structure_analysis: Dict[str, Any], mcp_strategy: Dict[str, Any]) -> Dict[str, Any]:
        """MCP ì „ëµì„ ë°±ì—”ë“œ ìš”êµ¬ì‚¬í•­ì— ë§ê²Œ ë³´ê°•"""
        
        # ê¸°ë³¸ ì „ëµ êµ¬ì¡°
        enhanced_strategy = {
            "url": url,
            "primary_crawler": mcp_strategy.get("recommended_crawler", "requests"),
            "fallback_crawlers": [],
            "crawler_settings": {},
            "extraction_rules": {},
            "quality_thresholds": {
                "minimum_score": 70.0,
                "retry_threshold": 50.0
            },
            "timeout_settings": {
                "page_load": 30,
                "element_wait": 10,
                "total_timeout": 120
            },
            "site_characteristics": site_analysis.get("site_type", {}),
            "content_structure": structure_analysis.get("hierarchy", {}),
            "mcp_analysis": mcp_strategy
        }
        
        # í¬ë¡¤ëŸ¬ë³„ í´ë°± ìˆœì„œ ì„¤ì •
        primary_crawler = enhanced_strategy["primary_crawler"]
        enhanced_strategy["fallback_crawlers"] = self._get_fallback_order(primary_crawler, site_analysis)
        
        # í¬ë¡¤ëŸ¬ë³„ ì„¸ë¶€ ì„¤ì •
        enhanced_strategy["crawler_settings"] = self._get_crawler_settings(primary_crawler, site_analysis)
        
        # ì¶”ì¶œ ê·œì¹™ ì„¤ì •
        enhanced_strategy["extraction_rules"] = self._get_extraction_rules(structure_analysis)
        
        return enhanced_strategy
    
    def _get_fallback_order(self, primary_crawler: str, site_analysis: Dict[str, Any]) -> List[str]:
        """ì£¼ í¬ë¡¤ëŸ¬ì— ë”°ë¥¸ í´ë°± ìˆœì„œ ê²°ì •"""
        all_crawlers = ["firecrawl", "crawl4ai", "playwright", "requests"]
        fallback_order = [c for c in all_crawlers if c != primary_crawler]
        
        # ì‚¬ì´íŠ¸ íŠ¹ì„±ì— ë”°ë¥¸ í´ë°± ìˆœì„œ ìµœì í™”
        site_type = site_analysis.get("site_type", {}).get("type", "simple_static")
        
        if site_type == "complex_spa":
            # SPAì˜ ê²½ìš° JS ì²˜ë¦¬ ê°€ëŠ¥í•œ ìˆœì„œ
            fallback_order = ["firecrawl", "playwright", "crawl4ai", "requests"]
        elif site_type == "anti_bot_heavy":
            # ì•ˆí‹°ë´‡ì´ ê°•í•œ ê²½ìš°
            fallback_order = ["firecrawl", "playwright", "crawl4ai", "requests"]
        elif site_type == "simple_static":
            # ë‹¨ìˆœ ì •ì  ì‚¬ì´íŠ¸
            fallback_order = ["requests", "playwright", "crawl4ai", "firecrawl"]
        
        # ì£¼ í¬ë¡¤ëŸ¬ ì œê±°
        return [c for c in fallback_order if c != primary_crawler]
    
    def _get_crawler_settings(self, crawler: str, site_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """í¬ë¡¤ëŸ¬ë³„ ì„¸ë¶€ ì„¤ì •"""
        base_settings = {
            "user_agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "headers": {
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
                "Accept-Language": "ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3",
                "Accept-Encoding": "gzip, deflate",
                "DNT": "1",
                "Connection": "keep-alive",
                "Upgrade-Insecure-Requests": "1",
            }
        }
        
        if crawler == "firecrawl":
            base_settings.update({
                "formats": ["markdown", "html"],
                "includeTags": ["article", "main", "content"],
                "excludeTags": ["nav", "footer", "aside", "script"],
                "onlyMainContent": True
            })
        elif crawler == "playwright":
            base_settings.update({
                "wait_for_selector": "body",
                "wait_for_load_state": "networkidle",
                "screenshot": False,
                "full_page": True
            })
        elif crawler == "crawl4ai":
            base_settings.update({
                "word_count_threshold": 50,
                "extraction_strategy": "LLMExtractionStrategy",
                "chunking_strategy": "RegexChunking"
            })
        
        return base_settings
    
    def _get_extraction_rules(self, structure_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """ì½˜í…ì¸  ì¶”ì¶œ ê·œì¹™ ìƒì„±"""
        rules = {
            "title_selectors": ["h1", "title", ".title", "#title"],
            "content_selectors": ["article", "main", ".content", "#content", ".post", ".article"],
            "exclude_selectors": ["nav", "footer", "aside", ".sidebar", ".ads", ".advertisement"],
            "text_filters": {
                "min_length": 10,
                "max_length": 100000,
                "remove_extra_whitespace": True,
                "normalize_unicode": True
            }
        }
        
        # êµ¬ì¡° ë¶„ì„ ê²°ê³¼ ë°˜ì˜
        if "hierarchy" in structure_analysis:
            hierarchy = structure_analysis["hierarchy"]
            if "headings" in hierarchy:
                # ë°œê²¬ëœ í—¤ë”© íƒœê·¸ ìš°ì„  ì‚¬ìš©
                heading_tags = list(hierarchy["headings"].keys())
                if heading_tags:
                    rules["title_selectors"] = heading_tags + rules["title_selectors"]
        
        return rules
    
    def _create_fallback_strategy(self, url: str) -> Dict[str, Any]:
        """ì˜¤ë¥˜ ì‹œ ì‚¬ìš©í•  ê¸°ë³¸ í´ë°± ì „ëµ"""
        logger.warning(f"í´ë°± ì „ëµ ì‚¬ìš©: {url}")
        
        return {
            "url": url,
            "primary_crawler": "requests",
            "fallback_crawlers": ["playwright", "crawl4ai", "firecrawl"],
            "crawler_settings": self._get_crawler_settings("requests", {}),
            "extraction_rules": self._get_extraction_rules({}),
            "quality_thresholds": {
                "minimum_score": 50.0,  # ë‚®ì€ ì„ê³„ê°’
                "retry_threshold": 30.0
            },
            "timeout_settings": {
                "page_load": 30,
                "element_wait": 10,
                "total_timeout": 120
            },
            "is_fallback": True,
            "error": "MCP ì „ëµ ìƒì„± ì‹¤íŒ¨ë¡œ ì¸í•œ í´ë°± ì „ëµ ì‚¬ìš©"
        }
    
    def get_strategy_summary(self, strategy: Dict[str, Any]) -> str:
        """ì „ëµ ìš”ì•½ ë¬¸ìì—´ ìƒì„±"""
        primary = strategy.get("primary_crawler", "unknown")
        fallbacks = strategy.get("fallback_crawlers", [])
        site_type = strategy.get("site_characteristics", {}).get("type", "unknown")
        
        return f"ì£¼ í¬ë¡¤ëŸ¬: {primary}, í´ë°±: {' â†’ '.join(fallbacks)}, ì‚¬ì´íŠ¸ ìœ í˜•: {site_type}" 