from fastapi import FastAPI, HTTPException, WebSocket
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
import uvicorn
import logging
import os
from contextlib import asynccontextmanager
from dotenv import load_dotenv

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ (ai-crawler ë£¨íŠ¸ ë””ë ‰í† ë¦¬ì˜ .env íŒŒì¼)
load_dotenv(os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), '.env'))

from app.api.routes import router as api_router, set_crawler_instance
from app.api.websocket import websocket_endpoint
from app.crawlers.multi_engine import MultiEngineCrawler

# ì „ì—­ í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤
crawler_instance = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ë¼ì´í”„ì‚¬ì´í´ ê´€ë¦¬"""
    global crawler_instance
    
    # ì‹œì‘ ì‹œ ì´ˆê¸°í™”
    logging.info("ğŸš€ AI Crawler ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
    crawler_instance = MultiEngineCrawler()
    await crawler_instance.initialize()
    
    # routesì— í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ì „ë‹¬
    set_crawler_instance(crawler_instance)
    
    logging.info("âœ… í¬ë¡¤ë§ ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ")
    
    yield
    
    # ì¢…ë£Œ ì‹œ ì •ë¦¬
    logging.info("ğŸ”„ ì‹œìŠ¤í…œ ì¢…ë£Œ ì¤‘...")
    if crawler_instance:
        await crawler_instance.cleanup()
    logging.info("âœ… ì‹œìŠ¤í…œ ì¢…ë£Œ ì™„ë£Œ")

# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="AI-Powered Smart Web Crawler",
    description="MCP ê¸°ë°˜ ì§€ëŠ¥í˜• ì›¹ í¬ë¡¤ë§ ì‹œìŠ¤í…œ",
    version="1.0.0",
    lifespan=lifespan
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://127.0.0.1:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# API ë¼ìš°í„° ë“±ë¡
app.include_router(api_router, prefix="/api/v1")

# WebSocket ì—”ë“œí¬ì¸íŠ¸ ë“±ë¡
@app.websocket("/ws/{connection_id}")
async def websocket_route(websocket: WebSocket, connection_id: str):
    await websocket_endpoint(websocket, connection_id)

# ì •ì  íŒŒì¼ ì„œë¹™ (ê²°ê³¼ íŒŒì¼ ë‹¤ìš´ë¡œë“œìš©)
app.mount("/downloads", StaticFiles(directory="results"), name="downloads")

@app.get("/")
async def root():
    """í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "message": "AI-Powered Smart Web Crawler",
        "status": "running",
        "version": "1.0.0"
    }

@app.get("/health")
async def health_check():
    """ìƒì„¸ í—¬ìŠ¤ì²´í¬"""
    global crawler_instance
    
    engine_status = {}
    if crawler_instance:
        engine_status = await crawler_instance.get_engine_status()
    
    return {
        "status": "healthy",
        "engines": engine_status,
        "message": "All systems operational"
    }

if __name__ == "__main__":
    # ë¡œê¹… ì„¤ì • - ëª¨ë“  ë””ë²„ê¹… ë¡œê·¸ í‘œì‹œ
    logging.basicConfig(
        level=logging.DEBUG,  # DEBUG ë ˆë²¨ë¡œ ëª¨ë“  ë¡œê·¸ í‘œì‹œ
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    )
    
    # ğŸ”§ ëª¨ë“  í¬ë¡¤ëŸ¬ ê´€ë ¨ ë¡œê±°ë¥¼ DEBUG ë ˆë²¨ë¡œ ì„¤ì •
    logging.getLogger("app.crawlers.multi_engine").setLevel(logging.DEBUG)
    logging.getLogger("app.api.routes").setLevel(logging.DEBUG)
    logging.getLogger("app.mcp.tools").setLevel(logging.DEBUG)
    logging.getLogger("app.mcp.client").setLevel(logging.DEBUG)
    
    # ì„œë²„ ì‹¤í–‰
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8001,
        reload=True,
        log_level="debug"  # uvicorn ë¡œê·¸ ë ˆë²¨ë„ debugë¡œ ë³€ê²½
    ) 